{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5-iCVx308906",
    "outputId": "7d55fa56-3316-4ab3-cd32-018fa903d01c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: nltk in /home/melpulomas/.local/lib/python3.6/site-packages (3.5)\n",
      "Requirement already satisfied: click in /usr/lib/python3/dist-packages (from nltk) (6.7)\n",
      "Requirement already satisfied: tqdm in /home/melpulomas/.local/lib/python3.6/site-packages (from nltk) (4.56.0)\n",
      "Requirement already satisfied: joblib in /home/melpulomas/.local/lib/python3.6/site-packages (from nltk) (1.0.0)\n",
      "Requirement already satisfied: regex in /home/melpulomas/.local/lib/python3.6/site-packages (from nltk) (2020.11.13)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: torch in /home/melpulomas/.local/lib/python3.6/site-packages (1.7.0)\n",
      "Requirement already satisfied: typing-extensions in /home/melpulomas/.local/lib/python3.6/site-packages (from torch) (3.7.4.3)\n",
      "Requirement already satisfied: future in /home/melpulomas/.local/lib/python3.6/site-packages (from torch) (0.18.2)\n",
      "Requirement already satisfied: dataclasses in /home/melpulomas/.local/lib/python3.6/site-packages (from torch) (0.7)\n",
      "Requirement already satisfied: numpy in /home/melpulomas/.local/lib/python3.6/site-packages (from torch) (1.19.3)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: fasttext in /home/melpulomas/.local/lib/python3.6/site-packages (0.9.2)\n",
      "Requirement already satisfied: pybind11>=2.2 in /home/melpulomas/.local/lib/python3.6/site-packages (from fasttext) (2.6.2)\n",
      "Requirement already satisfied: setuptools>=0.7.0 in /home/melpulomas/.local/lib/python3.6/site-packages (from fasttext) (54.1.2)\n",
      "Requirement already satisfied: numpy in /home/melpulomas/.local/lib/python3.6/site-packages (from fasttext) (1.19.3)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: sklearn-hierarchical-classification in /home/melpulomas/.local/lib/python3.6/site-packages (1.3.2)\n",
      "Requirement already satisfied: scipy>=0.19.1 in /home/melpulomas/.local/lib/python3.6/site-packages (from sklearn-hierarchical-classification) (1.5.4)\n",
      "Requirement already satisfied: networkx>=2.4 in /home/melpulomas/.local/lib/python3.6/site-packages (from sklearn-hierarchical-classification) (2.5)\n",
      "Requirement already satisfied: numpy>=1.13.1 in /home/melpulomas/.local/lib/python3.6/site-packages (from sklearn-hierarchical-classification) (1.19.3)\n",
      "Requirement already satisfied: scikit-learn>=0.19.0 in /home/melpulomas/.local/lib/python3.6/site-packages (from sklearn-hierarchical-classification) (0.24.1)\n",
      "Requirement already satisfied: decorator>=4.3.0 in /home/melpulomas/.local/lib/python3.6/site-packages (from networkx>=2.4->sklearn-hierarchical-classification) (4.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/melpulomas/.local/lib/python3.6/site-packages (from scikit-learn>=0.19.0->sklearn-hierarchical-classification) (2.1.0)\n",
      "Requirement already satisfied: joblib>=0.11 in /home/melpulomas/.local/lib/python3.6/site-packages (from scikit-learn>=0.19.0->sklearn-hierarchical-classification) (1.0.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk\n",
    "!pip install torch\n",
    "!pip install fasttext\n",
    "!pip install sklearn-hierarchical-classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iyw2yfE0HB4e",
    "outputId": "643942c9-393b-4b27-b09b-72a9068f99ff"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: prettytable in /home/melpulomas/.local/lib/python3.6/site-packages (2.1.0)\n",
      "Requirement already satisfied: wcwidth in /home/melpulomas/.local/lib/python3.6/site-packages (from prettytable) (0.2.5)\n",
      "Requirement already satisfied: importlib-metadata in /home/melpulomas/.local/lib/python3.6/site-packages (from prettytable) (3.7.3)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/melpulomas/.local/lib/python3.6/site-packages (from importlib-metadata->prettytable) (3.4.1)\n",
      "Requirement already satisfied: typing-extensions>=3.6.4 in /home/melpulomas/.local/lib/python3.6/site-packages (from importlib-metadata->prettytable) (3.7.4.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install prettytable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "arhXbfrS9B5y"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uAtjFYuZ9255",
    "outputId": "21bca754-b56d-4c05-84b5-d71c8c573cf5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/melpulomas/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "17B1O7A89CcB",
    "outputId": "83ffdcec-c5fc-4b15-9a5b-f860fe1b5790"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'pages', 'volume', 'punct', 'note', 'title', 'date', 'editor', 'location', 'journal', 'author', 'tech', 'other', 'institution', 'booktitle', 'publisher'}\n"
     ]
    }
   ],
   "source": [
    "''' \n",
    "Preprocess Data\n",
    "'''\n",
    "\n",
    "import re\n",
    "import nltk\n",
    "import string\n",
    "import fasttext\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from nltk import word_tokenize\n",
    "\n",
    "OTHER_TAG = \"other\"\n",
    "PUNCT_TAG = \"punct\"\n",
    "\n",
    "with open('./utils/tags.txt', encoding=\"utf-8\", errors='ignore') as f:\n",
    "    tags = set([str.rstrip(tag) for tag in f.readlines()])\n",
    "\n",
    "with open('./utils/tags_hierarchy.txt', encoding=\"utf-8\", errors='ignore') as f:\n",
    "    hierarchy_tags = set([str.rstrip(tag) for tag in f.readlines()])\n",
    "\n",
    "def remove_labels(text):\n",
    "    return re.sub(r'\\<\\/?[\\w-]*\\>\\s*', \"\", text).strip()\n",
    "\n",
    "def tag_token(token, tag):\n",
    "    if token in string.punctuation:\n",
    "        return (token, PUNCT_TAG)\n",
    "    return (token, tag)\n",
    "\n",
    "def get_tagged_tokens(groups):\n",
    "    tagged_tokens = []\n",
    "    for group in groups:\n",
    "        ref, tag = group[0], group[1]\n",
    "        if tag not in tags:\n",
    "            tag = OTHER_TAG\n",
    "        unlabelled = remove_labels(ref)\n",
    "        tokens = word_tokenize(unlabelled)\n",
    "        tagged_tokens.extend(list(map(lambda token: tag_token(token, tag), tokens)))\n",
    "    return tagged_tokens\n",
    "\n",
    "''' Attach tags to each token '''\n",
    "def attach_tags(dataset_path):\n",
    "    dataset = []\n",
    "    with open(dataset_path, encoding=\"utf-8\", errors='ignore') as f:\n",
    "        refs = f.readlines()\n",
    "        for ref in refs:\n",
    "            groups = re.findall(r'(\\<(.*)\\>.*\\<\\/\\2\\>)', ref) # format (<tag>...</tag>, tag)\n",
    "            tagged_tokens = get_tagged_tokens(groups)\n",
    "            dataset.append(tagged_tokens)\n",
    "    return dataset\n",
    "\n",
    "''' Removes labels and tokenizes '''\n",
    "def tokenize_dataset(dataset_path, sep=\" \"):\n",
    "    dataset = []\n",
    "    with open(dataset_path, encoding=\"utf-8\", errors='ignore') as f:\n",
    "        refs = f.readlines()\n",
    "        for ref in refs:\n",
    "            ref = remove_labels(ref) \n",
    "            tokenized = sep.join(word_tokenize(ref))\n",
    "            dataset.append(tokenized)\n",
    "    return dataset\n",
    "\n",
    "def train_word_embedding_model(dataset_path, embedding_dim, use_subwords=False, use_hierarchy=False):\n",
    "    embedding_dataset_path = './dataset/word_embedding_dataset'\n",
    "    hierarchy_dataset_path = './dataset/umass-citation/training'\n",
    "\n",
    "    with open(embedding_dataset_path, 'w', errors='ignore') as f:\n",
    "        # fasttext tokenizes by whitespaces\n",
    "        word_embedding_dataset = tokenize_dataset(dataset_path, sep=\" \") \n",
    "        f.write(\"\\n\".join(word_embedding_dataset))\n",
    "    if use_subwords:\n",
    "      model_path = './models/subword_embedding.bin'\n",
    "      model = fasttext.train_unsupervised(embedding_dataset_path, dim=embedding_dim)\n",
    "    elif use_hierarchy:\n",
    "        model_path = './models/hierarchy_word_embedding.bin'\n",
    "        model = fasttext.train_unsupervised(embedding_dataset_path, dim=embedding_dim, maxn=0)\n",
    "    else:\n",
    "      model_path = './models/word_embedding.bin'\n",
    "      model = fasttext.train_unsupervised(embedding_dataset_path, dim=embedding_dim, maxn=0)\n",
    "    model.save_model(model_path)\n",
    "    return model\n",
    "\n",
    "def map_to_index(keys, idx_start=0):\n",
    "    key_to_idx, keys_arr, idx = {}, [], idx_start\n",
    "    for key in keys:\n",
    "        key_to_idx[key] = idx\n",
    "        keys_arr.append(key)\n",
    "        idx += 1\n",
    "    return key_to_idx, keys_arr\n",
    "\n",
    "dataset_path = './dataset/standardized_dataset.txt'\n",
    "dataset = attach_tags(dataset_path)\n",
    "EMBEDDING_DIM = 100\n",
    "word_embedding_model = train_word_embedding_model(dataset_path, embedding_dim = EMBEDDING_DIM)\n",
    "\n",
    "all_tags = tags \n",
    "all_tags.add(OTHER_TAG)\n",
    "all_tags.add(PUNCT_TAG)\n",
    "tag_to_idx, tag_arr = map_to_index(all_tags)\n",
    "print(all_tags)\n",
    "\n",
    "''' Get inputs and outputs for model '''\n",
    "X, y = [], []\n",
    "for ref in dataset:\n",
    "    X_ref, y_ref = [], []\n",
    "    for token, tag in ref:\n",
    "        X_ref.append(word_embedding_model.get_word_vector(token))\n",
    "        y_ref.append(tag_to_idx[tag])\n",
    "    X.append(X_ref)\n",
    "    y.append(y_ref)\n",
    "\n",
    "max_length = max(map(lambda ref: len(ref), X))\n",
    "X = pad_sequences(X, maxlen=max_length, padding='post', truncating='pre', value=float(len(all_tags)), dtype='float32')\n",
    "y = pad_sequences(y, maxlen=max_length, padding='post', truncating='pre', value=float(len(all_tags)), dtype='float32')\n",
    "\n",
    "##create another model trained with the hierarchy dataset\n",
    "dataset_hierarch_path = \"./dataset/hierarchy_dataset.txt\"\n",
    "dataset_hierarch = attach_tags(dataset_hierarch_path)\n",
    "word_embedding_model = train_word_embedding_model(dataset_path, embedding_dim = EMBEDDING_DIM, use_subwords=False, use_hierarchy=True)\n",
    "all_tags = hierarchy_tags \n",
    "all_tags.add(OTHER_TAG)\n",
    "all_tags.add(PUNCT_TAG)\n",
    "tag_to_idx, tag_arr = map_to_index(all_tags)\n",
    "\n",
    "''' Get inputs and outputs for model '''\n",
    "X_hierarchy, y_hierarchy = [], []\n",
    "for ref in dataset:\n",
    "    X_ref, y_ref = [], []\n",
    "    for token, tag in ref:\n",
    "        X_ref.append(word_embedding_model.get_word_vector(token))\n",
    "        y_ref.append(tag_to_idx[tag])\n",
    "    X_hierarchy.append(X_ref)\n",
    "    y_hierarchy.append(y_ref)\n",
    "\n",
    "max_length = max(map(lambda ref: len(ref), X_hierarchy))\n",
    "X_hierarchy = pad_sequences(X_hierarchy, maxlen=max_length, padding='post', truncating='pre', value=float(len(all_tags)), dtype='float32')\n",
    "y_hierarchy = pad_sequences(y_hierarchy, maxlen=max_length, padding='post', truncating='pre', value=float(len(all_tags)), dtype='float32')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "N49G-Ws59HYu"
   },
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pack_padded_sequence\n",
    "class LSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, num_layers):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.num_layers = num_layers\n",
    "    \n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size = self.input_size,\n",
    "            hidden_size = self.hidden_size,\n",
    "            num_layers = self.num_layers,\n",
    "            batch_first = True\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Initialise hidden state\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size)\n",
    "\n",
    "        # Initialise internal state\n",
    "        c0 = torch.zeros(self.num_layers,x.size(0), self.hidden_size)\n",
    "\n",
    "        # Propagate input through LSTM\n",
    "        output, (hn, cn) = self.lstm(x, (h0, c0))\n",
    "\n",
    "        return output, (hn, cn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "40dkNSs69Igz"
   },
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, num_layers):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        self.lstm = LSTM(input_size, hidden_size, output_size, num_layers)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        output, (hn, cn) = self.lstm(x)\n",
    "        output = self.fc(output)\n",
    "\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "u8RRoJQo9JpA"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Hyperparameters\n",
    "'''\n",
    "num_epochs = 700\n",
    "learning_rate = 0.001\n",
    "\n",
    "input_size = EMBEDDING_DIM # Number of features\n",
    "hidden_size = 25 # Number of features in the hidden state\n",
    "num_layers = 1 # Number of stacked LSTM layers\n",
    "\n",
    "output_size = len(all_tags) # Number of output classes\n",
    "\n",
    "model = Net(input_size, hidden_size, output_size, num_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "5H6Lsfq19Ksk"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Loss Function and Optimiser\n",
    "'''\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=len(all_tags))\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 522
    },
    "id": "kesJ8HiJ9LyR",
    "outputId": "87633165-47b3-4780-daae-b142e48d087e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/melpulomas/.local/lib/python3.6/site-packages/torch/autograd/__init__.py:132: UserWarning: CUDA initialization: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx (Triggered internally at  /pytorch/c10/cuda/CUDAFunctions.cpp:100.)\n",
      "  allow_unreachable=True)  # allow_unreachable flag\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, loss after minibatch     1: 2.82900\n",
      "Epoch: 0, loss after minibatch     2: 2.81806\n",
      "Epoch: 0, loss after minibatch     3: 2.80610\n",
      "Epoch: 0, loss after minibatch     4: 2.79434\n",
      "Epoch: 0, loss after minibatch     5: 2.79059\n",
      "Epoch: 0, loss after minibatch     6: 2.77440\n",
      "Epoch: 0, loss after minibatch     7: 2.76638\n",
      "Epoch: 0, loss after minibatch     8: 2.75910\n",
      "Epoch: 0, loss after minibatch     9: 2.74750\n",
      "Epoch: 0, loss after minibatch    10: 2.73639\n",
      "Epoch: 100, loss after minibatch     1: 0.40795\n",
      "Epoch: 100, loss after minibatch     2: 0.43288\n",
      "Epoch: 100, loss after minibatch     3: 0.44232\n",
      "Epoch: 100, loss after minibatch     4: 0.47621\n",
      "Epoch: 100, loss after minibatch     5: 0.42850\n",
      "Epoch: 100, loss after minibatch     6: 0.40346\n",
      "Epoch: 100, loss after minibatch     7: 0.45167\n",
      "Epoch: 100, loss after minibatch     8: 0.44126\n",
      "Epoch: 100, loss after minibatch     9: 0.42357\n",
      "Epoch: 100, loss after minibatch    10: 0.49413\n",
      "Epoch: 200, loss after minibatch     1: 0.28935\n",
      "Epoch: 200, loss after minibatch     2: 0.30883\n",
      "Epoch: 200, loss after minibatch     3: 0.30851\n",
      "Epoch: 200, loss after minibatch     4: 0.32674\n",
      "Epoch: 200, loss after minibatch     5: 0.30943\n",
      "Epoch: 200, loss after minibatch     6: 0.28687\n",
      "Epoch: 200, loss after minibatch     7: 0.31162\n",
      "Epoch: 200, loss after minibatch     8: 0.29995\n",
      "Epoch: 200, loss after minibatch     9: 0.30718\n",
      "Epoch: 200, loss after minibatch    10: 0.35537\n",
      "Epoch: 300, loss after minibatch     1: 0.23998\n",
      "Epoch: 300, loss after minibatch     2: 0.25554\n",
      "Epoch: 300, loss after minibatch     3: 0.24670\n",
      "Epoch: 300, loss after minibatch     4: 0.27154\n",
      "Epoch: 300, loss after minibatch     5: 0.26086\n",
      "Epoch: 300, loss after minibatch     6: 0.23672\n",
      "Epoch: 300, loss after minibatch     7: 0.25737\n",
      "Epoch: 300, loss after minibatch     8: 0.24725\n",
      "Epoch: 300, loss after minibatch     9: 0.26321\n",
      "Epoch: 300, loss after minibatch    10: 0.28657\n",
      "Epoch: 400, loss after minibatch     1: 0.21278\n",
      "Epoch: 400, loss after minibatch     2: 0.22360\n",
      "Epoch: 400, loss after minibatch     3: 0.20350\n",
      "Epoch: 400, loss after minibatch     4: 0.23382\n",
      "Epoch: 400, loss after minibatch     5: 0.21507\n",
      "Epoch: 400, loss after minibatch     6: 0.19790\n",
      "Epoch: 400, loss after minibatch     7: 0.21744\n",
      "Epoch: 400, loss after minibatch     8: 0.20933\n",
      "Epoch: 400, loss after minibatch     9: 0.22623\n",
      "Epoch: 400, loss after minibatch    10: 0.22321\n",
      "Epoch: 500, loss after minibatch     1: 0.18705\n",
      "Epoch: 500, loss after minibatch     2: 0.19939\n",
      "Epoch: 500, loss after minibatch     3: 0.17678\n",
      "Epoch: 500, loss after minibatch     4: 0.20523\n",
      "Epoch: 500, loss after minibatch     5: 0.18918\n",
      "Epoch: 500, loss after minibatch     6: 0.17432\n",
      "Epoch: 500, loss after minibatch     7: 0.19188\n",
      "Epoch: 500, loss after minibatch     8: 0.18420\n",
      "Epoch: 500, loss after minibatch     9: 0.19718\n",
      "Epoch: 500, loss after minibatch    10: 0.19180\n",
      "Epoch: 600, loss after minibatch     1: 0.18836\n",
      "Epoch: 600, loss after minibatch     2: 0.20633\n",
      "Epoch: 600, loss after minibatch     3: 0.17897\n",
      "Epoch: 600, loss after minibatch     4: 0.22806\n",
      "Epoch: 600, loss after minibatch     5: 0.19068\n",
      "Epoch: 600, loss after minibatch     6: 0.17445\n",
      "Epoch: 600, loss after minibatch     7: 0.21876\n",
      "Epoch: 600, loss after minibatch     8: 0.18916\n",
      "Epoch: 600, loss after minibatch     9: 0.20119\n",
      "Epoch: 600, loss after minibatch    10: 0.21344\n",
      "Report after 0 fold\n",
      "+-------------+---------------------+--------------------+---------------------+\n",
      "|     Tag     |      Precision      |       Recall       |        FBeta        |\n",
      "+-------------+---------------------+--------------------+---------------------+\n",
      "|    punct    |  0.9964071856287425 | 0.985781990521327  |  0.9910661107802262 |\n",
      "|  publisher  |         0.8         |        0.8         |  0.8000000000000002 |\n",
      "|   location  |  0.7142857142857143 | 0.5555555555555556 |  0.6250000000000001 |\n",
      "|    author   |  0.9030303030303031 | 0.951063829787234  |  0.9264248704663213 |\n",
      "| institution |  0.5555555555555556 | 0.9090909090909091 |  0.6896551724137931 |\n",
      "|     tech    |         1.0         | 0.5769230769230769 |  0.7317073170731707 |\n",
      "|    pages    |  0.9090909090909091 |        1.0         |  0.9523809523809523 |\n",
      "|    editor   |         0.0         |        0.0         |         0.0         |\n",
      "|    title    |  0.9239130434782609 | 0.9659090909090909 |  0.9444444444444444 |\n",
      "|  booktitle  |  0.9454545454545454 |        1.0         |  0.9719626168224299 |\n",
      "|     date    |         0.85        | 0.8360655737704918 |  0.8429752066115703 |\n",
      "|     note    |         0.0         |        0.0         |         0.0         |\n",
      "|    volume   |         1.0         | 0.8333333333333334 |  0.9090909090909091 |\n",
      "|    other    | 0.03981913652275379 | 0.9646643109540636 | 0.07648129990194705 |\n",
      "|   journal   |  0.6666666666666666 | 0.5454545454545454 |         0.6         |\n",
      "+-------------+---------------------+--------------------+---------------------+\n",
      "Epoch: 0, loss after minibatch     1: 0.14782\n",
      "Epoch: 0, loss after minibatch     2: 0.15383\n",
      "Epoch: 0, loss after minibatch     3: 0.14048\n",
      "Epoch: 0, loss after minibatch     4: 0.15986\n",
      "Epoch: 0, loss after minibatch     5: 0.15506\n",
      "Epoch: 0, loss after minibatch     6: 0.13793\n",
      "Epoch: 0, loss after minibatch     7: 0.15895\n",
      "Epoch: 0, loss after minibatch     8: 0.14925\n",
      "Epoch: 0, loss after minibatch     9: 0.15634\n",
      "Epoch: 0, loss after minibatch    10: 0.14680\n",
      "Epoch: 100, loss after minibatch     1: 0.13812\n",
      "Epoch: 100, loss after minibatch     2: 0.13967\n",
      "Epoch: 100, loss after minibatch     3: 0.12799\n",
      "Epoch: 100, loss after minibatch     4: 0.14460\n",
      "Epoch: 100, loss after minibatch     5: 0.14366\n",
      "Epoch: 100, loss after minibatch     6: 0.12531\n",
      "Epoch: 100, loss after minibatch     7: 0.14835\n",
      "Epoch: 100, loss after minibatch     8: 0.13610\n",
      "Epoch: 100, loss after minibatch     9: 0.14191\n",
      "Epoch: 100, loss after minibatch    10: 0.13112\n",
      "Epoch: 200, loss after minibatch     1: 0.12897\n",
      "Epoch: 200, loss after minibatch     2: 0.12790\n",
      "Epoch: 200, loss after minibatch     3: 0.11880\n",
      "Epoch: 200, loss after minibatch     4: 0.13262\n",
      "Epoch: 200, loss after minibatch     5: 0.13475\n",
      "Epoch: 200, loss after minibatch     6: 0.11608\n",
      "Epoch: 200, loss after minibatch     7: 0.13789\n",
      "Epoch: 200, loss after minibatch     8: 0.12534\n",
      "Epoch: 200, loss after minibatch     9: 0.13158\n",
      "Epoch: 200, loss after minibatch    10: 0.11882\n",
      "Epoch: 300, loss after minibatch     1: 0.12558\n",
      "Epoch: 300, loss after minibatch     2: 0.12319\n",
      "Epoch: 300, loss after minibatch     3: 0.11392\n",
      "Epoch: 300, loss after minibatch     4: 0.12610\n",
      "Epoch: 300, loss after minibatch     5: 0.12786\n",
      "Epoch: 300, loss after minibatch     6: 0.11112\n",
      "Epoch: 300, loss after minibatch     7: 0.13249\n",
      "Epoch: 300, loss after minibatch     8: 0.11825\n",
      "Epoch: 300, loss after minibatch     9: 0.12542\n",
      "Epoch: 300, loss after minibatch    10: 0.11228\n",
      "Epoch: 400, loss after minibatch     1: 0.11930\n",
      "Epoch: 400, loss after minibatch     2: 0.11602\n",
      "Epoch: 400, loss after minibatch     3: 0.10769\n",
      "Epoch: 400, loss after minibatch     4: 0.11824\n",
      "Epoch: 400, loss after minibatch     5: 0.11638\n",
      "Epoch: 400, loss after minibatch     6: 0.10515\n",
      "Epoch: 400, loss after minibatch     7: 0.12579\n",
      "Epoch: 400, loss after minibatch     8: 0.11293\n",
      "Epoch: 400, loss after minibatch     9: 0.11834\n",
      "Epoch: 400, loss after minibatch    10: 0.10465\n",
      "Epoch: 500, loss after minibatch     1: 0.11290\n",
      "Epoch: 500, loss after minibatch     2: 0.10937\n",
      "Epoch: 500, loss after minibatch     3: 0.10077\n",
      "Epoch: 500, loss after minibatch     4: 0.11205\n",
      "Epoch: 500, loss after minibatch     5: 0.10967\n",
      "Epoch: 500, loss after minibatch     6: 0.09864\n",
      "Epoch: 500, loss after minibatch     7: 0.11997\n",
      "Epoch: 500, loss after minibatch     8: 0.10654\n",
      "Epoch: 500, loss after minibatch     9: 0.11233\n",
      "Epoch: 500, loss after minibatch    10: 0.09747\n",
      "Epoch: 600, loss after minibatch     1: 0.10725\n",
      "Epoch: 600, loss after minibatch     2: 0.10286\n",
      "Epoch: 600, loss after minibatch     3: 0.09539\n",
      "Epoch: 600, loss after minibatch     4: 0.10410\n",
      "Epoch: 600, loss after minibatch     5: 0.10333\n",
      "Epoch: 600, loss after minibatch     6: 0.09279\n",
      "Epoch: 600, loss after minibatch     7: 0.11406\n",
      "Epoch: 600, loss after minibatch     8: 0.10062\n",
      "Epoch: 600, loss after minibatch     9: 0.10380\n",
      "Epoch: 600, loss after minibatch    10: 0.09051\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Report after 1 fold\n",
      "+-------------+---------------------+--------------------+--------------------+\n",
      "|     Tag     |      Precision      |       Recall       |       FBeta        |\n",
      "+-------------+---------------------+--------------------+--------------------+\n",
      "|    punct    |  0.7272112264021575 | 0.9905213270142179 | 0.8091999709987763 |\n",
      "|  publisher  |         0.8         |        0.8         | 0.8000000000000002 |\n",
      "|   location  |  0.7071428571428571 | 0.6666666666666667 | 0.680921052631579  |\n",
      "|    author   |  0.4695797193273762 | 0.9553191489361702 | 0.4980306240697236 |\n",
      "| institution |  0.6215277777777778 | 0.9545454545454546 | 0.752234993614304  |\n",
      "|     tech    |         1.0         |        0.75        | 0.8458536585365855 |\n",
      "|    pages    |  0.9545454545454546 |        1.0         | 0.9761904761904762 |\n",
      "|    editor   |         0.0         |        0.0         |        0.0         |\n",
      "|    title    |  0.9286947191640231 | 0.9772727272727273 | 0.952354672553348  |\n",
      "|  booktitle  |  0.9727272727272727 | 0.9903846153846154 | 0.9811269394791761 |\n",
      "|     date    |  0.8977272727272727 | 0.8442622950819672 | 0.8697634653747506 |\n",
      "|     note    |         0.5         |        0.25        | 0.3333333333333333 |\n",
      "|    volume   |         1.0         | 0.9166666666666667 | 0.9545454545454546 |\n",
      "|    other    | 0.40328945652953335 | 0.9673144876325088 | 0.466477779436153  |\n",
      "|   journal   |  0.7833333333333333 | 0.6818181818181819 | 0.7285714285714286 |\n",
      "+-------------+---------------------+--------------------+--------------------+\n",
      "Epoch: 0, loss after minibatch     1: 0.10762\n",
      "Epoch: 0, loss after minibatch     2: 0.10729\n",
      "Epoch: 0, loss after minibatch     3: 0.09564\n",
      "Epoch: 0, loss after minibatch     4: 0.10506\n",
      "Epoch: 0, loss after minibatch     5: 0.10335\n",
      "Epoch: 0, loss after minibatch     6: 0.09173\n",
      "Epoch: 0, loss after minibatch     7: 0.11504\n",
      "Epoch: 0, loss after minibatch     8: 0.09962\n",
      "Epoch: 0, loss after minibatch     9: 0.09911\n",
      "Epoch: 0, loss after minibatch    10: 0.09119\n",
      "Epoch: 100, loss after minibatch     1: 0.13168\n",
      "Epoch: 100, loss after minibatch     2: 0.12127\n",
      "Epoch: 100, loss after minibatch     3: 0.11771\n",
      "Epoch: 100, loss after minibatch     4: 0.13975\n",
      "Epoch: 100, loss after minibatch     5: 0.12361\n",
      "Epoch: 100, loss after minibatch     6: 0.12101\n",
      "Epoch: 100, loss after minibatch     7: 0.13387\n",
      "Epoch: 100, loss after minibatch     8: 0.12569\n",
      "Epoch: 100, loss after minibatch     9: 0.12455\n",
      "Epoch: 100, loss after minibatch    10: 0.11521\n",
      "Epoch: 200, loss after minibatch     1: 0.09728\n",
      "Epoch: 200, loss after minibatch     2: 0.09067\n",
      "Epoch: 200, loss after minibatch     3: 0.08571\n",
      "Epoch: 200, loss after minibatch     4: 0.09502\n",
      "Epoch: 200, loss after minibatch     5: 0.08790\n",
      "Epoch: 200, loss after minibatch     6: 0.08155\n",
      "Epoch: 200, loss after minibatch     7: 0.10154\n",
      "Epoch: 200, loss after minibatch     8: 0.08874\n",
      "Epoch: 200, loss after minibatch     9: 0.09007\n",
      "Epoch: 200, loss after minibatch    10: 0.07903\n",
      "Epoch: 300, loss after minibatch     1: 0.14253\n",
      "Epoch: 300, loss after minibatch     2: 0.12788\n",
      "Epoch: 300, loss after minibatch     3: 0.11936\n",
      "Epoch: 300, loss after minibatch     4: 0.13286\n",
      "Epoch: 300, loss after minibatch     5: 0.12218\n",
      "Epoch: 300, loss after minibatch     6: 0.11255\n",
      "Epoch: 300, loss after minibatch     7: 0.12898\n",
      "Epoch: 300, loss after minibatch     8: 0.11829\n",
      "Epoch: 300, loss after minibatch     9: 0.11256\n",
      "Epoch: 300, loss after minibatch    10: 0.10967\n",
      "Epoch: 400, loss after minibatch     1: 0.09208\n",
      "Epoch: 400, loss after minibatch     2: 0.08402\n",
      "Epoch: 400, loss after minibatch     3: 0.08111\n",
      "Epoch: 400, loss after minibatch     4: 0.08873\n",
      "Epoch: 400, loss after minibatch     5: 0.08165\n",
      "Epoch: 400, loss after minibatch     6: 0.07726\n",
      "Epoch: 400, loss after minibatch     7: 0.09797\n",
      "Epoch: 400, loss after minibatch     8: 0.08284\n",
      "Epoch: 400, loss after minibatch     9: 0.08473\n",
      "Epoch: 400, loss after minibatch    10: 0.07179\n",
      "Epoch: 500, loss after minibatch     1: 0.08765\n",
      "Epoch: 500, loss after minibatch     2: 0.07973\n",
      "Epoch: 500, loss after minibatch     3: 0.07841\n",
      "Epoch: 500, loss after minibatch     4: 0.08569\n",
      "Epoch: 500, loss after minibatch     5: 0.07689\n",
      "Epoch: 500, loss after minibatch     6: 0.07445\n",
      "Epoch: 500, loss after minibatch     7: 0.09256\n",
      "Epoch: 500, loss after minibatch     8: 0.08054\n",
      "Epoch: 500, loss after minibatch     9: 0.08147\n",
      "Epoch: 500, loss after minibatch    10: 0.06960\n",
      "Epoch: 600, loss after minibatch     1: 0.08318\n",
      "Epoch: 600, loss after minibatch     2: 0.07561\n",
      "Epoch: 600, loss after minibatch     3: 0.07535\n",
      "Epoch: 600, loss after minibatch     4: 0.08315\n",
      "Epoch: 600, loss after minibatch     5: 0.07374\n",
      "Epoch: 600, loss after minibatch     6: 0.07206\n",
      "Epoch: 600, loss after minibatch     7: 0.08862\n",
      "Epoch: 600, loss after minibatch     8: 0.07779\n",
      "Epoch: 600, loss after minibatch     9: 0.07908\n",
      "Epoch: 600, loss after minibatch    10: 0.06547\n",
      "Report after 2 fold\n",
      "+-------------+--------------------+--------------------+--------------------+\n",
      "|     Tag     |     Precision      |       Recall       |       FBeta        |\n",
      "+-------------+--------------------+--------------------+--------------------+\n",
      "|    punct    | 0.5050962929323649 | 0.9932859399684043 | 0.5777135508305439 |\n",
      "|  publisher  | 0.8111111111111112 | 0.8666666666666667 | 0.8363636363636364 |\n",
      "|   location  | 0.8047619047619047 | 0.7037037037037037 | 0.7456140350877193 |\n",
      "|    author   | 0.6406269403147231 | 0.9595744680851063 | 0.6571365289439458 |\n",
      "| institution | 0.6762566137566138 | 0.9696969696969697 | 0.7948233290762027 |\n",
      "|     tech    |        1.0         | 0.8205128205128206 | 0.8906998246453054 |\n",
      "|    pages    | 0.8585858585858586 |        1.0         | 0.9174603174603174 |\n",
      "|    editor   |        0.0         |        0.0         |        0.0         |\n",
      "|    title    | 0.9386046486622298 | 0.9833333333333334 | 0.9604326579564127 |\n",
      "|  booktitle  | 0.9755288736420811 | 0.9935897435897436 | 0.9842433564781808 |\n",
      "|     date    | 0.9148690292758089 | 0.8688524590163934 | 0.8909534213609449 |\n",
      "|     note    | 0.6666666666666666 | 0.3333333333333333 | 0.4444444444444444 |\n",
      "|    volume   | 0.9523809523809524 | 0.9444444444444445 | 0.9440559440559441 |\n",
      "|    other    | 0.5253411191678371 | 0.9711425206124852 | 0.5981806243415722 |\n",
      "|   journal   | 0.8555555555555555 | 0.7575757575757577 | 0.8031746031746033 |\n",
      "+-------------+--------------------+--------------------+--------------------+\n",
      "Epoch: 0, loss after minibatch     1: 0.07896\n",
      "Epoch: 0, loss after minibatch     2: 0.07268\n",
      "Epoch: 0, loss after minibatch     3: 0.07293\n",
      "Epoch: 0, loss after minibatch     4: 0.08028\n",
      "Epoch: 0, loss after minibatch     5: 0.07053\n",
      "Epoch: 0, loss after minibatch     6: 0.06972\n",
      "Epoch: 0, loss after minibatch     7: 0.08539\n",
      "Epoch: 0, loss after minibatch     8: 0.07542\n",
      "Epoch: 0, loss after minibatch     9: 0.07682\n",
      "Epoch: 0, loss after minibatch    10: 0.06191\n",
      "Epoch: 100, loss after minibatch     1: 0.07491\n",
      "Epoch: 100, loss after minibatch     2: 0.07096\n",
      "Epoch: 100, loss after minibatch     3: 0.07201\n",
      "Epoch: 100, loss after minibatch     4: 0.07914\n",
      "Epoch: 100, loss after minibatch     5: 0.06823\n",
      "Epoch: 100, loss after minibatch     6: 0.06783\n",
      "Epoch: 100, loss after minibatch     7: 0.08327\n",
      "Epoch: 100, loss after minibatch     8: 0.07365\n",
      "Epoch: 100, loss after minibatch     9: 0.07517\n",
      "Epoch: 100, loss after minibatch    10: 0.05920\n",
      "Epoch: 200, loss after minibatch     1: 0.07224\n",
      "Epoch: 200, loss after minibatch     2: 0.06737\n",
      "Epoch: 200, loss after minibatch     3: 0.06930\n",
      "Epoch: 200, loss after minibatch     4: 0.07620\n",
      "Epoch: 200, loss after minibatch     5: 0.06655\n",
      "Epoch: 200, loss after minibatch     6: 0.06526\n",
      "Epoch: 200, loss after minibatch     7: 0.07869\n",
      "Epoch: 200, loss after minibatch     8: 0.07064\n",
      "Epoch: 200, loss after minibatch     9: 0.07331\n",
      "Epoch: 200, loss after minibatch    10: 0.05446\n",
      "Epoch: 300, loss after minibatch     1: 0.07160\n",
      "Epoch: 300, loss after minibatch     2: 0.06564\n",
      "Epoch: 300, loss after minibatch     3: 0.06828\n",
      "Epoch: 300, loss after minibatch     4: 0.07410\n",
      "Epoch: 300, loss after minibatch     5: 0.06516\n",
      "Epoch: 300, loss after minibatch     6: 0.06404\n",
      "Epoch: 300, loss after minibatch     7: 0.07613\n",
      "Epoch: 300, loss after minibatch     8: 0.06937\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 300, loss after minibatch     9: 0.07175\n",
      "Epoch: 300, loss after minibatch    10: 0.05159\n",
      "Epoch: 400, loss after minibatch     1: 0.06831\n",
      "Epoch: 400, loss after minibatch     2: 0.06294\n",
      "Epoch: 400, loss after minibatch     3: 0.06529\n",
      "Epoch: 400, loss after minibatch     4: 0.07189\n",
      "Epoch: 400, loss after minibatch     5: 0.06170\n",
      "Epoch: 400, loss after minibatch     6: 0.06100\n",
      "Epoch: 400, loss after minibatch     7: 0.07243\n",
      "Epoch: 400, loss after minibatch     8: 0.06704\n",
      "Epoch: 400, loss after minibatch     9: 0.06984\n",
      "Epoch: 400, loss after minibatch    10: 0.04903\n",
      "Epoch: 500, loss after minibatch     1: 0.08240\n",
      "Epoch: 500, loss after minibatch     2: 0.08291\n",
      "Epoch: 500, loss after minibatch     3: 0.07706\n",
      "Epoch: 500, loss after minibatch     4: 0.08654\n",
      "Epoch: 500, loss after minibatch     5: 0.06720\n",
      "Epoch: 500, loss after minibatch     6: 0.06549\n",
      "Epoch: 500, loss after minibatch     7: 0.08778\n",
      "Epoch: 500, loss after minibatch     8: 0.07460\n",
      "Epoch: 500, loss after minibatch     9: 0.08509\n",
      "Epoch: 500, loss after minibatch    10: 0.05572\n",
      "Epoch: 600, loss after minibatch     1: 0.06488\n",
      "Epoch: 600, loss after minibatch     2: 0.06015\n",
      "Epoch: 600, loss after minibatch     3: 0.06321\n",
      "Epoch: 600, loss after minibatch     4: 0.06781\n",
      "Epoch: 600, loss after minibatch     5: 0.05886\n",
      "Epoch: 600, loss after minibatch     6: 0.05731\n",
      "Epoch: 600, loss after minibatch     7: 0.06888\n",
      "Epoch: 600, loss after minibatch     8: 0.06424\n",
      "Epoch: 600, loss after minibatch     9: 0.06694\n",
      "Epoch: 600, loss after minibatch    10: 0.04409\n",
      "Report after 3 fold\n",
      "+-------------+---------------------+--------------------+--------------------+\n",
      "|     Tag     |      Precision      |       Recall       |       FBeta        |\n",
      "+-------------+---------------------+--------------------+--------------------+\n",
      "|    punct    | 0.39411057660895815 | 0.9946682464454976 | 0.4620977955653168 |\n",
      "|  publisher  |  0.8166666666666668 |        0.9         | 0.8545454545454546 |\n",
      "|   location  |  0.8535714285714285 |        0.75        | 0.7945046439628483 |\n",
      "|    author   |  0.7267308889967261 | 0.9648936170212765 | 0.7385880043838655 |\n",
      "| institution |  0.7571924603174603 | 0.9772727272727273 | 0.846117496807152  |\n",
      "|     tech    |         1.0         | 0.8653846153846154 | 0.918024868483979  |\n",
      "|    pages    |  0.6606060606060606 |        1.0         | 0.7193452380952381 |\n",
      "|    editor   |         0.0         |        0.0         |        0.0         |\n",
      "|    title    |  0.9462101236648139 | 0.9863636363636364 | 0.9658401885345741 |\n",
      "|  booktitle  |  0.9816466552315608 | 0.9951923076923077 | 0.9881825173586356 |\n",
      "|     date    |  0.9275310823016844 | 0.8811475409836066 | 0.9035091836677676 |\n",
      "|     note    |         0.75        |        0.5         | 0.5833333333333333 |\n",
      "|    volume   |  0.8809523809523809 | 0.9583333333333334 | 0.9080419580419581 |\n",
      "|    other    |  0.6078519932220317 | 0.9739399293286218 | 0.6772538893088107 |\n",
      "|   journal   |  0.8916666666666666 | 0.7954545454545455 | 0.8404761904761906 |\n",
      "+-------------+---------------------+--------------------+--------------------+\n",
      "Epoch: 0, loss after minibatch     1: 0.06320\n",
      "Epoch: 0, loss after minibatch     2: 0.05763\n",
      "Epoch: 0, loss after minibatch     3: 0.06053\n",
      "Epoch: 0, loss after minibatch     4: 0.06486\n",
      "Epoch: 0, loss after minibatch     5: 0.05762\n",
      "Epoch: 0, loss after minibatch     6: 0.05509\n",
      "Epoch: 0, loss after minibatch     7: 0.06626\n",
      "Epoch: 0, loss after minibatch     8: 0.06308\n",
      "Epoch: 0, loss after minibatch     9: 0.06517\n",
      "Epoch: 0, loss after minibatch    10: 0.04312\n",
      "Epoch: 100, loss after minibatch     1: 0.06256\n",
      "Epoch: 100, loss after minibatch     2: 0.05786\n",
      "Epoch: 100, loss after minibatch     3: 0.06123\n",
      "Epoch: 100, loss after minibatch     4: 0.06436\n",
      "Epoch: 100, loss after minibatch     5: 0.05640\n",
      "Epoch: 100, loss after minibatch     6: 0.05446\n",
      "Epoch: 100, loss after minibatch     7: 0.06557\n",
      "Epoch: 100, loss after minibatch     8: 0.06134\n",
      "Epoch: 100, loss after minibatch     9: 0.06479\n",
      "Epoch: 100, loss after minibatch    10: 0.04241\n",
      "Epoch: 200, loss after minibatch     1: 0.06477\n",
      "Epoch: 200, loss after minibatch     2: 0.05864\n",
      "Epoch: 200, loss after minibatch     3: 0.06184\n",
      "Epoch: 200, loss after minibatch     4: 0.06626\n",
      "Epoch: 200, loss after minibatch     5: 0.05895\n",
      "Epoch: 200, loss after minibatch     6: 0.05646\n",
      "Epoch: 200, loss after minibatch     7: 0.06813\n",
      "Epoch: 200, loss after minibatch     8: 0.06410\n",
      "Epoch: 200, loss after minibatch     9: 0.10079\n",
      "Epoch: 200, loss after minibatch    10: 0.04784\n",
      "Epoch: 300, loss after minibatch     1: 0.06031\n",
      "Epoch: 300, loss after minibatch     2: 0.05544\n",
      "Epoch: 300, loss after minibatch     3: 0.05898\n",
      "Epoch: 300, loss after minibatch     4: 0.06150\n",
      "Epoch: 300, loss after minibatch     5: 0.05583\n",
      "Epoch: 300, loss after minibatch     6: 0.05298\n",
      "Epoch: 300, loss after minibatch     7: 0.06397\n",
      "Epoch: 300, loss after minibatch     8: 0.06112\n",
      "Epoch: 300, loss after minibatch     9: 0.06377\n",
      "Epoch: 300, loss after minibatch    10: 0.03980\n",
      "Epoch: 400, loss after minibatch     1: 0.05847\n",
      "Epoch: 400, loss after minibatch     2: 0.05367\n",
      "Epoch: 400, loss after minibatch     3: 0.05588\n",
      "Epoch: 400, loss after minibatch     4: 0.05959\n",
      "Epoch: 400, loss after minibatch     5: 0.05436\n",
      "Epoch: 400, loss after minibatch     6: 0.05091\n",
      "Epoch: 400, loss after minibatch     7: 0.06139\n",
      "Epoch: 400, loss after minibatch     8: 0.05951\n",
      "Epoch: 400, loss after minibatch     9: 0.06189\n",
      "Epoch: 400, loss after minibatch    10: 0.03802\n",
      "Epoch: 500, loss after minibatch     1: 0.05919\n",
      "Epoch: 500, loss after minibatch     2: 0.05347\n",
      "Epoch: 500, loss after minibatch     3: 0.06370\n",
      "Epoch: 500, loss after minibatch     4: 0.05764\n",
      "Epoch: 500, loss after minibatch     5: 0.05373\n",
      "Epoch: 500, loss after minibatch     6: 0.05079\n",
      "Epoch: 500, loss after minibatch     7: 0.06148\n",
      "Epoch: 500, loss after minibatch     8: 0.05823\n",
      "Epoch: 500, loss after minibatch     9: 0.06037\n",
      "Epoch: 500, loss after minibatch    10: 0.03665\n",
      "Epoch: 600, loss after minibatch     1: 0.05800\n",
      "Epoch: 600, loss after minibatch     2: 0.05250\n",
      "Epoch: 600, loss after minibatch     3: 0.05596\n",
      "Epoch: 600, loss after minibatch     4: 0.05563\n",
      "Epoch: 600, loss after minibatch     5: 0.05245\n",
      "Epoch: 600, loss after minibatch     6: 0.05003\n",
      "Epoch: 600, loss after minibatch     7: 0.06108\n",
      "Epoch: 600, loss after minibatch     8: 0.05716\n",
      "Epoch: 600, loss after minibatch     9: 0.06007\n",
      "Epoch: 600, loss after minibatch    10: 0.03592\n",
      "Report after 4 fold\n",
      "+-------------+--------------------+--------------------+--------------------+\n",
      "|     Tag     |     Precision      |       Recall       |       FBeta        |\n",
      "+-------------+--------------------+--------------------+--------------------+\n",
      "|    punct    | 0.3273356209870594 | 0.9954976303317535 | 0.3924021396802338 |\n",
      "|  publisher  | 0.8533333333333335 | 0.9199999999999999 | 0.8836363636363636 |\n",
      "|   location  | 0.8828571428571429 | 0.7777777777777778 | 0.8238390092879257 |\n",
      "|    author   | 0.7783932582059279 | 0.9680851063829786 | 0.7874588896478174 |\n",
      "| institution | 0.8057539682539684 | 0.9818181818181818 | 0.8768939974457215 |\n",
      "|     tech    |        1.0         | 0.8923076923076924 | 0.9344198947871831 |\n",
      "|    pages    | 0.7284848484848485 |        1.0         | 0.7754761904761904 |\n",
      "|    editor   |        0.0         |        0.0         |        0.0         |\n",
      "|    title    | 0.9512031321912747 | 0.9881818181818183 | 0.9693051474606559 |\n",
      "|  booktitle  | 0.9853173241852486 | 0.9961538461538462 | 0.9905460138869084 |\n",
      "|     date    | 0.9352452048243982 | 0.8918032786885247 | 0.9128073469342141 |\n",
      "|     note    |        0.8         |        0.6         | 0.6666666666666666 |\n",
      "|    volume   | 0.8547619047619047 | 0.9666666666666668 | 0.8978621378621379 |\n",
      "|    other    | 0.6780341718972129 | 0.9763250883392226 | 0.7362281985550625 |\n",
      "|   journal   | 0.9133333333333333 | 0.8181818181818181 | 0.862857142857143  |\n",
      "+-------------+--------------------+--------------------+--------------------+\n",
      "Epoch: 0, loss after minibatch     1: 0.05784\n",
      "Epoch: 0, loss after minibatch     2: 0.05298\n",
      "Epoch: 0, loss after minibatch     3: 0.05418\n",
      "Epoch: 0, loss after minibatch     4: 0.05571\n",
      "Epoch: 0, loss after minibatch     5: 0.05175\n",
      "Epoch: 0, loss after minibatch     6: 0.04830\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, loss after minibatch     7: 0.06104\n",
      "Epoch: 0, loss after minibatch     8: 0.05611\n",
      "Epoch: 0, loss after minibatch     9: 0.05972\n",
      "Epoch: 0, loss after minibatch    10: 0.03517\n",
      "Epoch: 100, loss after minibatch     1: 0.05533\n",
      "Epoch: 100, loss after minibatch     2: 0.05057\n",
      "Epoch: 100, loss after minibatch     3: 0.05288\n",
      "Epoch: 100, loss after minibatch     4: 0.05210\n",
      "Epoch: 100, loss after minibatch     5: 0.04922\n",
      "Epoch: 100, loss after minibatch     6: 0.04707\n",
      "Epoch: 100, loss after minibatch     7: 0.05727\n",
      "Epoch: 100, loss after minibatch     8: 0.05481\n",
      "Epoch: 100, loss after minibatch     9: 0.05851\n",
      "Epoch: 100, loss after minibatch    10: 0.03313\n",
      "Epoch: 200, loss after minibatch     1: 0.05487\n",
      "Epoch: 200, loss after minibatch     2: 0.05020\n",
      "Epoch: 200, loss after minibatch     3: 0.05202\n",
      "Epoch: 200, loss after minibatch     4: 0.05188\n",
      "Epoch: 200, loss after minibatch     5: 0.04829\n",
      "Epoch: 200, loss after minibatch     6: 0.04628\n",
      "Epoch: 200, loss after minibatch     7: 0.05688\n",
      "Epoch: 200, loss after minibatch     8: 0.05420\n",
      "Epoch: 200, loss after minibatch     9: 0.05629\n",
      "Epoch: 200, loss after minibatch    10: 0.03206\n",
      "Epoch: 300, loss after minibatch     1: 0.05167\n",
      "Epoch: 300, loss after minibatch     2: 0.04783\n",
      "Epoch: 300, loss after minibatch     3: 0.04913\n",
      "Epoch: 300, loss after minibatch     4: 0.04846\n",
      "Epoch: 300, loss after minibatch     5: 0.04618\n",
      "Epoch: 300, loss after minibatch     6: 0.04377\n",
      "Epoch: 300, loss after minibatch     7: 0.05399\n",
      "Epoch: 300, loss after minibatch     8: 0.05170\n",
      "Epoch: 300, loss after minibatch     9: 0.05467\n",
      "Epoch: 300, loss after minibatch    10: 0.03036\n",
      "Epoch: 400, loss after minibatch     1: 0.05055\n",
      "Epoch: 400, loss after minibatch     2: 0.04735\n",
      "Epoch: 400, loss after minibatch     3: 0.04915\n",
      "Epoch: 400, loss after minibatch     4: 0.04653\n",
      "Epoch: 400, loss after minibatch     5: 0.04456\n",
      "Epoch: 400, loss after minibatch     6: 0.04216\n",
      "Epoch: 400, loss after minibatch     7: 0.05296\n",
      "Epoch: 400, loss after minibatch     8: 0.05159\n",
      "Epoch: 400, loss after minibatch     9: 0.05468\n",
      "Epoch: 400, loss after minibatch    10: 0.02971\n",
      "Epoch: 500, loss after minibatch     1: 0.05004\n",
      "Epoch: 500, loss after minibatch     2: 0.04609\n",
      "Epoch: 500, loss after minibatch     3: 0.04755\n",
      "Epoch: 500, loss after minibatch     4: 0.04594\n",
      "Epoch: 500, loss after minibatch     5: 0.04423\n",
      "Epoch: 500, loss after minibatch     6: 0.04160\n",
      "Epoch: 500, loss after minibatch     7: 0.05259\n",
      "Epoch: 500, loss after minibatch     8: 0.04985\n",
      "Epoch: 500, loss after minibatch     9: 0.05298\n",
      "Epoch: 500, loss after minibatch    10: 0.02904\n",
      "Epoch: 600, loss after minibatch     1: 0.04869\n",
      "Epoch: 600, loss after minibatch     2: 0.04438\n",
      "Epoch: 600, loss after minibatch     3: 0.04603\n",
      "Epoch: 600, loss after minibatch     4: 0.04472\n",
      "Epoch: 600, loss after minibatch     5: 0.04340\n",
      "Epoch: 600, loss after minibatch     6: 0.03937\n",
      "Epoch: 600, loss after minibatch     7: 0.05033\n",
      "Epoch: 600, loss after minibatch     8: 0.04788\n",
      "Epoch: 600, loss after minibatch     9: 0.05150\n",
      "Epoch: 600, loss after minibatch    10: 0.02770\n",
      "Report after 5 fold\n",
      "+-------------+--------------------+--------------------+--------------------+\n",
      "|     Tag     |     Precision      |       Recall       |       FBeta        |\n",
      "+-------------+--------------------+--------------------+--------------------+\n",
      "|    punct    | 0.2828215649858847 | 0.9962480252764613 | 0.3459442375062187 |\n",
      "|  publisher  | 0.8777777777777779 | 0.9333333333333332 | 0.903030303030303  |\n",
      "|   location  | 0.9023809523809524 | 0.8148148148148149 | 0.8531991744066048 |\n",
      "|    author   | 0.8132000555971385 | 0.9712765957446807 | 0.8207547484653798 |\n",
      "| institution | 0.838128306878307  | 0.9848484848484849 | 0.8974116645381013 |\n",
      "|     tech    |        1.0         | 0.9102564102564102 | 0.9453499123226526 |\n",
      "|    pages    | 0.7737373737373737 |        1.0         | 0.8128968253968254 |\n",
      "|    editor   |        0.0         |        0.0         |        0.0         |\n",
      "|    title    | 0.9559877292070147 | 0.9897727272727274 | 0.9725440793403365 |\n",
      "|  booktitle  | 0.9877644368210405 | 0.9967948717948718 | 0.9921216782390904 |\n",
      "|     date    | 0.940482115131443  | 0.9016393442622951 | 0.920452403436914  |\n",
      "|     note    | 0.8333333333333334 | 0.6666666666666666 | 0.7222222222222222 |\n",
      "|    volume   | 0.8373015873015873 | 0.9722222222222223 | 0.891075591075591  |\n",
      "|    other    | 0.729343585340564  | 0.9782096584216725 | 0.7779836341298072 |\n",
      "|   journal   | 0.9277777777777777 | 0.8484848484848485 | 0.8857142857142858 |\n",
      "+-------------+--------------------+--------------------+--------------------+\n",
      "Epoch: 0, loss after minibatch     1: 0.04740\n",
      "Epoch: 0, loss after minibatch     2: 0.04411\n",
      "Epoch: 0, loss after minibatch     3: 0.04522\n",
      "Epoch: 0, loss after minibatch     4: 0.04322\n",
      "Epoch: 0, loss after minibatch     5: 0.04230\n",
      "Epoch: 0, loss after minibatch     6: 0.03867\n",
      "Epoch: 0, loss after minibatch     7: 0.04955\n",
      "Epoch: 0, loss after minibatch     8: 0.04698\n",
      "Epoch: 0, loss after minibatch     9: 0.04987\n",
      "Epoch: 0, loss after minibatch    10: 0.02718\n",
      "Epoch: 100, loss after minibatch     1: 0.04973\n",
      "Epoch: 100, loss after minibatch     2: 0.04392\n",
      "Epoch: 100, loss after minibatch     3: 0.04583\n",
      "Epoch: 100, loss after minibatch     4: 0.04490\n",
      "Epoch: 100, loss after minibatch     5: 0.04313\n",
      "Epoch: 100, loss after minibatch     6: 0.04002\n",
      "Epoch: 100, loss after minibatch     7: 0.05044\n",
      "Epoch: 100, loss after minibatch     8: 0.04833\n",
      "Epoch: 100, loss after minibatch     9: 0.05075\n",
      "Epoch: 100, loss after minibatch    10: 0.02772\n",
      "Epoch: 200, loss after minibatch     1: 0.04539\n",
      "Epoch: 200, loss after minibatch     2: 0.04242\n",
      "Epoch: 200, loss after minibatch     3: 0.04308\n",
      "Epoch: 200, loss after minibatch     4: 0.04141\n",
      "Epoch: 200, loss after minibatch     5: 0.04052\n",
      "Epoch: 200, loss after minibatch     6: 0.03673\n",
      "Epoch: 200, loss after minibatch     7: 0.04735\n",
      "Epoch: 200, loss after minibatch     8: 0.04465\n",
      "Epoch: 200, loss after minibatch     9: 0.04753\n",
      "Epoch: 200, loss after minibatch    10: 0.02524\n",
      "Epoch: 300, loss after minibatch     1: 0.04615\n",
      "Epoch: 300, loss after minibatch     2: 0.04281\n",
      "Epoch: 300, loss after minibatch     3: 0.04347\n",
      "Epoch: 300, loss after minibatch     4: 0.04138\n",
      "Epoch: 300, loss after minibatch     5: 0.04045\n",
      "Epoch: 300, loss after minibatch     6: 0.03726\n",
      "Epoch: 300, loss after minibatch     7: 0.04734\n",
      "Epoch: 300, loss after minibatch     8: 0.04544\n",
      "Epoch: 300, loss after minibatch     9: 0.04885\n",
      "Epoch: 300, loss after minibatch    10: 0.02538\n",
      "Epoch: 400, loss after minibatch     1: 0.06288\n",
      "Epoch: 400, loss after minibatch     2: 0.05751\n",
      "Epoch: 400, loss after minibatch     3: 0.05951\n",
      "Epoch: 400, loss after minibatch     4: 0.05886\n",
      "Epoch: 400, loss after minibatch     5: 0.05175\n",
      "Epoch: 400, loss after minibatch     6: 0.05047\n",
      "Epoch: 400, loss after minibatch     7: 0.06172\n",
      "Epoch: 400, loss after minibatch     8: 0.05805\n",
      "Epoch: 400, loss after minibatch     9: 0.07414\n",
      "Epoch: 400, loss after minibatch    10: 0.06877\n",
      "Epoch: 500, loss after minibatch     1: 0.04407\n",
      "Epoch: 500, loss after minibatch     2: 0.04096\n",
      "Epoch: 500, loss after minibatch     3: 0.04079\n",
      "Epoch: 500, loss after minibatch     4: 0.03984\n",
      "Epoch: 500, loss after minibatch     5: 0.03895\n",
      "Epoch: 500, loss after minibatch     6: 0.03520\n",
      "Epoch: 500, loss after minibatch     7: 0.04502\n",
      "Epoch: 500, loss after minibatch     8: 0.04301\n",
      "Epoch: 500, loss after minibatch     9: 0.04574\n",
      "Epoch: 500, loss after minibatch    10: 0.02395\n",
      "Epoch: 600, loss after minibatch     1: 0.06162\n",
      "Epoch: 600, loss after minibatch     2: 0.07083\n",
      "Epoch: 600, loss after minibatch     3: 0.05560\n",
      "Epoch: 600, loss after minibatch     4: 0.05869\n",
      "Epoch: 600, loss after minibatch     5: 0.04960\n",
      "Epoch: 600, loss after minibatch     6: 0.04823\n",
      "Epoch: 600, loss after minibatch     7: 0.05961\n",
      "Epoch: 600, loss after minibatch     8: 0.05534\n",
      "Epoch: 600, loss after minibatch     9: 0.06076\n",
      "Epoch: 600, loss after minibatch    10: 0.03400\n",
      "Report after 6 fold\n",
      "+-------------+---------------------+--------------------+--------------------+\n",
      "|     Tag     |      Precision      |       Recall       |       FBeta        |\n",
      "+-------------+---------------------+--------------------+--------------------+\n",
      "|    punct    | 0.25102089784178244 | 0.9967840216655383 | 0.3127512806773165 |\n",
      "|  publisher  |  0.8952380952380954 | 0.9428571428571428 | 0.9168831168831169 |\n",
      "|   location  |  0.9163265306122449 | 0.8412698412698413 | 0.874170720919947  |\n",
      "|    author   |  0.8380774436763899 | 0.974772036474164  | 0.8451505592172988 |\n",
      "| institution |  0.8612528344671203 | 0.987012987012987  | 0.9120671410326582 |\n",
      "|     tech    |         1.0         | 0.9230769230769231 | 0.9531570677051308 |\n",
      "|    pages    |  0.806060606060606  |        1.0         | 0.839625850340136  |\n",
      "|    editor   |         0.0         |        0.0         |        0.0         |\n",
      "|    title    |  0.9609852899813754 | 0.9909090909090911 | 0.9756574231265333 |\n",
      "|  booktitle  |  0.9895123744180347 | 0.9972527472527473 | 0.9932471527763632 |\n",
      "|     date    |  0.946603717731713  | 0.9110070257611242 | 0.9282744331702474 |\n",
      "|     note    |  0.8571428571428571 | 0.7142857142857143 | 0.7619047619047619 |\n",
      "|    volume   |  0.8605442176870748 | 0.9761904761904763 | 0.9066362209219351 |\n",
      "|    other    |  0.7677536853939528 | 0.9795557799091368 | 0.8086852870059601 |\n",
      "|   journal   |  0.9380952380952381 | 0.8701298701298701 | 0.9020408163265307 |\n",
      "+-------------+---------------------+--------------------+--------------------+\n",
      "Epoch: 0, loss after minibatch     1: 0.04237\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, loss after minibatch     2: 0.03965\n",
      "Epoch: 0, loss after minibatch     3: 0.03920\n",
      "Epoch: 0, loss after minibatch     4: 0.03808\n",
      "Epoch: 0, loss after minibatch     5: 0.03678\n",
      "Epoch: 0, loss after minibatch     6: 0.03291\n",
      "Epoch: 0, loss after minibatch     7: 0.04331\n",
      "Epoch: 0, loss after minibatch     8: 0.04102\n",
      "Epoch: 0, loss after minibatch     9: 0.04445\n",
      "Epoch: 0, loss after minibatch    10: 0.02250\n",
      "Epoch: 100, loss after minibatch     1: 0.04159\n",
      "Epoch: 100, loss after minibatch     2: 0.03875\n",
      "Epoch: 100, loss after minibatch     3: 0.03812\n",
      "Epoch: 100, loss after minibatch     4: 0.03691\n",
      "Epoch: 100, loss after minibatch     5: 0.03593\n",
      "Epoch: 100, loss after minibatch     6: 0.03249\n",
      "Epoch: 100, loss after minibatch     7: 0.04275\n",
      "Epoch: 100, loss after minibatch     8: 0.03994\n",
      "Epoch: 100, loss after minibatch     9: 0.04323\n",
      "Epoch: 100, loss after minibatch    10: 0.02204\n",
      "Epoch: 200, loss after minibatch     1: 0.04273\n",
      "Epoch: 200, loss after minibatch     2: 0.03892\n",
      "Epoch: 200, loss after minibatch     3: 0.03906\n",
      "Epoch: 200, loss after minibatch     4: 0.03734\n",
      "Epoch: 200, loss after minibatch     5: 0.03682\n",
      "Epoch: 200, loss after minibatch     6: 0.03312\n",
      "Epoch: 200, loss after minibatch     7: 0.04289\n",
      "Epoch: 200, loss after minibatch     8: 0.04100\n",
      "Epoch: 200, loss after minibatch     9: 0.04400\n",
      "Epoch: 200, loss after minibatch    10: 0.02242\n",
      "Epoch: 300, loss after minibatch     1: 0.04037\n",
      "Epoch: 300, loss after minibatch     2: 0.03740\n",
      "Epoch: 300, loss after minibatch     3: 0.03680\n",
      "Epoch: 300, loss after minibatch     4: 0.03553\n",
      "Epoch: 300, loss after minibatch     5: 0.03469\n",
      "Epoch: 300, loss after minibatch     6: 0.03138\n",
      "Epoch: 300, loss after minibatch     7: 0.04146\n",
      "Epoch: 300, loss after minibatch     8: 0.03894\n",
      "Epoch: 300, loss after minibatch     9: 0.04154\n",
      "Epoch: 300, loss after minibatch    10: 0.02121\n",
      "Epoch: 400, loss after minibatch     1: 0.04086\n",
      "Epoch: 400, loss after minibatch     2: 0.03709\n",
      "Epoch: 400, loss after minibatch     3: 0.03723\n",
      "Epoch: 400, loss after minibatch     4: 0.03644\n",
      "Epoch: 400, loss after minibatch     5: 0.03458\n",
      "Epoch: 400, loss after minibatch     6: 0.03250\n",
      "Epoch: 400, loss after minibatch     7: 0.04217\n",
      "Epoch: 400, loss after minibatch     8: 0.03928\n",
      "Epoch: 400, loss after minibatch     9: 0.04171\n",
      "Epoch: 400, loss after minibatch    10: 0.02159\n",
      "Epoch: 500, loss after minibatch     1: 0.03976\n",
      "Epoch: 500, loss after minibatch     2: 0.03649\n",
      "Epoch: 500, loss after minibatch     3: 0.03633\n",
      "Epoch: 500, loss after minibatch     4: 0.03534\n",
      "Epoch: 500, loss after minibatch     5: 0.03383\n",
      "Epoch: 500, loss after minibatch     6: 0.03143\n",
      "Epoch: 500, loss after minibatch     7: 0.04083\n",
      "Epoch: 500, loss after minibatch     8: 0.03820\n",
      "Epoch: 500, loss after minibatch     9: 0.04043\n",
      "Epoch: 500, loss after minibatch    10: 0.02105\n",
      "Epoch: 600, loss after minibatch     1: 0.03861\n",
      "Epoch: 600, loss after minibatch     2: 0.03577\n",
      "Epoch: 600, loss after minibatch     3: 0.03531\n",
      "Epoch: 600, loss after minibatch     4: 0.03408\n",
      "Epoch: 600, loss after minibatch     5: 0.03307\n",
      "Epoch: 600, loss after minibatch     6: 0.03017\n",
      "Epoch: 600, loss after minibatch     7: 0.03993\n",
      "Epoch: 600, loss after minibatch     8: 0.03726\n",
      "Epoch: 600, loss after minibatch     9: 0.03958\n",
      "Epoch: 600, loss after minibatch    10: 0.02028\n",
      "Report after 7 fold\n",
      "+-------------+--------------------+--------------------+--------------------+\n",
      "|     Tag     |     Precision      |       Recall       |       FBeta        |\n",
      "+-------------+--------------------+--------------------+--------------------+\n",
      "|    punct    | 0.2271714717111743 | 0.9971860189573459 | 0.287858474375126  |\n",
      "|  publisher  | 0.9083333333333334 |        0.95        | 0.9272727272727272 |\n",
      "|   location  | 0.9267857142857143 | 0.8611111111111112 | 0.8898993808049536 |\n",
      "|    author   | 0.856996410151302  | 0.9773936170212765 | 0.8635788496014567 |\n",
      "| institution | 0.8785962301587302 | 0.9886363636363636 | 0.9230587484035759 |\n",
      "|     tech    |        1.0         | 0.9326923076923077 | 0.9590124342419895 |\n",
      "|    pages    | 0.8303030303030303 |        1.0         | 0.859672619047619  |\n",
      "|    editor   |        0.0         |        0.0         |        0.0         |\n",
      "|    title    | 0.964457634351681  | 0.9920454545454547 | 0.9779940305464511 |\n",
      "|  booktitle  | 0.9908233276157804 | 0.9975961538461539 | 0.9940912586793178 |\n",
      "|     date    | 0.9532782530152489 | 0.9159836065573771 | 0.9340888685197648 |\n",
      "|     note    |       0.875        |        0.75        | 0.7916666666666666 |\n",
      "|    volume   | 0.8779761904761905 | 0.9791666666666667 | 0.9183066933066932 |\n",
      "|    other    | 0.7965616583204217 | 0.9807862190812721 | 0.8318232286146872 |\n",
      "|   journal   | 0.9458333333333333 | 0.8863636363636364 | 0.9142857142857144 |\n",
      "+-------------+--------------------+--------------------+--------------------+\n",
      "Epoch: 0, loss after minibatch     1: 0.04072\n",
      "Epoch: 0, loss after minibatch     2: 0.03757\n",
      "Epoch: 0, loss after minibatch     3: 0.03732\n",
      "Epoch: 0, loss after minibatch     4: 0.03580\n",
      "Epoch: 0, loss after minibatch     5: 0.03452\n",
      "Epoch: 0, loss after minibatch     6: 0.03122\n",
      "Epoch: 0, loss after minibatch     7: 0.04063\n",
      "Epoch: 0, loss after minibatch     8: 0.03903\n",
      "Epoch: 0, loss after minibatch     9: 0.04239\n",
      "Epoch: 0, loss after minibatch    10: 0.02156\n",
      "Epoch: 100, loss after minibatch     1: 0.03832\n",
      "Epoch: 100, loss after minibatch     2: 0.03544\n",
      "Epoch: 100, loss after minibatch     3: 0.03496\n",
      "Epoch: 100, loss after minibatch     4: 0.03355\n",
      "Epoch: 100, loss after minibatch     5: 0.03282\n",
      "Epoch: 100, loss after minibatch     6: 0.02959\n",
      "Epoch: 100, loss after minibatch     7: 0.03894\n",
      "Epoch: 100, loss after minibatch     8: 0.03684\n",
      "Epoch: 100, loss after minibatch     9: 0.03941\n",
      "Epoch: 100, loss after minibatch    10: 0.02016\n",
      "Epoch: 200, loss after minibatch     1: 0.03846\n",
      "Epoch: 200, loss after minibatch     2: 0.03562\n",
      "Epoch: 200, loss after minibatch     3: 0.03493\n",
      "Epoch: 200, loss after minibatch     4: 0.03377\n",
      "Epoch: 200, loss after minibatch     5: 0.03356\n",
      "Epoch: 200, loss after minibatch     6: 0.02992\n",
      "Epoch: 200, loss after minibatch     7: 0.03944\n",
      "Epoch: 200, loss after minibatch     8: 0.03698\n",
      "Epoch: 200, loss after minibatch     9: 0.04197\n",
      "Epoch: 200, loss after minibatch    10: 0.02061\n",
      "Epoch: 300, loss after minibatch     1: 0.03747\n",
      "Epoch: 300, loss after minibatch     2: 0.03477\n",
      "Epoch: 300, loss after minibatch     3: 0.03412\n",
      "Epoch: 300, loss after minibatch     4: 0.03302\n",
      "Epoch: 300, loss after minibatch     5: 0.03230\n",
      "Epoch: 300, loss after minibatch     6: 0.02967\n",
      "Epoch: 300, loss after minibatch     7: 0.03820\n",
      "Epoch: 300, loss after minibatch     8: 0.03617\n",
      "Epoch: 300, loss after minibatch     9: 0.03947\n",
      "Epoch: 300, loss after minibatch    10: 0.01952\n",
      "Epoch: 400, loss after minibatch     1: 0.05299\n",
      "Epoch: 400, loss after minibatch     2: 0.03988\n",
      "Epoch: 400, loss after minibatch     3: 0.04316\n",
      "Epoch: 400, loss after minibatch     4: 0.04271\n",
      "Epoch: 400, loss after minibatch     5: 0.04080\n",
      "Epoch: 400, loss after minibatch     6: 0.03865\n",
      "Epoch: 400, loss after minibatch     7: 0.04566\n",
      "Epoch: 400, loss after minibatch     8: 0.04428\n",
      "Epoch: 400, loss after minibatch     9: 0.04593\n",
      "Epoch: 400, loss after minibatch    10: 0.02457\n",
      "Epoch: 500, loss after minibatch     1: 0.03674\n",
      "Epoch: 500, loss after minibatch     2: 0.03309\n",
      "Epoch: 500, loss after minibatch     3: 0.03290\n",
      "Epoch: 500, loss after minibatch     4: 0.03213\n",
      "Epoch: 500, loss after minibatch     5: 0.03163\n",
      "Epoch: 500, loss after minibatch     6: 0.02783\n",
      "Epoch: 500, loss after minibatch     7: 0.03682\n",
      "Epoch: 500, loss after minibatch     8: 0.03498\n",
      "Epoch: 500, loss after minibatch     9: 0.03654\n",
      "Epoch: 500, loss after minibatch    10: 0.01891\n",
      "Epoch: 600, loss after minibatch     1: 0.03857\n",
      "Epoch: 600, loss after minibatch     2: 0.03477\n",
      "Epoch: 600, loss after minibatch     3: 0.03550\n",
      "Epoch: 600, loss after minibatch     4: 0.03394\n",
      "Epoch: 600, loss after minibatch     5: 0.03396\n",
      "Epoch: 600, loss after minibatch     6: 0.03166\n",
      "Epoch: 600, loss after minibatch     7: 0.03881\n",
      "Epoch: 600, loss after minibatch     8: 0.03755\n",
      "Epoch: 600, loss after minibatch     9: 0.03938\n",
      "Epoch: 600, loss after minibatch    10: 0.01987\n",
      "Report after 8 fold\n",
      "+-------------+---------------------+--------------------+--------------------+\n",
      "|     Tag     |      Precision      |       Recall       |       FBeta        |\n",
      "+-------------+---------------------+--------------------+--------------------+\n",
      "|    punct    | 0.20868165695980873 | 0.9974986835176409 | 0.2686036385347937 |\n",
      "|  publisher  |  0.9185185185185186 | 0.9555555555555555 | 0.9353535353535354 |\n",
      "|   location  |  0.9349206349206349 | 0.8765432098765432 | 0.9021327829377365 |\n",
      "|    author   |  0.8717111618540114 | 0.9794326241134752 | 0.8779119643446907 |\n",
      "| institution |  0.8920855379188714 | 0.9898989898989901 | 0.9316077763587342 |\n",
      "|     tech    |         1.0         | 0.9401709401709402 | 0.9635666082151018 |\n",
      "|    pages    |  0.8491582491582492 |        1.0         | 0.8752645502645503 |\n",
      "|    editor   |         0.0         |        0.0         |        0.0         |\n",
      "|    title    |  0.967403525493443  | 0.9926767676767678 | 0.9798099700239248 |\n",
      "|  booktitle  |  0.9918429578806938 | 0.9957264957264957 | 0.9936690368411628 |\n",
      "|     date    |  0.8837782002110854 | 0.9216757741347905 | 0.884704167130524  |\n",
      "|     note    |  0.8888888888888888 | 0.7777777777777778 | 0.8148148148148148 |\n",
      "|    volume   |  0.8915343915343915 | 0.9814814814814815 | 0.9273837273837273 |\n",
      "|    other    |  0.818967859485453  | 0.9817432273262663 | 0.8498194054214748 |\n",
      "|   journal   |  0.9280423280423281 | 0.8989898989898989 | 0.9104761904761907 |\n",
      "+-------------+---------------------+--------------------+--------------------+\n",
      "Epoch: 0, loss after minibatch     1: 0.04788\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, loss after minibatch     2: 0.03661\n",
      "Epoch: 0, loss after minibatch     3: 0.03591\n",
      "Epoch: 0, loss after minibatch     4: 0.03478\n",
      "Epoch: 0, loss after minibatch     5: 0.03604\n",
      "Epoch: 0, loss after minibatch     6: 0.03391\n",
      "Epoch: 0, loss after minibatch     7: 0.04049\n",
      "Epoch: 0, loss after minibatch     8: 0.03811\n",
      "Epoch: 0, loss after minibatch     9: 0.04050\n",
      "Epoch: 0, loss after minibatch    10: 0.02260\n",
      "Epoch: 100, loss after minibatch     1: 0.03504\n",
      "Epoch: 100, loss after minibatch     2: 0.03201\n",
      "Epoch: 100, loss after minibatch     3: 0.03203\n",
      "Epoch: 100, loss after minibatch     4: 0.03107\n",
      "Epoch: 100, loss after minibatch     5: 0.03074\n",
      "Epoch: 100, loss after minibatch     6: 0.02674\n",
      "Epoch: 100, loss after minibatch     7: 0.03569\n",
      "Epoch: 100, loss after minibatch     8: 0.03363\n",
      "Epoch: 100, loss after minibatch     9: 0.03513\n",
      "Epoch: 100, loss after minibatch    10: 0.01799\n",
      "Epoch: 200, loss after minibatch     1: 0.03429\n",
      "Epoch: 200, loss after minibatch     2: 0.03099\n",
      "Epoch: 200, loss after minibatch     3: 0.03125\n",
      "Epoch: 200, loss after minibatch     4: 0.03058\n",
      "Epoch: 200, loss after minibatch     5: 0.03005\n",
      "Epoch: 200, loss after minibatch     6: 0.02600\n",
      "Epoch: 200, loss after minibatch     7: 0.03519\n",
      "Epoch: 200, loss after minibatch     8: 0.03259\n",
      "Epoch: 200, loss after minibatch     9: 0.03552\n",
      "Epoch: 200, loss after minibatch    10: 0.01744\n",
      "Epoch: 300, loss after minibatch     1: 0.03347\n",
      "Epoch: 300, loss after minibatch     2: 0.03064\n",
      "Epoch: 300, loss after minibatch     3: 0.03055\n",
      "Epoch: 300, loss after minibatch     4: 0.02950\n",
      "Epoch: 300, loss after minibatch     5: 0.02948\n",
      "Epoch: 300, loss after minibatch     6: 0.02527\n",
      "Epoch: 300, loss after minibatch     7: 0.03414\n",
      "Epoch: 300, loss after minibatch     8: 0.03225\n",
      "Epoch: 300, loss after minibatch     9: 0.03330\n",
      "Epoch: 300, loss after minibatch    10: 0.01695\n",
      "Epoch: 400, loss after minibatch     1: 0.03453\n",
      "Epoch: 400, loss after minibatch     2: 0.03072\n",
      "Epoch: 400, loss after minibatch     3: 0.03085\n",
      "Epoch: 400, loss after minibatch     4: 0.03026\n",
      "Epoch: 400, loss after minibatch     5: 0.03034\n",
      "Epoch: 400, loss after minibatch     6: 0.02594\n",
      "Epoch: 400, loss after minibatch     7: 0.03531\n",
      "Epoch: 400, loss after minibatch     8: 0.03267\n",
      "Epoch: 400, loss after minibatch     9: 0.03433\n",
      "Epoch: 400, loss after minibatch    10: 0.01721\n",
      "Epoch: 500, loss after minibatch     1: 0.03266\n",
      "Epoch: 500, loss after minibatch     2: 0.02982\n",
      "Epoch: 500, loss after minibatch     3: 0.02961\n",
      "Epoch: 500, loss after minibatch     4: 0.02878\n",
      "Epoch: 500, loss after minibatch     5: 0.02874\n",
      "Epoch: 500, loss after minibatch     6: 0.02454\n",
      "Epoch: 500, loss after minibatch     7: 0.03332\n",
      "Epoch: 500, loss after minibatch     8: 0.03133\n",
      "Epoch: 500, loss after minibatch     9: 0.03228\n",
      "Epoch: 500, loss after minibatch    10: 0.01633\n",
      "Epoch: 600, loss after minibatch     1: 0.03354\n",
      "Epoch: 600, loss after minibatch     2: 0.03082\n",
      "Epoch: 600, loss after minibatch     3: 0.03033\n",
      "Epoch: 600, loss after minibatch     4: 0.02901\n",
      "Epoch: 600, loss after minibatch     5: 0.02877\n",
      "Epoch: 600, loss after minibatch     6: 0.02525\n",
      "Epoch: 600, loss after minibatch     7: 0.03357\n",
      "Epoch: 600, loss after minibatch     8: 0.03198\n",
      "Epoch: 600, loss after minibatch     9: 0.03472\n",
      "Epoch: 600, loss after minibatch    10: 0.01682\n",
      "Report after 9 fold\n",
      "+-------------+--------------------+--------------------+--------------------+\n",
      "|     Tag     |     Precision      |       Recall       |       FBeta        |\n",
      "+-------------+--------------------+--------------------+--------------------+\n",
      "|    punct    | 0.1938893677306049 | 0.9977488151658769 | 0.2531989923603099 |\n",
      "|  publisher  | 0.9266666666666667 |        0.96        | 0.9418181818181818 |\n",
      "|   location  | 0.9414285714285715 | 0.888888888888889  | 0.9119195046439629 |\n",
      "|    author   | 0.8836925880414916 | 0.981063829787234  | 0.8894838252350624 |\n",
      "| institution | 0.9028769841269841 | 0.990909090909091  | 0.9384469987228607 |\n",
      "|     tech    |        1.0         | 0.9461538461538461 | 0.9672099473935916 |\n",
      "|    pages    | 0.8642424242424243 |        1.0         | 0.8877380952380953 |\n",
      "|    editor   |        0.0         |        0.0         |        0.0         |\n",
      "|    title    | 0.9699859720411641 | 0.993409090909091  | 0.9814892221721552 |\n",
      "|  booktitle  | 0.9926586620926244 | 0.9961538461538462 | 0.9943021331570465 |\n",
      "|     date    | 0.8287337135233102 | 0.9278688524590164 | 0.8460262815378036 |\n",
      "|     note    |        0.9         |        0.8         | 0.8333333333333333 |\n",
      "|    volume   | 0.9023809523809524 | 0.9833333333333334 | 0.9346453546453546 |\n",
      "|    other    | 0.8368931375938473 | 0.9826855123674912 | 0.8643055499857104 |\n",
      "|   journal   |  0.91985347985348  | 0.909090909090909  | 0.9110952380952382 |\n",
      "+-------------+--------------------+--------------------+--------------------+\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'evaluate' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-c296feba1261>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[0;31m# run(training_set, test_set)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_set\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconcat_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 121\u001b[0;31m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_set\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'evaluate' is not defined"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from torch.nn.utils.rnn import pad_packed_sequence\n",
    "from sklearn_hierarchical_classification.classifier import HierarchicalClassifier\n",
    "from sklearn_hierarchical_classification.constants import ROOT\n",
    "from sklearn_hierarchical_classification.metrics import h_fbeta_score, multi_labeled\n",
    "from sklearn.metrics import f1_score, precision_recall_fscore_support\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from prettytable import PrettyTable\n",
    "from torch.utils.data import TensorDataset, DataLoader, ConcatDataset\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "X_train, X_test = torch.tensor(X_train), torch.tensor(X_test)\n",
    "y_train, y_test = torch.tensor(y_train), torch.tensor(y_test)\n",
    "\n",
    "def categorical_accuracy(outputs, y, pad_index):\n",
    "    max_outputs = outputs.argmax(dim = 1, keepdim=True)\n",
    "    non_padded_elements = (y != pad_index).nonzero()\n",
    "    correct = max_outputs[non_padded_elements].squeeze(1).eq(y[non_padded_elements])\n",
    "    return correct.sum() / torch.FloatTensor([y[non_padded_elements].shape[0]])\n",
    "\n",
    "def get_max_outputs(outputs):\n",
    "    max_outputs = outputs.argmax(dim = 1)\n",
    "    return max_outputs\n",
    "\n",
    "def print_report(report):\n",
    "    table = PrettyTable(float_format=\"1.5f\")\n",
    "    table.field_names = [\"Tag\", \"Precision\", \"Recall\", \"FBeta\"]\n",
    "    for i in range(len(tag_arr)):\n",
    "      tag, scores = [tag_arr[i]], list(map(lambda metric: metric[i], report))[:-1] # exclude support metric\n",
    "      tag.extend(scores)\n",
    "      table.add_row(tag)\n",
    "    print(table)\n",
    "\n",
    "def sum_report(report1, report2):\n",
    "    summed_report = []\n",
    "    for i in range(len(report1)):\n",
    "        label_report1, label_report2 = np.array(report1[i]), np.array(report2[i])\n",
    "        summed_report.append(np.add(label_report1, label_report2))\n",
    "    return np.array(summed_report)\n",
    "\n",
    "def average_report(report, num):\n",
    "    return list(map(lambda label_report: np.true_divide(label_report, num), report))\n",
    "\n",
    "def train(train_dataset, concat_dataset, k_folds=10):\n",
    "    kfold = KFold(n_splits=k_folds, shuffle=True)\n",
    "    summed_report = np.array([])\n",
    "    for fold, (train_ids, test_ids) in enumerate(kfold.split(concat_dataset)):\n",
    "        train_loader = DataLoader(train_dataset, batch_size=256)\n",
    "        for epoch in range(num_epochs):\n",
    "            for i, data in enumerate(train_loader, 0):\n",
    "                X_train, y_train = data\n",
    "                outputs = model.forward(X_train)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                outputs = outputs.view(-1, outputs.shape[-1]) # [batch_size * seq_len, output_dim]\n",
    "                y_train = y_train.view(-1) # [batch_size * seq_len]\n",
    "              \n",
    "                # Get the loss function\n",
    "                loss = criterion(outputs, y_train.long())\n",
    "\n",
    "                # Calculate loss\n",
    "                loss.backward()\n",
    "\n",
    "                # Backpropagation\n",
    "                optimizer.step()\n",
    "\n",
    "                # Print loss at every 100th epoch\n",
    "                if epoch % 100 == 0:\n",
    "                    print(\"Epoch: %d, loss after minibatch %5d: %1.5f\" % (epoch, i+1, loss.item()))\n",
    "            \n",
    "        report = precision_recall_fscore_support(y_train.long(), \\\n",
    "                                                 get_max_outputs(outputs.detach()), \\\n",
    "                                                 average=None, \\\n",
    "                                                 zero_division=0, \\\n",
    "                                                 labels = [i for i in range(len(all_tags))])\n",
    "        if len(summed_report) == 0:\n",
    "            summed_report = np.array(report)\n",
    "        else:\n",
    "            summed_report = sum_report(summed_report, report)\n",
    "        print(\"Report after %d fold\" % (fold))\n",
    "        averaged_report = average_report(summed_report, fold + 1)\n",
    "        print_report(averaged_report)  \n",
    "\n",
    "def test(test_set):\n",
    "    test_loader = DataLoader(test_dataset, batch_size=256)\n",
    "    with torch.no_grad():\n",
    "        summed_report = []\n",
    "        for i, data in enumerate(test_loader, 0):\n",
    "              X_test, y_test = data\n",
    "              outputs = model.forward(X_test)\n",
    "\n",
    "              outputs = outputs.view(-1, outputs.shape[-1]) # [batch_size * seq_len, output_dim]\n",
    "              y_test = y_test.view(-1) # [batch_size * seq_len]\n",
    "            \n",
    "              # Get the loss function\n",
    "              loss = criterion(outputs, y_test.long())\n",
    "\n",
    "              report = precision_recall_fscore_support(y_test.long(), \\\n",
    "                                                       get_max_outputs(outputs.detach()), \\\n",
    "                                                       average=None, \\\n",
    "                                                       zero_division=0, \\\n",
    "                                                       labels = [i for i in range(len(all_tags))])\n",
    "              if len(summed_report) == 0:\n",
    "                  summed_report = np.array(report)\n",
    "              else:\n",
    "                  summed_report = sum_report(summed_report, report)\n",
    "        averaged_report = average_report(summed_report, i + 1)\n",
    "        print_report(averaged_report)  \n",
    "          \n",
    "\n",
    "training_set = TensorDataset(X_train, y_train)\n",
    "test_set = TensorDataset(X_test, y_test)\n",
    "concat_dataset = ConcatDataset([training_set, test_set])\n",
    "# run(training_set, test_set)\n",
    "train(training_set, concat_dataset)\n",
    "evaluate(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "reference_parsing.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
