{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "reference_parsing.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5-iCVx308906",
        "outputId": "b51d72da-284e-4d81-b5d8-017c0f60aec9"
      },
      "source": [
        "!pip install nltk\n",
        "!pip install torch\n",
        "!pip install fasttext"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (3.2.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk) (1.15.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (1.8.0+cu101)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch) (1.19.5)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch) (3.7.4.3)\n",
            "Collecting fasttext\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f8/85/e2b368ab6d3528827b147fdb814f8189acc981a4bc2f99ab894650e05c40/fasttext-0.9.2.tar.gz (68kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 4.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: pybind11>=2.2 in /usr/local/lib/python3.7/dist-packages (from fasttext) (2.6.2)\n",
            "Requirement already satisfied: setuptools>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from fasttext) (54.1.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from fasttext) (1.19.5)\n",
            "Building wheels for collected packages: fasttext\n",
            "  Building wheel for fasttext (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fasttext: filename=fasttext-0.9.2-cp37-cp37m-linux_x86_64.whl size=3098126 sha256=21114c61c5b924ea70648cc65ee0750e83f3fb3a6273668998c188cdb90e52bb\n",
            "  Stored in directory: /root/.cache/pip/wheels/98/ba/7f/b154944a1cf5a8cee91c154b75231136cc3a3321ab0e30f592\n",
            "Successfully built fasttext\n",
            "Installing collected packages: fasttext\n",
            "Successfully installed fasttext-0.9.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "arhXbfrS9B5y"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uAtjFYuZ9255",
        "outputId": "745d7592-ec7b-4745-8dcd-ba309416cc92"
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "  "
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "17B1O7A89CcB"
      },
      "source": [
        "''' \n",
        "Preprocess Data\n",
        "'''\n",
        "\n",
        "import re\n",
        "import nltk\n",
        "import string\n",
        "import fasttext\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from nltk import word_tokenize\n",
        "\n",
        "OTHER_TAG = \"other\"\n",
        "PUNCT_TAG = \"punct\"\n",
        "\n",
        "with open('./utils/tags.txt', encoding=\"utf-8\", errors='ignore') as f:\n",
        "    tags = set([str.rstrip(tag) for tag in f.readlines()])\n",
        "\n",
        "def remove_labels(text):\n",
        "    return re.sub(r'\\<\\/?[\\w-]*\\>\\s*', \"\", text).strip()\n",
        "\n",
        "def tag_token(token, tag):\n",
        "    if token in string.punctuation:\n",
        "        return (token, PUNCT_TAG)\n",
        "    return (token, tag)\n",
        "\n",
        "def get_tagged_tokens(groups):\n",
        "    tagged_tokens = []\n",
        "    for group in groups:\n",
        "        ref, tag = group[0], group[1]\n",
        "        if tag not in tags:\n",
        "            tag = OTHER_TAG\n",
        "        unlabelled = remove_labels(ref)\n",
        "        tokens = word_tokenize(unlabelled)\n",
        "        tagged_tokens.extend(list(map(lambda token: tag_token(token, tag), tokens)))\n",
        "    return tagged_tokens\n",
        "\n",
        "''' Attach tags to each token '''\n",
        "def attach_tags(dataset_path):\n",
        "    dataset = []\n",
        "    with open(dataset_path, encoding=\"utf-8\", errors='ignore') as f:\n",
        "        refs = f.readlines()\n",
        "        for ref in refs:\n",
        "            groups = re.findall(r'(\\<(.*)\\>.*\\<\\/\\2\\>)', ref) # format (<tag>...</tag>, tag)\n",
        "            tagged_tokens = get_tagged_tokens(groups)\n",
        "            dataset.append(tagged_tokens)\n",
        "    return dataset\n",
        "\n",
        "''' Removes labels and tokenizes '''\n",
        "def tokenized_dataset(dataset_path, sep=\" \"):\n",
        "    dataset = []\n",
        "    with open(dataset_path, encoding=\"utf-8\", errors='ignore') as f:\n",
        "        refs = f.readlines()\n",
        "        for ref in refs:\n",
        "            ref = remove_labels(ref) \n",
        "            tokenized = \" \".join(word_tokenize(ref))\n",
        "            dataset.append(tokenized)\n",
        "    return dataset\n",
        "\n",
        "def train_word_embedding_model(dataset_path, embedding_dim, retrain=True):\n",
        "    embedding_dataset_path = './dataset/word_embedding_dataset'\n",
        "    model_path = './models/word_embedding.bin'\n",
        "\n",
        "    if retrain:\n",
        "        with open(embedding_dataset_path, 'w', errors='ignore') as f:\n",
        "            # fasttext tokenizes by whitespaces\n",
        "            word_embedding_dataset = tokenized_dataset(dataset_path, sep=\" \") \n",
        "            f.write(\"\\n\".join(word_embedding_dataset))\n",
        "        model = fasttext.train_unsupervised(embedding_dataset_path, dim=embedding_dim)\n",
        "        model.save_model(model_path)\n",
        "        return model\n",
        "    else:\n",
        "        return fasttext.load_model(model_path)\n",
        "\n",
        "def map_to_index(keys, idx_start=0):\n",
        "    idx_map, idx = {}, idx_start\n",
        "    for key in keys:\n",
        "        idx_map[key] = idx\n",
        "        idx += 1\n",
        "    return idx_map\n",
        "\n",
        "dataset_path = './dataset/standardized_dataset.txt'\n",
        "dataset = attach_tags(dataset_path)\n",
        "EMBEDDING_DIM = 100\n",
        "word_embedding_model = train_word_embedding_model(dataset_path, embedding_dim = EMBEDDING_DIM, retrain=True)\n",
        "\n",
        "all_tags = tags \n",
        "all_tags.add(OTHER_TAG)\n",
        "all_tags.add(PUNCT_TAG)\n",
        "tag_to_idx = map_to_index(all_tags)\n",
        "\n",
        "X, y = [], []\n",
        "for ref in dataset:\n",
        "    X_ref, y_ref = [], []\n",
        "    for token, tag in ref:\n",
        "        X_ref.append(word_embedding_model.get_word_vector(token))\n",
        "        y_ref.append(tag_to_idx[tag])\n",
        "    X.append(X_ref)\n",
        "    y.append(y_ref)\n",
        "\n",
        "max_length = max(map(lambda ref: len(ref), X))\n",
        "X = pad_sequences(X, maxlen=max_length, padding='post', truncating='pre', value=float(len(all_tags)), dtype='float32')\n",
        "y = pad_sequences(y, maxlen=max_length, padding='post', truncating='pre', value=float(len(all_tags)), dtype='float32')\n"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N49G-Ws59HYu"
      },
      "source": [
        "from torch.nn.utils.rnn import pack_padded_sequence\n",
        "class LSTM(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size, num_layers):\n",
        "        super(LSTM, self).__init__()\n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.output_size = output_size\n",
        "        self.num_layers = num_layers\n",
        "    \n",
        "        self.lstm = nn.LSTM(\n",
        "            input_size = self.input_size,\n",
        "            hidden_size = self.hidden_size,\n",
        "            num_layers = self.num_layers,\n",
        "            batch_first = True\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Initialise hidden state\n",
        "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size)\n",
        "\n",
        "        # Initialise internal state\n",
        "        c0 = torch.zeros(self.num_layers,x.size(0), self.hidden_size)\n",
        "\n",
        "        # Propagate input through LSTM\n",
        "        output, (hn, cn) = self.lstm(x, (h0, c0))\n",
        "\n",
        "        return output, (hn, cn)\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "40dkNSs69Igz"
      },
      "source": [
        "class Net(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size, num_layers):\n",
        "        super().__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        \n",
        "        self.lstm = LSTM(input_size, hidden_size, output_size, num_layers)\n",
        "        self.fc = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        output, (hn, cn) = self.lstm(x)\n",
        "        output = self.fc(output)\n",
        "\n",
        "        return output\n"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u8RRoJQo9JpA"
      },
      "source": [
        "'''\n",
        "Hyperparameters\n",
        "'''\n",
        "num_epochs = 1000\n",
        "learning_rate = 0.001\n",
        "\n",
        "input_size = EMBEDDING_DIM # Number of features\n",
        "hidden_size = 50 # Number of features in the hidden state\n",
        "num_layers = 1 # Number of stacked LSTM layers\n",
        "\n",
        "output_size = len(all_tags) # Number of output classes\n",
        "\n",
        "model = Net(input_size, hidden_size, output_size, num_layers)"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5H6Lsfq19Ksk"
      },
      "source": [
        "'''\n",
        "Loss Function and Optimiser\n",
        "'''\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=15)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr = learning_rate)"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kesJ8HiJ9LyR",
        "outputId": "e52e64a8-1660-4046-d21b-5c027744687d"
      },
      "source": [
        "import numpy as np\n",
        "from torch.nn.utils.rnn import pad_packed_sequence\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "X_train, X_test = torch.tensor(X_train), torch.tensor(X_test)\n",
        "y_train, y_test = torch.tensor(y_train), torch.tensor(y_test)\n",
        "\n",
        "def categorical_accuracy(outputs, y, pad_index):\n",
        "    max_outputs = outputs.argmax(dim = 1, keepdim=True)\n",
        "    non_padded_elements = (y != pad_index).nonzero()\n",
        "    correct = max_outputs[non_padded_elements].squeeze(1).eq(y[non_padded_elements])\n",
        "    return correct.sum() / torch.FloatTensor([y[non_padded_elements].shape[0]])\n",
        "\n",
        "def train(X_train, y_train):\n",
        "  for epoch in range(num_epochs):\n",
        "      outputs = model.forward(X_train)\n",
        "\n",
        "      optimizer.zero_grad()\n",
        "\n",
        "      outputs = outputs.view(-1, outputs.shape[-1]) # [batch_size * seq_len, output_dim]\n",
        "      y_train = y_train.view(-1) # [batch_size * seq_len]\n",
        "\n",
        "      # Get the loss function\n",
        "      loss = criterion(outputs, y_train.long())\n",
        "\n",
        "      # Calculate loss\n",
        "      loss.backward()\n",
        "\n",
        "      # Backpropagation\n",
        "      optimizer.step()\n",
        "\n",
        "      # Print loss at every 100th epoch\n",
        "      if epoch % 100 == 0:\n",
        "          print(\"Epoch: %d, loss: %1.5f, acc: %1.5f\" \\\n",
        "                % (epoch, loss.item(), categorical_accuracy(outputs, y_train, 15)))\n",
        "\n",
        "def test(X_test, y_test):\n",
        "    with torch.no_grad():\n",
        "        outputs = model.forward(X_test)\n",
        "\n",
        "        outputs = outputs.view(-1, outputs.shape[-1])\n",
        "        y_test = y_test.view(-1)\n",
        "\n",
        "        loss = criterion(outputs, y_test.long())\n",
        "        print(\"Testing loss: %1.5f, acc: %1.5f\" \\\n",
        "               % (loss.item(), categorical_accuracy(outputs, y_test, 15)))\n",
        "\n",
        "train(X_train, y_train)\n",
        "test(X_test, y_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 0, loss: 2.77792, acc: 0.00314\n",
            "Epoch: 100, loss: 1.25505, acc: 0.54088\n",
            "Epoch: 200, loss: 0.95351, acc: 0.67839\n",
            "Epoch: 300, loss: 0.79727, acc: 0.73529\n",
            "Epoch: 400, loss: 0.69455, acc: 0.77637\n",
            "Epoch: 500, loss: 0.61941, acc: 0.80500\n",
            "Epoch: 600, loss: 0.55211, acc: 0.83099\n",
            "Epoch: 700, loss: 0.49745, acc: 0.84765\n",
            "Epoch: 800, loss: 0.46640, acc: 0.85569\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}