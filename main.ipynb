{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "reference_parsing.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5-iCVx308906",
<<<<<<< HEAD
        "outputId": "af2e83b2-831d-49e3-f5c6-27f8e25ebfd6"
=======
        "outputId": "b51d72da-284e-4d81-b5d8-017c0f60aec9"
>>>>>>> c237b2813c2bb6d93f9f01931c6e6ec9523c420f
      },
      "source": [
        "!pip install nltk\n",
        "!pip install torch\n",
        "!pip install fasttext"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (3.2.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk) (1.15.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (1.8.0+cu101)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch) (1.19.5)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch) (3.7.4.3)\n",
            "Collecting fasttext\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f8/85/e2b368ab6d3528827b147fdb814f8189acc981a4bc2f99ab894650e05c40/fasttext-0.9.2.tar.gz (68kB)\n",
<<<<<<< HEAD
            "\u001b[K     |████████████████████████████████| 71kB 3.0MB/s \n",
=======
            "\u001b[K     |████████████████████████████████| 71kB 4.7MB/s \n",
>>>>>>> c237b2813c2bb6d93f9f01931c6e6ec9523c420f
            "\u001b[?25hRequirement already satisfied: pybind11>=2.2 in /usr/local/lib/python3.7/dist-packages (from fasttext) (2.6.2)\n",
            "Requirement already satisfied: setuptools>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from fasttext) (54.1.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from fasttext) (1.19.5)\n",
            "Building wheels for collected packages: fasttext\n",
            "  Building wheel for fasttext (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
<<<<<<< HEAD
            "  Created wheel for fasttext: filename=fasttext-0.9.2-cp37-cp37m-linux_x86_64.whl size=3095479 sha256=06ec497481368d3865acfafcdd7318070035fd4282e277a8d6c8bb7b6da1b5fa\n",
=======
            "  Created wheel for fasttext: filename=fasttext-0.9.2-cp37-cp37m-linux_x86_64.whl size=3098126 sha256=21114c61c5b924ea70648cc65ee0750e83f3fb3a6273668998c188cdb90e52bb\n",
>>>>>>> c237b2813c2bb6d93f9f01931c6e6ec9523c420f
            "  Stored in directory: /root/.cache/pip/wheels/98/ba/7f/b154944a1cf5a8cee91c154b75231136cc3a3321ab0e30f592\n",
            "Successfully built fasttext\n",
            "Installing collected packages: fasttext\n",
            "Successfully installed fasttext-0.9.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
<<<<<<< HEAD
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iyw2yfE0HB4e",
        "outputId": "2cb77b86-2951-4f9b-9e26-64e9c6630d65"
      },
      "source": [
        "!pip install prettytable"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: prettytable in /usr/local/lib/python3.7/dist-packages (2.1.0)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from prettytable) (3.7.2)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from prettytable) (0.2.5)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->prettytable) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->prettytable) (3.4.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
=======
>>>>>>> c237b2813c2bb6d93f9f01931c6e6ec9523c420f
        "id": "arhXbfrS9B5y"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uAtjFYuZ9255",
<<<<<<< HEAD
        "outputId": "68117dce-37b9-40ef-869e-26af69878635"
=======
        "outputId": "745d7592-ec7b-4745-8dcd-ba309416cc92"
>>>>>>> c237b2813c2bb6d93f9f01931c6e6ec9523c420f
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "  "
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
<<<<<<< HEAD
        "id": "17B1O7A89CcB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0be9ccee-5489-46fa-843f-bf12aaa7a8cf"
=======
        "id": "17B1O7A89CcB"
>>>>>>> c237b2813c2bb6d93f9f01931c6e6ec9523c420f
      },
      "source": [
        "''' \n",
        "Preprocess Data\n",
        "'''\n",
        "\n",
        "import re\n",
        "import nltk\n",
        "import string\n",
        "import fasttext\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from nltk import word_tokenize\n",
        "\n",
        "OTHER_TAG = \"other\"\n",
        "PUNCT_TAG = \"punct\"\n",
        "\n",
        "with open('./utils/tags.txt', encoding=\"utf-8\", errors='ignore') as f:\n",
        "    tags = set([str.rstrip(tag) for tag in f.readlines()])\n",
        "\n",
        "def remove_labels(text):\n",
        "    return re.sub(r'\\<\\/?[\\w-]*\\>\\s*', \"\", text).strip()\n",
        "\n",
        "def tag_token(token, tag):\n",
        "    if token in string.punctuation:\n",
        "        return (token, PUNCT_TAG)\n",
        "    return (token, tag)\n",
        "\n",
        "def get_tagged_tokens(groups):\n",
        "    tagged_tokens = []\n",
        "    for group in groups:\n",
        "        ref, tag = group[0], group[1]\n",
        "        if tag not in tags:\n",
        "            tag = OTHER_TAG\n",
        "        unlabelled = remove_labels(ref)\n",
        "        tokens = word_tokenize(unlabelled)\n",
        "        tagged_tokens.extend(list(map(lambda token: tag_token(token, tag), tokens)))\n",
        "    return tagged_tokens\n",
        "\n",
        "''' Attach tags to each token '''\n",
        "def attach_tags(dataset_path):\n",
        "    dataset = []\n",
        "    with open(dataset_path, encoding=\"utf-8\", errors='ignore') as f:\n",
        "        refs = f.readlines()\n",
        "        for ref in refs:\n",
        "            groups = re.findall(r'(\\<(.*)\\>.*\\<\\/\\2\\>)', ref) # format (<tag>...</tag>, tag)\n",
        "            tagged_tokens = get_tagged_tokens(groups)\n",
        "            dataset.append(tagged_tokens)\n",
        "    return dataset\n",
        "\n",
        "''' Removes labels and tokenizes '''\n",
        "def tokenized_dataset(dataset_path, sep=\" \"):\n",
        "    dataset = []\n",
        "    with open(dataset_path, encoding=\"utf-8\", errors='ignore') as f:\n",
        "        refs = f.readlines()\n",
        "        for ref in refs:\n",
        "            ref = remove_labels(ref) \n",
        "            tokenized = \" \".join(word_tokenize(ref))\n",
        "            dataset.append(tokenized)\n",
        "    return dataset\n",
        "\n",
<<<<<<< HEAD
        "def train_word_embedding_model(dataset_path, embedding_dim, use_subwords=False):\n",
        "    embedding_dataset_path = './dataset/word_embedding_dataset'\n",
        "\n",
        "    with open(embedding_dataset_path, 'w', errors='ignore') as f:\n",
        "        # fasttext tokenizes by whitespaces\n",
        "        word_embedding_dataset = tokenized_dataset(dataset_path, sep=\" \") \n",
        "        f.write(\"\\n\".join(word_embedding_dataset))\n",
        "    if use_subwords:\n",
        "      model_path = './models/subword_embedding.bin'\n",
        "      model = fasttext.train_unsupervised(embedding_dataset_path, dim=embedding_dim)\n",
        "    else:\n",
        "      model_path = './models/word_embedding.bin'\n",
        "      model = fasttext.train_unsupervised(embedding_dataset_path, dim=embedding_dim, maxn=0)\n",
        "    model.save_model(model_path)\n",
        "    return model\n",
        "\n",
        "def map_to_index(keys, idx_start=0):\n",
        "    key_to_idx, keys_arr, idx = {}, [], idx_start\n",
        "    for key in keys:\n",
        "        key_to_idx[key] = idx\n",
        "        keys_arr.append(key)\n",
        "        idx += 1\n",
        "    return key_to_idx, keys_arr\n",
=======
        "def train_word_embedding_model(dataset_path, embedding_dim, retrain=True):\n",
        "    embedding_dataset_path = './dataset/word_embedding_dataset'\n",
        "    model_path = './models/word_embedding.bin'\n",
        "\n",
        "    if retrain:\n",
        "        with open(embedding_dataset_path, 'w', errors='ignore') as f:\n",
        "            # fasttext tokenizes by whitespaces\n",
        "            word_embedding_dataset = tokenized_dataset(dataset_path, sep=\" \") \n",
        "            f.write(\"\\n\".join(word_embedding_dataset))\n",
        "        model = fasttext.train_unsupervised(embedding_dataset_path, dim=embedding_dim)\n",
        "        model.save_model(model_path)\n",
        "        return model\n",
        "    else:\n",
        "        return fasttext.load_model(model_path)\n",
        "\n",
        "def map_to_index(keys, idx_start=0):\n",
        "    idx_map, idx = {}, idx_start\n",
        "    for key in keys:\n",
        "        idx_map[key] = idx\n",
        "        idx += 1\n",
        "    return idx_map\n",
>>>>>>> c237b2813c2bb6d93f9f01931c6e6ec9523c420f
        "\n",
        "dataset_path = './dataset/standardized_dataset.txt'\n",
        "dataset = attach_tags(dataset_path)\n",
        "EMBEDDING_DIM = 100\n",
        "word_embedding_model = train_word_embedding_model(dataset_path, embedding_dim = EMBEDDING_DIM, retrain=True)\n",
        "\n",
        "all_tags = tags \n",
        "all_tags.add(OTHER_TAG)\n",
        "all_tags.add(PUNCT_TAG)\n",
<<<<<<< HEAD
        "tag_to_idx, tag_arr = map_to_index(all_tags)\n",
        "print(all_tags)\n",
=======
        "tag_to_idx = map_to_index(all_tags)\n",
>>>>>>> c237b2813c2bb6d93f9f01931c6e6ec9523c420f
        "\n",
        "X, y = [], []\n",
        "for ref in dataset:\n",
        "    X_ref, y_ref = [], []\n",
        "    for token, tag in ref:\n",
        "        X_ref.append(word_embedding_model.get_word_vector(token))\n",
        "        y_ref.append(tag_to_idx[tag])\n",
        "    X.append(X_ref)\n",
        "    y.append(y_ref)\n",
        "\n",
        "max_length = max(map(lambda ref: len(ref), X))\n",
        "X = pad_sequences(X, maxlen=max_length, padding='post', truncating='pre', value=float(len(all_tags)), dtype='float32')\n",
        "y = pad_sequences(y, maxlen=max_length, padding='post', truncating='pre', value=float(len(all_tags)), dtype='float32')\n"
      ],
<<<<<<< HEAD
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'pages', 'institution', 'editor', 'volume', 'title', 'other', 'author', 'journal', 'note', 'tech', 'punct', 'date', 'publisher', 'location', 'booktitle'}\n"
          ],
          "name": "stdout"
        }
      ]
=======
      "execution_count": 4,
      "outputs": []
>>>>>>> c237b2813c2bb6d93f9f01931c6e6ec9523c420f
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N49G-Ws59HYu"
      },
      "source": [
        "from torch.nn.utils.rnn import pack_padded_sequence\n",
        "class LSTM(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size, num_layers):\n",
        "        super(LSTM, self).__init__()\n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.output_size = output_size\n",
        "        self.num_layers = num_layers\n",
        "    \n",
        "        self.lstm = nn.LSTM(\n",
        "            input_size = self.input_size,\n",
        "            hidden_size = self.hidden_size,\n",
        "            num_layers = self.num_layers,\n",
        "            batch_first = True\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Initialise hidden state\n",
        "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size)\n",
        "\n",
        "        # Initialise internal state\n",
        "        c0 = torch.zeros(self.num_layers,x.size(0), self.hidden_size)\n",
        "\n",
        "        # Propagate input through LSTM\n",
        "        output, (hn, cn) = self.lstm(x, (h0, c0))\n",
        "\n",
        "        return output, (hn, cn)\n"
      ],
<<<<<<< HEAD
      "execution_count": 46,
=======
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "40dkNSs69Igz"
      },
      "source": [
        "class Net(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size, num_layers):\n",
        "        super().__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        \n",
        "        self.lstm = LSTM(input_size, hidden_size, output_size, num_layers)\n",
        "        self.fc = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        output, (hn, cn) = self.lstm(x)\n",
        "        output = self.fc(output)\n",
        "\n",
        "        return output\n"
      ],
      "execution_count": 6,
>>>>>>> c237b2813c2bb6d93f9f01931c6e6ec9523c420f
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
<<<<<<< HEAD
        "id": "40dkNSs69Igz"
      },
      "source": [
        "class Net(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size, num_layers):\n",
        "        super().__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        \n",
        "        self.lstm = LSTM(input_size, hidden_size, output_size, num_layers)\n",
        "        self.fc = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        output, (hn, cn) = self.lstm(x)\n",
        "        output = self.fc(output)\n",
        "\n",
        "        return output\n"
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
=======
>>>>>>> c237b2813c2bb6d93f9f01931c6e6ec9523c420f
        "id": "u8RRoJQo9JpA"
      },
      "source": [
        "'''\n",
        "Hyperparameters\n",
        "'''\n",
        "num_epochs = 1000\n",
        "learning_rate = 0.001\n",
        "\n",
        "input_size = EMBEDDING_DIM # Number of features\n",
<<<<<<< HEAD
        "hidden_size = 25 # Number of features in the hidden state\n",
        "num_layers = 2 # Number of stacked LSTM layers\n",
=======
        "hidden_size = 50 # Number of features in the hidden state\n",
        "num_layers = 1 # Number of stacked LSTM layers\n",
>>>>>>> c237b2813c2bb6d93f9f01931c6e6ec9523c420f
        "\n",
        "output_size = len(all_tags) # Number of output classes\n",
        "\n",
        "model = Net(input_size, hidden_size, output_size, num_layers)"
      ],
<<<<<<< HEAD
      "execution_count": 75,
=======
      "execution_count": 18,
>>>>>>> c237b2813c2bb6d93f9f01931c6e6ec9523c420f
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5H6Lsfq19Ksk"
      },
      "source": [
        "'''\n",
        "Loss Function and Optimiser\n",
        "'''\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=15)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr = learning_rate)"
      ],
<<<<<<< HEAD
      "execution_count": 76,
=======
      "execution_count": 19,
>>>>>>> c237b2813c2bb6d93f9f01931c6e6ec9523c420f
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kesJ8HiJ9LyR",
<<<<<<< HEAD
        "outputId": "1eaca9e3-a601-46ba-b22d-a67bd14e239f"
=======
        "outputId": "e52e64a8-1660-4046-d21b-5c027744687d"
>>>>>>> c237b2813c2bb6d93f9f01931c6e6ec9523c420f
      },
      "source": [
        "import numpy as np\n",
        "from torch.nn.utils.rnn import pad_packed_sequence\n",
<<<<<<< HEAD
        "from sklearn.metrics import f1_score, precision_recall_fscore_support\n",
        "from sklearn.model_selection import train_test_split\n",
        "from prettytable import PrettyTable\n",
        "\n",
=======
        "from sklearn.metrics import f1_score\n",
        "from sklearn.model_selection import train_test_split\n",
>>>>>>> c237b2813c2bb6d93f9f01931c6e6ec9523c420f
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "X_train, X_test = torch.tensor(X_train), torch.tensor(X_test)\n",
        "y_train, y_test = torch.tensor(y_train), torch.tensor(y_test)\n",
        "\n",
        "def categorical_accuracy(outputs, y, pad_index):\n",
        "    max_outputs = outputs.argmax(dim = 1, keepdim=True)\n",
        "    non_padded_elements = (y != pad_index).nonzero()\n",
        "    correct = max_outputs[non_padded_elements].squeeze(1).eq(y[non_padded_elements])\n",
        "    return correct.sum() / torch.FloatTensor([y[non_padded_elements].shape[0]])\n",
        "\n",
<<<<<<< HEAD
        "def get_max_outputs(outputs):\n",
        "    max_outputs = outputs.argmax(dim = 1)\n",
        "    return max_outputs\n",
        "\n",
        "def print_report(report):\n",
        "    table = PrettyTable(float_format=\"1.5f\")\n",
        "    table.field_names = [\"Tag\", \"Precision\", \"Recall\", \"FBeta\"]\n",
        "    for i in range(len(tag_arr)):\n",
        "      tag, scores = [tag_arr[i]], list(map(lambda metric: metric[i], report))[:-1] # exclude support metric\n",
        "      tag.extend(scores)\n",
        "      table.add_row(tag)\n",
        "    print(table)\n",
        "\n",
        "\n",
=======
>>>>>>> c237b2813c2bb6d93f9f01931c6e6ec9523c420f
        "def train(X_train, y_train):\n",
        "  for epoch in range(num_epochs):\n",
        "      outputs = model.forward(X_train)\n",
        "\n",
        "      optimizer.zero_grad()\n",
        "\n",
        "      outputs = outputs.view(-1, outputs.shape[-1]) # [batch_size * seq_len, output_dim]\n",
        "      y_train = y_train.view(-1) # [batch_size * seq_len]\n",
<<<<<<< HEAD
        "    \n",
=======
        "\n",
>>>>>>> c237b2813c2bb6d93f9f01931c6e6ec9523c420f
        "      # Get the loss function\n",
        "      loss = criterion(outputs, y_train.long())\n",
        "\n",
        "      # Calculate loss\n",
        "      loss.backward()\n",
        "\n",
        "      # Backpropagation\n",
        "      optimizer.step()\n",
        "\n",
        "      # Print loss at every 100th epoch\n",
        "      if epoch % 100 == 0:\n",
<<<<<<< HEAD
        "          print(\"Epoch: %d, loss: %1.5f\" % (epoch, loss.item()))\n",
        "          report = precision_recall_fscore_support(y_train.long(), get_max_outputs(outputs.detach()), average=None, zero_division=0, labels = [i for i in range(len(all_tags))])\n",
        "          print_report(report)\n",
=======
        "          print(\"Epoch: %d, loss: %1.5f, acc: %1.5f\" \\\n",
        "                % (epoch, loss.item(), categorical_accuracy(outputs, y_train, 15)))\n",
>>>>>>> c237b2813c2bb6d93f9f01931c6e6ec9523c420f
        "\n",
        "def test(X_test, y_test):\n",
        "    with torch.no_grad():\n",
        "        outputs = model.forward(X_test)\n",
        "\n",
        "        outputs = outputs.view(-1, outputs.shape[-1])\n",
        "        y_test = y_test.view(-1)\n",
        "\n",
        "        loss = criterion(outputs, y_test.long())\n",
<<<<<<< HEAD
        "        \n",
        "        print(\"Test loss: %1.5f\" % (loss.item()))\n",
        "        report = precision_recall_fscore_support(y_test.long(), get_max_outputs(outputs.detach()), average=None, zero_division=0, labels = [i for i in range(len(all_tags))])\n",
        "        print_report(report)\n",
=======
        "        print(\"Testing loss: %1.5f, acc: %1.5f\" \\\n",
        "               % (loss.item(), categorical_accuracy(outputs, y_test, 15)))\n",
>>>>>>> c237b2813c2bb6d93f9f01931c6e6ec9523c420f
        "\n",
        "train(X_train, y_train)\n",
        "test(X_test, y_test)"
      ],
<<<<<<< HEAD
      "execution_count": 77,
=======
      "execution_count": null,
>>>>>>> c237b2813c2bb6d93f9f01931c6e6ec9523c420f
      "outputs": [
        {
          "output_type": "stream",
          "text": [
<<<<<<< HEAD
            "Epoch: 0, loss: 2.69518\n",
            "+-------------+---------------------+--------+---------------------+\n",
            "|     Tag     |      Precision      | Recall |        FBeta        |\n",
            "+-------------+---------------------+--------+---------------------+\n",
            "|    pages    |         0.0         |  0.0   |         0.0         |\n",
            "| institution |         0.0         |  0.0   |         0.0         |\n",
            "|    editor   |         0.0         |  0.0   |         0.0         |\n",
            "|    volume   |         0.0         |  0.0   |         0.0         |\n",
            "|    title    |         0.0         |  0.0   |         0.0         |\n",
            "|    other    |         0.0         |  0.0   |         0.0         |\n",
            "|    author   |         0.0         |  0.0   |         0.0         |\n",
            "|   journal   |         0.0         |  0.0   |         0.0         |\n",
            "|     note    |         0.0         |  0.0   |         0.0         |\n",
            "|     tech    |         0.0         |  0.0   |         0.0         |\n",
            "|    punct    |         0.0         |  0.0   |         0.0         |\n",
            "|     date    | 0.01215048436811977 |  1.0   | 0.02400924478281558 |\n",
            "|  publisher  |         0.0         |  0.0   |         0.0         |\n",
            "|   location  |         0.0         |  0.0   |         0.0         |\n",
            "|  booktitle  |         0.0         |  0.0   |         0.0         |\n",
            "+-------------+---------------------+--------+---------------------+\n",
            "Epoch: 100, loss: 1.64057\n",
            "+-------------+----------------------+--------+---------------------+\n",
            "|     Tag     |      Precision       | Recall |        FBeta        |\n",
            "+-------------+----------------------+--------+---------------------+\n",
            "|    pages    |         0.0          |  0.0   |         0.0         |\n",
            "| institution |         0.0          |  0.0   |         0.0         |\n",
            "|    editor   |         0.0          |  0.0   |         0.0         |\n",
            "|    volume   |         0.0          |  0.0   |         0.0         |\n",
            "|    title    |         0.0          |  0.0   |         0.0         |\n",
            "|    other    |         0.0          |  0.0   |         0.0         |\n",
            "|    author   |         0.0          |  0.0   |         0.0         |\n",
            "|   journal   |         0.0          |  0.0   |         0.0         |\n",
            "|     note    |         0.0          |  0.0   |         0.0         |\n",
            "|     tech    |         0.0          |  0.0   |         0.0         |\n",
            "|    punct    | 0.054027215317537895 |  1.0   | 0.10251578807908024 |\n",
            "|     date    |         0.0          |  0.0   |         0.0         |\n",
            "|  publisher  |         0.0          |  0.0   |         0.0         |\n",
            "|   location  |         0.0          |  0.0   |         0.0         |\n",
            "|  booktitle  |         0.0          |  0.0   |         0.0         |\n",
            "+-------------+----------------------+--------+---------------------+\n",
            "Epoch: 200, loss: 1.44301\n",
            "+-------------+------------------------+------------------------+------------------------+\n",
            "|     Tag     |       Precision        |         Recall         |         FBeta          |\n",
            "+-------------+------------------------+------------------------+------------------------+\n",
            "|    pages    |          0.0           |          0.0           |          0.0           |\n",
            "| institution |          0.0           |          0.0           |          0.0           |\n",
            "|    editor   |          0.0           |          0.0           |          0.0           |\n",
            "|    volume   |          0.0           |          0.0           |          0.0           |\n",
            "|    title    | 1.0547215930514941e-05 | 0.00032581249490917975 | 2.0432974734626743e-05 |\n",
            "|    other    |   0.2783886713511774   |   0.3279828109201213   |   0.3011576290364697   |\n",
            "|    author   |          0.94          | 0.0035017136045298763  | 0.0069774346793349176  |\n",
            "|   journal   |          0.0           |          0.0           |          0.0           |\n",
            "|     note    |          0.0           |          0.0           |          0.0           |\n",
            "|     tech    |          0.0           |          0.0           |          0.0           |\n",
            "|    punct    |  0.42155517546003046   |   0.9201608271108558   |   0.5782126145635949   |\n",
            "|     date    |          0.0           |          0.0           |          0.0           |\n",
            "|  publisher  |          0.0           |          0.0           |          0.0           |\n",
            "|   location  |          0.0           |          0.0           |          0.0           |\n",
            "|  booktitle  |          0.0           |          0.0           |          0.0           |\n",
            "+-------------+------------------------+------------------------+------------------------+\n",
            "Epoch: 300, loss: 1.20985\n",
            "+-------------+----------------------+--------------------+---------------------+\n",
            "|     Tag     |      Precision       |       Recall       |        FBeta        |\n",
            "+-------------+----------------------+--------------------+---------------------+\n",
            "|    pages    |         0.0          |        0.0         |         0.0         |\n",
            "| institution |         0.0          |        0.0         |         0.0         |\n",
            "|    editor   |         0.0          |        0.0         |         0.0         |\n",
            "|    volume   |         0.0          |        0.0         |         0.0         |\n",
            "|    title    | 0.024566046232742417 | 0.7909098313920339 | 0.04765199809588309 |\n",
            "|    other    | 0.44961492569693023  | 0.2619438827098079 | 0.33103062732100785 |\n",
            "|    author   |  0.6175176643114252  | 0.6902101028162718 |  0.6518435125246271 |\n",
            "|   journal   |         0.0          |        0.0         |         0.0         |\n",
            "|     note    |         0.0          |        0.0         |         0.0         |\n",
            "|     tech    |         0.0          |        0.0         |         0.0         |\n",
            "|    punct    |  0.6108145668172199  | 0.7934274226634939 |  0.6902471669492282 |\n",
            "|     date    |         0.0          |        0.0         |         0.0         |\n",
            "|  publisher  |         0.0          |        0.0         |         0.0         |\n",
            "|   location  |         0.0          |        0.0         |         0.0         |\n",
            "|  booktitle  |         0.0          |        0.0         |         0.0         |\n",
            "+-------------+----------------------+--------------------+---------------------+\n",
            "Epoch: 400, loss: 1.01448\n",
            "+-------------+----------------------+---------------------+----------------------+\n",
            "|     Tag     |      Precision       |        Recall       |        FBeta         |\n",
            "+-------------+----------------------+---------------------+----------------------+\n",
            "|    pages    |         0.0          |         0.0         |         0.0          |\n",
            "| institution |         0.0          |         0.0         |         0.0          |\n",
            "|    editor   |         0.0          |         0.0         |         0.0          |\n",
            "|    volume   |         0.0          |         0.0         |         0.0          |\n",
            "|    title    | 0.027904488215469644 |  0.8241427058727703 | 0.053981236211923564 |\n",
            "|    other    |  0.1476017261245792  | 0.41285389282103135 | 0.21745860031621866  |\n",
            "|    author   |  0.7262349066959385  |  0.7393831023692445 |  0.7327500276885592  |\n",
            "|   journal   |         0.0          |         0.0         |         0.0          |\n",
            "|     note    |         0.0          |         0.0         |         0.0          |\n",
            "|     tech    |         0.0          |         0.0         |         0.0          |\n",
            "|    punct    |  0.6727296476504588  |  0.8452039058012636 |  0.7491681364438061  |\n",
            "|     date    |         0.0          |         0.0         |         0.0          |\n",
            "|  publisher  |         0.0          |         0.0         |         0.0          |\n",
            "|   location  |         0.0          |         0.0         |         0.0          |\n",
            "|  booktitle  |         0.0          |         0.0         |         0.0          |\n",
            "+-------------+----------------------+---------------------+----------------------+\n",
            "Epoch: 500, loss: 0.86260\n",
            "+-------------+----------------------+---------------------+---------------------+\n",
            "|     Tag     |      Precision       |        Recall       |        FBeta        |\n",
            "+-------------+----------------------+---------------------+---------------------+\n",
            "|    pages    |         0.0          |         0.0         |         0.0         |\n",
            "| institution |         0.0          |         0.0         |         0.0         |\n",
            "|    editor   |         0.0          |         0.0         |         0.0         |\n",
            "|    volume   |         0.0          |         0.0         |         0.0         |\n",
            "|    title    |  0.7013169329500635  |  0.8545247210230512 |  0.7703774416213833 |\n",
            "|    other    | 0.019880503048547545 | 0.49330131445904957 | 0.03822067716111343 |\n",
            "|    author   |  0.7716594343857549  |  0.7684398748323648 |  0.770046289383306  |\n",
            "|   journal   |         0.0          |         0.0         |         0.0         |\n",
            "|     note    |         0.0          |         0.0         |         0.0         |\n",
            "|     tech    |         0.0          |         0.0         |         0.0         |\n",
            "|    punct    |  0.708139997348535   |  0.8765898088126692 |  0.7834121658783412 |\n",
            "|     date    |         0.0          |         0.0         |         0.0         |\n",
            "|  publisher  |         0.0          |         0.0         |         0.0         |\n",
            "|   location  |         0.0          |         0.0         |         0.0         |\n",
            "|  booktitle  |         0.0          |         0.0         |         0.0         |\n",
            "+-------------+----------------------+---------------------+---------------------+\n",
            "Epoch: 600, loss: 0.77448\n",
            "+-------------+----------------------+--------------------+----------------------+\n",
            "|     Tag     |      Precision       |       Recall       |        FBeta         |\n",
            "+-------------+----------------------+--------------------+----------------------+\n",
            "|    pages    |         0.0          |        0.0         |         0.0          |\n",
            "| institution |         0.0          |        0.0         |         0.0          |\n",
            "|    editor   |         0.0          |        0.0         |         0.0          |\n",
            "|    volume   |         0.0          |        0.0         |         0.0          |\n",
            "|    title    |  0.7908873260511746  | 0.8610409709212349 |  0.8244745154623094  |\n",
            "|    other    | 0.024939590937124022 | 0.623546511627907  | 0.047960919651971023 |\n",
            "|    author   |  0.8166079362647465  | 0.7942184473252868 |  0.8052575917812357  |\n",
            "|   journal   |         0.0          |        0.0         |         0.0          |\n",
            "|     note    |         0.0          |        0.0         |         0.0          |\n",
            "|     tech    |         0.0          |        0.0         |         0.0          |\n",
            "|    punct    |  0.7502320624333895  | 0.8952982686469189 |  0.8163708123678942  |\n",
            "|     date    |         0.0          |        0.0         |         0.0          |\n",
            "|  publisher  |         0.0          |        0.0         |         0.0          |\n",
            "|   location  |         0.0          |        0.0         |         0.0          |\n",
            "|  booktitle  |         0.0          |        0.0         |         0.0          |\n",
            "+-------------+----------------------+--------------------+----------------------+\n",
            "Epoch: 700, loss: 0.70113\n",
            "+-------------+---------------------+---------------------+---------------------+\n",
            "|     Tag     |      Precision      |        Recall       |        FBeta        |\n",
            "+-------------+---------------------+---------------------+---------------------+\n",
            "|    pages    |         0.0         |         0.0         |         0.0         |\n",
            "| institution |         0.0         |         0.0         |         0.0         |\n",
            "|    editor   |         0.0         |         0.0         |         0.0         |\n",
            "|    volume   |         0.0         |         0.0         |         0.0         |\n",
            "|    title    |  0.8567959263611438 |  0.8908528142054247 |  0.8734925325453239 |\n",
            "|    other    | 0.02765422889960159 |  0.6935035389282103 | 0.05318754498331528 |\n",
            "|    author   |  0.8522032303591031 |  0.8097898971837282 |  0.8304553789731051 |\n",
            "|   journal   |         0.0         |         0.0         |         0.0         |\n",
            "|     note    |         0.0         |         0.0         |         0.0         |\n",
            "|     tech    |         0.0         |         0.0         |         0.0         |\n",
            "|    punct    |  0.7830699055903256 |  0.9085911216870436 |  0.8411736777134177 |\n",
            "|     date    |         0.0         |         0.0         |         0.0         |\n",
            "|  publisher  |         0.0         |         0.0         |         0.0         |\n",
            "|   location  |         0.0         |         0.0         |         0.0         |\n",
            "|  booktitle  |  0.5933852140077821 | 0.20608108108108109 |  0.3059177532597794 |\n",
            "+-------------+---------------------+---------------------+---------------------+\n",
            "Epoch: 800, loss: 0.62508\n",
            "+-------------+----------------------+---------------------+---------------------+\n",
            "|     Tag     |      Precision       |        Recall       |        FBeta        |\n",
            "+-------------+----------------------+---------------------+---------------------+\n",
            "|    pages    |         0.0          |         0.0         |         0.0         |\n",
            "| institution |         0.0          |         0.0         |         0.0         |\n",
            "|    editor   |         0.0          |         0.0         |         0.0         |\n",
            "|    volume   |         0.0          |         0.0         |         0.0         |\n",
            "|    title    |  0.8774122978357685  |  0.9147185794575222 |  0.8956771414898709 |\n",
            "|    other    | 0.029715417250309878 |   0.74538675429727  | 0.05715240965606799 |\n",
            "|    author   |  0.8790297339593114  |  0.8369840560274177 |  0.8574917945195023 |\n",
            "|   journal   |         0.0          |         0.0         |         0.0         |\n",
            "|     note    |         0.0          |         0.0         |         0.0         |\n",
            "|     tech    |         0.0          |         0.0         |         0.0         |\n",
            "|    punct    |  0.821852298032344   |  0.9236481496676787 |  0.8697819073927406 |\n",
            "|     date    |         0.0          |         0.0         |         0.0         |\n",
            "|  publisher  |         0.0          |         0.0         |         0.0         |\n",
            "|   location  |         0.0          |         0.0         |         0.0         |\n",
            "|  booktitle  |  0.5924132364810331  | 0.49594594594594593 |  0.5399043766090474 |\n",
            "+-------------+----------------------+---------------------+---------------------+\n",
            "Epoch: 900, loss: 0.54217\n",
            "+-------------+---------------------+--------------------+----------------------+\n",
            "|     Tag     |      Precision      |       Recall       |        FBeta         |\n",
            "+-------------+---------------------+--------------------+----------------------+\n",
            "|    pages    |         0.0         |        0.0         |         0.0          |\n",
            "| institution |         0.0         |        0.0         |         0.0          |\n",
            "|    editor   |         0.0         |        0.0         |         0.0          |\n",
            "|    volume   |         0.0         |        0.0         |         0.0          |\n",
            "|    title    |  0.8951194184839044 | 0.9127637044880671 |  0.9038554605581545  |\n",
            "|    other    | 0.03844444510727399 | 0.8145222446916077 | 0.07342339648007475  |\n",
            "|    author   |  0.9000076271832812 | 0.8791536283713306 |  0.889458410281536   |\n",
            "|   journal   |         0.0         |        0.0         |         0.0          |\n",
            "|     note    |         0.0         |        0.0         |         0.0          |\n",
            "|     tech    |         0.0         |        0.0         |         0.0          |\n",
            "|    punct    |  0.8689610782977435 | 0.9416181176663658 |  0.9038317646595518  |\n",
            "|     date    |         0.0         |        0.0         |         0.0          |\n",
            "|  publisher  |         0.0         |        0.0         |         0.0          |\n",
            "|   location  |         0.0         |        0.0         |         0.0          |\n",
            "|  booktitle  | 0.01417650928913813 | 0.6114864864864865 | 0.027710585137328145 |\n",
            "+-------------+---------------------+--------------------+----------------------+\n",
            "Test loss: 0.51628\n",
            "+-------------+----------------------+--------------------+---------------------+\n",
            "|     Tag     |      Precision       |       Recall       |        FBeta        |\n",
            "+-------------+----------------------+--------------------+---------------------+\n",
            "|    pages    |         0.0          |        0.0         |         0.0         |\n",
            "| institution |         0.0          |        0.0         |         0.0         |\n",
            "|    editor   |         0.0          |        0.0         |         0.0         |\n",
            "|    volume   |         0.0          |        0.0         |         0.0         |\n",
            "|    title    |  0.9141004862236629  | 0.9000957548675391 |  0.9070440656159536 |\n",
            "|    other    | 0.08505474211845403  | 0.8588172615876398 | 0.15478047960824792 |\n",
            "|    author   |  0.9042042042042042  | 0.8926771420100801 |  0.8984036998358944 |\n",
            "|   journal   |         0.0          |        0.0         |         0.0         |\n",
            "|     note    |         0.0          |        0.0         |         0.0         |\n",
            "|     tech    |         0.0          |        0.0         |         0.0         |\n",
            "|    punct    |  0.8938478227181156  | 0.9466601017561136 |  0.9194962537860673 |\n",
            "|     date    |         0.0          |        0.0         |         0.0         |\n",
            "|  publisher  |         0.0          |        0.0         |         0.0         |\n",
            "|   location  |         0.0          |        0.0         |         0.0         |\n",
            "|  booktitle  | 0.004576216564614889 | 0.7135678391959799 | 0.00909411124275513 |\n",
            "+-------------+----------------------+--------------------+---------------------+\n"
=======
            "Epoch: 0, loss: 2.77792, acc: 0.00314\n",
            "Epoch: 100, loss: 1.25505, acc: 0.54088\n",
            "Epoch: 200, loss: 0.95351, acc: 0.67839\n",
            "Epoch: 300, loss: 0.79727, acc: 0.73529\n",
            "Epoch: 400, loss: 0.69455, acc: 0.77637\n",
            "Epoch: 500, loss: 0.61941, acc: 0.80500\n",
            "Epoch: 600, loss: 0.55211, acc: 0.83099\n",
            "Epoch: 700, loss: 0.49745, acc: 0.84765\n",
            "Epoch: 800, loss: 0.46640, acc: 0.85569\n"
>>>>>>> c237b2813c2bb6d93f9f01931c6e6ec9523c420f
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}