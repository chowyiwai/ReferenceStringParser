{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "reference_parsing.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3.8.3 64-bit",
      "metadata": {
        "interpreter": {
          "hash": "6e61ca267e818e5d9a430b77a02997073c0a797ef0ae9dc5b9fbd2a4d1cc9a76"
        }
      }
    },
    "language_info": {
      "name": "python",
      "version": "3.8.3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5-iCVx308906",
        "outputId": "af2e83b2-831d-49e3-f5c6-27f8e25ebfd6"
      },
      "source": [
        "!pip install nltk\n",
        "!pip install torch\n",
        "!pip install fasttext"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iyw2yfE0HB4e",
        "outputId": "2cb77b86-2951-4f9b-9e26-64e9c6630d65"
      },
      "source": [
        "!pip install prettytable"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "arhXbfrS9B5y"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uAtjFYuZ9255",
        "outputId": "68117dce-37b9-40ef-869e-26af69878635"
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "17B1O7A89CcB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0be9ccee-5489-46fa-843f-bf12aaa7a8cf"
      },
      "source": [
        "''' \n",
        "Preprocess Data\n",
        "'''\n",
        "\n",
        "import re\n",
        "import nltk\n",
        "import string\n",
        "import fasttext\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from nltk import word_tokenize\n",
        "\n",
        "OTHER_TAG = \"other\"\n",
        "PUNCT_TAG = \"punct\"\n",
        "\n",
        "with open('./utils/tags.txt', encoding=\"utf-8\", errors='ignore') as f:\n",
        "    tags = set([str.rstrip(tag) for tag in f.readlines()])\n",
        "\n",
        "def remove_labels(text):\n",
        "    return re.sub(r'\\<\\/?[\\w-]*\\>\\s*', \"\", text).strip()\n",
        "\n",
        "def tag_token(token, tag):\n",
        "    if token in string.punctuation:\n",
        "        return (token, PUNCT_TAG)\n",
        "    return (token, tag)\n",
        "\n",
        "def get_tagged_tokens(groups):\n",
        "    tagged_tokens = []\n",
        "    for group in groups:\n",
        "        ref, tag = group[0], group[1]\n",
        "        if tag not in tags:\n",
        "            tag = OTHER_TAG\n",
        "        unlabelled = remove_labels(ref)\n",
        "        tokens = word_tokenize(unlabelled)\n",
        "        tagged_tokens.extend(list(map(lambda token: tag_token(token, tag), tokens)))\n",
        "    return tagged_tokens\n",
        "\n",
        "''' Attach tags to each token '''\n",
        "def attach_tags(dataset_path):\n",
        "    dataset = []\n",
        "    with open(dataset_path, encoding=\"utf-8\", errors='ignore') as f:\n",
        "        refs = f.readlines()\n",
        "        for ref in refs:\n",
        "            groups = re.findall(r'(\\<(.*)\\>.*\\<\\/\\2\\>)', ref) # format (<tag>...</tag>, tag)\n",
        "            tagged_tokens = get_tagged_tokens(groups)\n",
        "            dataset.append(tagged_tokens)\n",
        "    return dataset\n",
        "\n",
        "''' Removes labels and tokenizes '''\n",
        "def tokenized_dataset(dataset_path, sep=\" \"):\n",
        "    dataset = []\n",
        "    with open(dataset_path, encoding=\"utf-8\", errors='ignore') as f:\n",
        "        refs = f.readlines()\n",
        "        for ref in refs:\n",
        "            ref = remove_labels(ref) \n",
        "            tokenized = \" \".join(word_tokenize(ref))\n",
        "            dataset.append(tokenized)\n",
        "    return dataset\n",
        "\n",
        "def train_word_embedding_model(dataset_path, embedding_dim, use_subwords=False):\n",
        "    embedding_dataset_path = './dataset/word_embedding_dataset'\n",
        "\n",
        "    with open(embedding_dataset_path, 'w', errors='ignore') as f:\n",
        "        # fasttext tokenizes by whitespaces\n",
        "        word_embedding_dataset = tokenized_dataset(dataset_path, sep=\" \") \n",
        "        f.write(\"\\n\".join(word_embedding_dataset))\n",
        "    if use_subwords:\n",
        "      model_path = './models/subword_embedding.bin'\n",
        "      model = fasttext.train_unsupervised(embedding_dataset_path, dim=embedding_dim)\n",
        "    else:\n",
        "      model_path = './models/word_embedding.bin'\n",
        "      model = fasttext.train_unsupervised(embedding_dataset_path, dim=embedding_dim, maxn=0)\n",
        "    model.save_model(model_path)\n",
        "    return model\n",
        "\n",
        "def map_to_index(keys, idx_start=0):\n",
        "    key_to_idx, keys_arr, idx = {}, [], idx_start\n",
        "    for key in keys:\n",
        "        key_to_idx[key] = idx\n",
        "        keys_arr.append(key)\n",
        "        idx += 1\n",
        "    return key_to_idx, keys_arr\n",
        "\n",
        "dataset_path = './dataset/standardized_dataset.txt'\n",
        "dataset = attach_tags(dataset_path)\n",
        "EMBEDDING_DIM = 100\n",
        "word_embedding_model = train_word_embedding_model(dataset_path, embedding_dim = EMBEDDING_DIM)\n",
        "\n",
        "all_tags = tags \n",
        "all_tags.add(OTHER_TAG)\n",
        "all_tags.add(PUNCT_TAG)\n",
        "tag_to_idx, tag_arr = map_to_index(all_tags)\n",
        "print(all_tags)\n",
        "\n",
        "X, y = [], []\n",
        "for ref in dataset:\n",
        "    X_ref, y_ref = [], []\n",
        "    for token, tag in ref:\n",
        "        X_ref.append(word_embedding_model.get_word_vector(token))\n",
        "        y_ref.append(tag_to_idx[tag])\n",
        "    X.append(X_ref)\n",
        "    y.append(y_ref)\n",
        "\n",
        "max_length = max(map(lambda ref: len(ref), X))\n",
        "X = pad_sequences(X, maxlen=max_length, padding='post', truncating='pre', value=float(len(all_tags)), dtype='float32')\n",
        "y = pad_sequences(y, maxlen=max_length, padding='post', truncating='pre', value=float(len(all_tags)), dtype='float32')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N49G-Ws59HYu"
      },
      "source": [
        "from torch.nn.utils.rnn import pack_padded_sequence\n",
        "class LSTM(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size, num_layers):\n",
        "        super(LSTM, self).__init__()\n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.output_size = output_size\n",
        "        self.num_layers = num_layers\n",
        "    \n",
        "        self.lstm = nn.LSTM(\n",
        "            input_size = self.input_size,\n",
        "            hidden_size = self.hidden_size,\n",
        "            num_layers = self.num_layers,\n",
        "            batch_first = True\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Initialise hidden state\n",
        "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size)\n",
        "\n",
        "        # Initialise internal state\n",
        "        c0 = torch.zeros(self.num_layers,x.size(0), self.hidden_size)\n",
        "\n",
        "        # Propagate input through LSTM\n",
        "        output, (hn, cn) = self.lstm(x, (h0, c0))\n",
        "\n",
        "        return output, (hn, cn)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "40dkNSs69Igz"
      },
      "source": [
        "class Net(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size, num_layers):\n",
        "        super().__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        \n",
        "        self.lstm = LSTM(input_size, hidden_size, output_size, num_layers)\n",
        "        self.fc = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        output, (hn, cn) = self.lstm(x)\n",
        "        output = self.fc(output)\n",
        "\n",
        "        return output\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u8RRoJQo9JpA"
      },
      "source": [
        "'''\n",
        "Hyperparameters\n",
        "'''\n",
        "num_epochs = 1000\n",
        "learning_rate = 0.001\n",
        "\n",
        "input_size = EMBEDDING_DIM # Number of features\n",
        "hidden_size = 25 # Number of features in the hidden state\n",
        "num_layers = 2 # Number of stacked LSTM layers\n",
        "\n",
        "output_size = len(all_tags) # Number of output classes\n",
        "\n",
        "model = Net(input_size, hidden_size, output_size, num_layers)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5H6Lsfq19Ksk"
      },
      "source": [
        "'''\n",
        "Loss Function and Optimiser\n",
        "'''\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=15)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr = learning_rate)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kesJ8HiJ9LyR",
        "outputId": "1eaca9e3-a601-46ba-b22d-a67bd14e239f"
      },
      "source": [
        "import numpy as np\n",
        "from torch.nn.utils.rnn import pad_packed_sequence\n",
        "from sklearn.metrics import f1_score, precision_recall_fscore_support\n",
        "from sklearn.model_selection import train_test_split\n",
        "from prettytable import PrettyTable\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "X_train, X_test = torch.tensor(X_train), torch.tensor(X_test)\n",
        "y_train, y_test = torch.tensor(y_train), torch.tensor(y_test)\n",
        "\n",
        "def categorical_accuracy(outputs, y, pad_index):\n",
        "    max_outputs = outputs.argmax(dim = 1, keepdim=True)\n",
        "    non_padded_elements = (y != pad_index).nonzero()\n",
        "    correct = max_outputs[non_padded_elements].squeeze(1).eq(y[non_padded_elements])\n",
        "    return correct.sum() / torch.FloatTensor([y[non_padded_elements].shape[0]])\n",
        "\n",
        "def get_max_outputs(outputs):\n",
        "    max_outputs = outputs.argmax(dim = 1)\n",
        "    return max_outputs\n",
        "\n",
        "def print_report(report):\n",
        "    table = PrettyTable(float_format=\"1.5f\")\n",
        "    table.field_names = [\"Tag\", \"Precision\", \"Recall\", \"FBeta\"]\n",
        "    for i in range(len(tag_arr)):\n",
        "      tag, scores = [tag_arr[i]], list(map(lambda metric: metric[i], report))[:-1] # exclude support metric\n",
        "      tag.extend(scores)\n",
        "      table.add_row(tag)\n",
        "    print(table)\n",
        "\n",
        "\n",
        "def train(X_train, y_train):\n",
        "  for epoch in range(num_epochs):\n",
        "      outputs = model.forward(X_train)\n",
        "\n",
        "      optimizer.zero_grad()\n",
        "\n",
        "      outputs = outputs.view(-1, outputs.shape[-1]) # [batch_size * seq_len, output_dim]\n",
        "      y_train = y_train.view(-1) # [batch_size * seq_len]\n",
        "    \n",
        "      # Get the loss function\n",
        "      loss = criterion(outputs, y_train.long())\n",
        "\n",
        "      # Calculate loss\n",
        "      loss.backward()\n",
        "\n",
        "      # Backpropagation\n",
        "      optimizer.step()\n",
        "\n",
        "      # Print loss at every 100th epoch\n",
        "      if epoch % 100 == 0:\n",
        "          print(\"Epoch: %d, loss: %1.5f\" % (epoch, loss.item()))\n",
        "          report = precision_recall_fscore_support(y_train.long(), get_max_outputs(outputs.detach()), average=None, zero_division=0, labels = [i for i in range(len(all_tags))])\n",
        "          print_report(report)\n",
        "\n",
        "def test(X_test, y_test):\n",
        "    with torch.no_grad():\n",
        "        outputs = model.forward(X_test)\n",
        "\n",
        "        outputs = outputs.view(-1, outputs.shape[-1])\n",
        "        y_test = y_test.view(-1)\n",
        "\n",
        "        loss = criterion(outputs, y_test.long())\n",
        "        \n",
        "        print(\"Test loss: %1.5f\" % (loss.item()))\n",
        "        report = precision_recall_fscore_support(y_test.long(), get_max_outputs(outputs.detach()), average=None, zero_division=0, labels = [i for i in range(len(all_tags))])\n",
        "        print_report(report)\n",
        "\n",
        "train(X_train, y_train)\n",
        "test(X_test, y_test)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}