<?xml version="1.0" encoding="UTF-8"?>


<article dtd-version="1.0" article-type="research-article">
  <front>
      <journal-meta>
         <journal-id xmlns:xlink="http://www.w3.org/1999/xlink" journal-id-type="jstor">jcompgrapstat</journal-id>
         <journal-id xmlns:xlink="http://www.w3.org/1999/xlink" journal-id-type="jstor">j100879</journal-id>
         <journal-title-group xmlns:xlink="http://www.w3.org/1999/xlink">
            <journal-title>Journal of Computational and Graphical Statistics</journal-title>
         </journal-title-group>
      
         <publisher>
            <publisher-name>JCGS Management Committee of the American Statistical Association, Institute of Mathematical Statistics, and Interface Foundation of North America</publisher-name>
         </publisher>
         <issn xmlns:xlink="http://www.w3.org/1999/xlink" pub-type="ppub">10618600</issn>
         <custom-meta-group xmlns:xlink="http://www.w3.org/1999/xlink"/>
      </journal-meta>
      <article-meta xmlns:xlink="http://www.w3.org/1999/xlink">
         <volume xmlns:mml="http://www.w3.org/1998/Math/MathML"
                 xmlns:oasis="http://docs.oasis-open.org/ns/oasis-exchange/table">19</volume>
         <issue xmlns:mml="http://www.w3.org/1998/Math/MathML"
                xmlns:oasis="http://docs.oasis-open.org/ns/oasis-exchange/table">1</issue>
         <issue-id>i25651294</issue-id>
         <article-id pub-id-type="jstor">25651305</article-id>
         <article-categories>
            <subj-group>
               <subject>Boosting</subject>
            </subj-group>
         </article-categories>
         <title-group>
            <article-title>Feature Extraction in Signal Regression: A Boosting Technique for Functional Data Regression</article-title>
         </title-group>
         <contrib-group>
            <contrib>
               <string-name>
                  <given-names>Gerhard</given-names>
                  <surname>Tutz</surname>
               </string-name>
            </contrib>
            <contrib>
               <string-name>
                  <given-names>Jan</given-names>
                  <surname>Gertheiss</surname>
               </string-name>
            </contrib>
         </contrib-group>
         <pub-date pub-type="ppub">
            <day>1</day>
            <month>3</month>
            <year>2010</year>
         </pub-date>
         <fpage>154</fpage>
         <lpage>174</lpage>
      
      
      
      
      
      
      
      
      
      
      
      
      
      
         <permissions>
            <copyright-statement>© 2010 The American Statistical Association, the Institute of Mathematical Statistics, and the Interface Foundation of North America</copyright-statement>
         </permissions>
      
         <self-uri xlink:href="https://www.jstor.org/stable/25651305"/>
      
      
         <abstract>
            <p>The main objectives of feature extraction in signal regression are improved accuracy of the prediction of future data and the identification of relevant parts of the signal. A feature extraction procedure is introduced that uses boosting techniques to select the relevant parts of the signal, whereby the proposed blockwise boosting procedure simultaneously selects intervals in the signal's domain and estimates the effect on the response. The blocks that are defined explicitly use the underlying metric of the signal. It is demonstrated in simulation studies and with real-world data that the proposed approach competes well with procedures like PLS, P-Spline signal regression, and functional data regression. Supplemental materials are available online.</p>
         </abstract>
         <custom-meta-group>
            <custom-meta>
               <meta-name>lang</meta-name>
               <meta-value>eng</meta-value>
            </custom-meta>
         </custom-meta-group>
      </article-meta>
  </front>
  <back>
    
      <ref-list>
        <title>References</title>
        <ref id="d265129e283a1310">
          
            <mixed-citation id="d265129e287" publication-type="other">
Breiman, L. (1998), "Arcing Classifiers," The Annals of Statistics, 26, 801-849. [158]</mixed-citation>
        </ref>
        <ref id="d265129e294a1310">
          
            <mixed-citation id="d265129e298" publication-type="other">
- (1999), "Prediction Games and Arcing Algorithms," Neural Computation, 11, 1493-1517. [158]</mixed-citation>
        </ref>
        <ref id="d265129e305a1310">
          
            <mixed-citation id="d265129e309" publication-type="other">
- (2001), "Random Forests," Machine Learning, 45, 5-32. [165]</mixed-citation>
        </ref>
        <ref id="d265129e316a1310">
          
            <mixed-citation id="d265129e320" publication-type="other">
Buhlmann, P. (2006), "Boosting for High-Dimensional Linear Models," The Annals of Statistics, 34, 559-583.
[158,161]</mixed-citation>
        </ref>
        <ref id="d265129e331a1310">
          
            <mixed-citation id="d265129e335" publication-type="other">
Buhlmann, P., and Yu, B. (2003), "Boosting With the L2 Loss: Regression and Classification," Journal of the
American Statistical Association, 98, 324-339. [158,159,161]</mixed-citation>
        </ref>
        <ref id="d265129e345a1310">
          
            <mixed-citation id="d265129e349" publication-type="other">
Christensen, J., N0rgaard, L., Heimdal, H., Pedersen, J. G., and Engelsen, S. B. (2004), "Rapid Spectroscopic
Analysis of Marzipan—Comparative Instrumentation," Journal of Near Infrared Spectroscopy, 12, 63-75.
[154,169,170,173]</mixed-citation>
        </ref>
        <ref id="d265129e362a1310">
          
            <mixed-citation id="d265129e366" publication-type="other">
Efron, B., Hastie, T., Johnstone, I., and Tibshirani, R. (2004), "Least Angle Regression," The Annals of Statistics,
32, 407-499. [164]</mixed-citation>
        </ref>
        <ref id="d265129e376a1310">
          
            <mixed-citation id="d265129e380" publication-type="other">
Eilers, P. H. C, and Marx, B. D. (1996), "Flexible Smoothing With B-Splines and Penalties," Statistical Science,
11, 89-121. [172]</mixed-citation>
        </ref>
        <ref id="d265129e390a1310">
          
            <mixed-citation id="d265129e394" publication-type="other">
Frank, I. E., and Friedman, J. H. (1993), "A Statistical View of Some Chemometrics Regression Tools" (with
discussion), Technometrics, 35, 109-148. [154]</mixed-citation>
        </ref>
        <ref id="d265129e404a1310">
          
            <mixed-citation id="d265129e408" publication-type="other">
Freund, Y., and Schapire, R. E. (1996), "Experiments With a New Boosting Algorithm," in Proceedings of the
Thirteenth International Conference on Machine Learning, San Francisco, CA: Morgan Kaufmann, pp. 148-
156. [158]</mixed-citation>
        </ref>
        <ref id="d265129e422a1310">
          
            <mixed-citation id="d265129e426" publication-type="other">
Friedman, J. H. (2002), "Stochastic Gradient Boosting," Computational Statistics &amp;amp; Data Analysis, 38, 367-378.
[172]</mixed-citation>
        </ref>
        <ref id="d265129e436a1310">
          
            <mixed-citation id="d265129e440" publication-type="other">
Friedman, J. H., Hastie, T., and Tibshirani, R. (2000), "Additive Logistic Regression: A Statistical View of Boost-
ing," The Annals of Statistics, 28, 337-^07. [158,173]</mixed-citation>
        </ref>
        <ref id="d265129e450a1310">
          
            <mixed-citation id="d265129e454" publication-type="other">
Gertheiss, J., and Tutz, G. (2009), "Supervised Feature Selection in Mass Spectrometry Based Proteomic Profiling
by Blockwise Boosting," Bioinformatics, 25, 1076-1077. [173]</mixed-citation>
        </ref>
        <ref id="d265129e464a1310">
          
            <mixed-citation id="d265129e468" publication-type="other">
Hastie, T, and Mallows, C. (1993), Discussion of "A Statistical View of Some Chemometrics Regression Tools,"
by I. E. Frank and J. H. Friedman, Technometrics, 35, 140-143. [156]</mixed-citation>
        </ref>
        <ref id="d265129e478a1310">
          
            <mixed-citation id="d265129e482" publication-type="other">
Hoerl, A. E., and Kennard, R. W. (1970), "Ridge Regression: Biased Estimation for Nonorthogonal Problems,"
Technometrics, 12, 55-67. [156,165]</mixed-citation>
        </ref>
        <ref id="d265129e492a1310">
          
            <mixed-citation id="d265129e496" publication-type="other">
Hurvich, C. M., Simonoff, J. S., and Tsai, C. (1998), "Smoothing Parameter Selection in Nonparametric Regres-
sion Using an Improved Akaike Information Criterion," Journal of the Royal Statistical Society, Ser. B, 60,
271-293.[161]</mixed-citation>
        </ref>
        <ref id="d265129e510a1310">
          
            <mixed-citation id="d265129e514" publication-type="other">
Kramer, N. (2006), "Boosting for Functional Data," in COMPSTAT 2006—Proceedings in Computational Statis-
tics, Heidelberg: Physica, pp. 1121-1128. [172]</mixed-citation>
        </ref>
        <ref id="d265129e524a1310">
          
            <mixed-citation id="d265129e528" publication-type="other">
Land, S. R., and Friedman, J. H. (1997), "Variable Fusion: A New Adaptive Signal Regression Method," Technical
Report 656, Carnegie Mellon University Pittsburg, Dept. of Statistics. [157,158]</mixed-citation>
        </ref>
        <ref id="d265129e538a1310">
          
            <mixed-citation id="d265129e542" publication-type="other">
Martens, H., and Naes, T. (1989), Multivariate Calibration, Chichester: Wiley. [164]</mixed-citation>
        </ref>
        <ref id="d265129e549a1310">
          
            <mixed-citation id="d265129e553" publication-type="other">
Marx, B. D., and Eilers, P. H. C. (1999), "Genaralized Linear Regression on Sampled Signals and Curves: A P-
Spline Approach," Technometrics, 41, 1-13. [156,157,160,165,168,169,172]</mixed-citation>
        </ref>
        <ref id="d265129e563a1310">
          
            <mixed-citation id="d265129e567" publication-type="other">
- (2005), "Multidimensional Penalized Signal Regression," Technometrics, 47, 13-22. [156]</mixed-citation>
        </ref>
        <ref id="d265129e574a1310">
          
            <mixed-citation id="d265129e578" publication-type="other">
Massy, W. F. (1965), "Principal Components Regression in Exploratory Statistical Research," Journal of the
American Statistical Association, 60, 234-256. [164]</mixed-citation>
        </ref>
        <ref id="d265129e589a1310">
          
            <mixed-citation id="d265129e593" publication-type="other">
Osborne, B. G. (2000), "Near-Infrared Spectroscopy in Food Analysis," in Encyclopedia of Analytical Chemistry,
ed. R. A. Meyers, Chichester: Wiley. [169,170]</mixed-citation>
        </ref>
        <ref id="d265129e603a1310">
          
            <mixed-citation id="d265129e607" publication-type="other">
R Development Core Team (2007), R: A Language and Environment for Statistical Computing, Vienna, Austria:
R Foundation for Statistical Computing. [164]</mixed-citation>
        </ref>
        <ref id="d265129e617a1310">
          
            <mixed-citation id="d265129e621" publication-type="other">
Ramsay, J. O., and Silverman, B. W. (2005), Functional Data Analysis (2nd ed.), New York: Springer. [154-157,
165,167,168,172,173]</mixed-citation>
        </ref>
        <ref id="d265129e631a1310">
          
            <mixed-citation id="d265129e635" publication-type="other">
Schapire, R. E. (1990), "The Strength of Weak Learnability," Machine Learning, 5, 197-227. [158]</mixed-citation>
        </ref>
        <ref id="d265129e642a1310">
          
            <mixed-citation id="d265129e646" publication-type="other">
Tibshirani, R. (1996), "Regression Shrinkage and Selection via the Lasso," Journal of the Royal Statistical Soci-
ety, Ser. B, 58, 267-288. [156,157,164]</mixed-citation>
        </ref>
        <ref id="d265129e656a1310">
          
            <mixed-citation id="d265129e660" publication-type="other">
Tibshirani, R., Saunders, M., Rosset, S., Zhu, J., and Kneight, K. (2005), "Sparsity and Smoothness via the Fused
Lasso," Journal of the Royal Statistical Society, Ser. B, 67, 91-108. [156,158,165,170]</mixed-citation>
        </ref>
        <ref id="d265129e671a1310">
          
            <mixed-citation id="d265129e675" publication-type="other">
Wold, H. (1975), "Soft Modeling by Latent Variables: The Nonlinear Partial Least Squares Approach," in Per-
spectives in Probability and Statistics, Papers in Honour of M. S. Bartlett, ed. J. Gani, London: Academic
Press. [164]</mixed-citation>
        </ref>
        <ref id="d265129e688a1310">
          
            <mixed-citation id="d265129e692" publication-type="other">
Zou, H., and Hastie, T. (2005), "Regularization and Variable Selection via the Elastic Net," Journal of the Royal
Statistical Society, Ser. B, 67, 301-320. [156,165]</mixed-citation>
        </ref>
      </ref-list>
    
  </back>
</article>


