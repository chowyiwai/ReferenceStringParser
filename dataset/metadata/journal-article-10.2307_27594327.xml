<?xml version="1.0" encoding="UTF-8"?>


<article dtd-version="1.0" article-type="research-article">
  <front>
      <journal-meta>
         <journal-id xmlns:xlink="http://www.w3.org/1999/xlink" journal-id-type="jstor">jcompgrapstat</journal-id>
         <journal-id xmlns:xlink="http://www.w3.org/1999/xlink" journal-id-type="jstor">j100879</journal-id>
         <journal-title-group xmlns:xlink="http://www.w3.org/1999/xlink">
            <journal-title>Journal of Computational and Graphical Statistics</journal-title>
         </journal-title-group>
      
         <publisher>
            <publisher-name>American Statistical Association, Institute of Mathematical Statistics, and Interface Foundation of North America</publisher-name>
         </publisher>
         <issn xmlns:xlink="http://www.w3.org/1999/xlink" pub-type="ppub">10618600</issn>
         <custom-meta-group xmlns:xlink="http://www.w3.org/1999/xlink"/>
      </journal-meta>
      <article-meta xmlns:xlink="http://www.w3.org/1999/xlink">
         <volume xmlns:mml="http://www.w3.org/1998/Math/MathML"
                 xmlns:oasis="http://docs.oasis-open.org/ns/oasis-exchange/table">17</volume>
         <issue xmlns:mml="http://www.w3.org/1998/Math/MathML"
                xmlns:oasis="http://docs.oasis-open.org/ns/oasis-exchange/table">3</issue>
         <issue-id>i27594322</issue-id>
         <article-id pub-id-type="jstor">27594327</article-id>
         <article-id pub-id-type="pub-doi">10.1198/106186008X345161</article-id>
         <article-categories>
            <subj-group>
               <subject>Machine Learning and Classification</subject>
            </subj-group>
         </article-categories>
         <title-group>
            <article-title>Kernel Sliced Inverse Regression with Applications to Classification</article-title>
         </title-group>
         <contrib-group>
            <contrib>
               <string-name>
                  <given-names>Han-Ming</given-names>
                  <surname>Wu</surname>
               </string-name>
            </contrib>
         </contrib-group>
         <pub-date pub-type="ppub">
            <day>1</day>
            <month>9</month>
            <year>2008</year>
         </pub-date>
         <fpage>590</fpage>
         <lpage>610</lpage>
      
      
      
      
      
      
         <permissions>
            <copyright-statement>Copyright 2008 American Statistical Association, the Institute of Mathematical Statistics, and the Interface Foundation of North America</copyright-statement>
         </permissions>
      
         <self-uri xlink:href="https://www.jstor.org/stable/27594327"/>
      
      
         <abstract>
            <p>Sliced inverse regression (SIR) was introduced by Li to find the effective dimension reduction directions for exploring the intrinsic structure of high-dimensional data. In this study, we propose a hybrid SIR method using a kernel machine which we call kernel SIR. The kernel mixtures result in the transformed data distribution being more Gaussian like and symmetric; providing more suitable conditions for performing SIR analysis. The proposed method can be regarded as a nonlinear extension of the SIR algorithm. We provide a theoretical description of the kernel SIR algorithm within the framework of reproducing kernel Hilbert space (RKHS). We also illustrate that kernel SIR performs better than several standard methods for discriminative, visualization, and regression purposes. We show how the features found with kernel SIR can be used for classification of microarray data and several other classification problems and compare the results with those obtained with several existing dimension reduction techniques. The results show that kernel SIR is a powerful nonlinear feature extractor for classification problems.</p>
         </abstract>
         <kwd-group>
            <kwd>Dimension reduction</kwd>
            <kwd>Kernel machines</kwd>
            <kwd>Reproducing kernel Hilbert space</kwd>
            <kwd>Visualization</kwd>
         </kwd-group>
         <custom-meta-group>
            <custom-meta>
               <meta-name>lang</meta-name>
               <meta-value>eng</meta-value>
            </custom-meta>
         </custom-meta-group>
      </article-meta>
  </front>
  <back>
    
      <ref-list>
        <title>References</title>
        <ref id="d774193e198a1310">
          
            <mixed-citation id="d774193e202" publication-type="other">
Aizerman, M., Braverman, E., and Rozonoer, L. (1964), "Theoretical Foundations of The Potential Function
Method in Pattern Recognition Learning," Automation and Remote Control, 25, 821–837.</mixed-citation>
        </ref>
        <ref id="d774193e212a1310">
          
            <mixed-citation id="d774193e216" publication-type="other">
Bach, F., and Jordan, M. (2002), "Kernel Independent Component Analysis," Journal of Machine Learning Re-
search, 3, 1–8.</mixed-citation>
        </ref>
        <ref id="d774193e226a1310">
          
            <mixed-citation id="d774193e230" publication-type="other">
Baudat, G., and Anouar, F. (2000), "Generalized Discriminant Analysis Using a Kernel Approach," Neural Com-
putation, 12, 2385–2404.</mixed-citation>
        </ref>
        <ref id="d774193e240a1310">
          
            <mixed-citation id="d774193e244" publication-type="other">
Berlinet, A., and Thomas-Agnan, C. (2004), Reproducing Kernel Hubert Spaces in Probability and Statistics,
Boston: Kluwer Academic Publishers.</mixed-citation>
        </ref>
        <ref id="d774193e255a1310">
          
            <mixed-citation id="d774193e259" publication-type="other">
Boser, B. E., Guyon, I. M., and Vapnik, V. (1992), "A Training Algorithm for Optimum Margin Classifiers," in
Fifth Annual Workshop on Computational Learning Theory, Pittsburgh, ACM.</mixed-citation>
        </ref>
        <ref id="d774193e269a1310">
          
            <mixed-citation id="d774193e273" publication-type="other">
Bura, E., and Pfeiffer, R. M. (2003), "Graphical Methods for Class Prediction Using Dimension Reduction Tech-
niques on DNA Microarray Data," Bioinformatics, 19, 1252–1258.</mixed-citation>
        </ref>
        <ref id="d774193e283a1310">
          
            <mixed-citation id="d774193e287" publication-type="other">
Chang, C. C, and Lin, C. J. (2004), "LIBSVM: a Library for Support Vector Machines," software available at
http://www. csie. ntu. edu. tw/~cjlin/libsvm.</mixed-citation>
        </ref>
        <ref id="d774193e297a1310">
          
            <mixed-citation id="d774193e301" publication-type="other">
Chen, C. H., and Li, K. C. (1998), "Can SIR be as Popular as Multiple Linear Regression?" Statistica Sínica, 8,
289–316.</mixed-citation>
        </ref>
        <ref id="d774193e311a1310">
          
            <mixed-citation id="d774193e315" publication-type="other">
—(2001), "Generalization of Fisher's Linear Discriminant Analysis via the Approach of Sliced Inverse
Regression," Journal of the Korean Statistical Society, 30, 193–217.</mixed-citation>
        </ref>
        <ref id="d774193e325a1310">
          
            <mixed-citation id="d774193e329" publication-type="other">
Cook, R. D. (1994), "On the Interpretation of Regression Plots," Journal of the American Statistical Association,
89,177–190.</mixed-citation>
        </ref>
        <ref id="d774193e340a1310">
          
            <mixed-citation id="d774193e344" publication-type="other">
Cook, R. D. (1996), "Graphics for Regressions With a Binary Response," Journal of the American Statistical
Association, 91, 983–992.</mixed-citation>
        </ref>
        <ref id="d774193e354a1310">
          
            <mixed-citation id="d774193e358" publication-type="other">
Cook, R. D., and Yin, X. (2001), "Dimension-reduction and Visualization in Discriminant Analysis," Australian
and New Zealand Journal of Statistics, 43, 147–200.</mixed-citation>
        </ref>
        <ref id="d774193e368a1310">
          
            <mixed-citation id="d774193e372" publication-type="other">
Cook, R. D., and Weisberg, S. (1991), Discussion of "Sliced Inverse Regression For Dimension Reduction," by
Li, Journal of the American Statistical Association, 86, 328–332.</mixed-citation>
        </ref>
        <ref id="d774193e382a1310">
          
            <mixed-citation id="d774193e386" publication-type="other">
Cristianini, N., and Shawe-Taylor, J. (2000), An Introduction to Support Vector Machines, Cambridge, UK: Cam-
bridge University Press.</mixed-citation>
        </ref>
        <ref id="d774193e396a1310">
          
            <mixed-citation id="d774193e400" publication-type="other">
Dettling, M., and Bühlmann, P. (2002), "Supervised Clustering of Genes," Genome Biology, 3( 12), research0069.1 -
0069.15.</mixed-citation>
        </ref>
        <ref id="d774193e410a1310">
          
            <mixed-citation id="d774193e414" publication-type="other">
Duan, K., Keerthi, S. S., and Poo, A. N. (2003), "Evaluation of Simple Performance Measures for Tuning SVM
Hyperparameters," Neurocomputing, 51, 41–59.</mixed-citation>
        </ref>
        <ref id="d774193e425a1310">
          
            <mixed-citation id="d774193e429" publication-type="other">
Fukumizu, K., Bach, F. R., and Jordan, M. I. (2004), "Dimensionality Reduction for Supervised Learning With
Reproducing Kernel Hubert Spaces," Journal of Machine Learning Research, 5, 73–99.</mixed-citation>
        </ref>
        <ref id="d774193e439a1310">
          
            <mixed-citation id="d774193e443" publication-type="other">
Girosi, F. (1998), "An Equivalence Between Sparse Approximation and Support Vector Machines," Neural Com-
putation, 10, 1455–1480.</mixed-citation>
        </ref>
        <ref id="d774193e453a1310">
          
            <mixed-citation id="d774193e457" publication-type="other">
Hastie, T., Tibshirani, R., and Friedman, J. (eds.) (2001), The Elements of Statistical Learning: Data Mining,
Inference, and Prediction, New York: Springer.</mixed-citation>
        </ref>
        <ref id="d774193e467a1310">
          
            <mixed-citation id="d774193e471" publication-type="other">
Hsu, C, Chang, C, and Lin, C. J. (2003), "A Practical Guide to Support Vector Classification," Dept. of Computer
Science and Information Engineering, National Taiwan University. Available online at http://www.csie.ntu.
edu.tw/^cjlin/.</mixed-citation>
        </ref>
        <ref id="d774193e484a1310">
          
            <mixed-citation id="d774193e488" publication-type="other">
Huang, S. Y, and Hwang, C. R. (2006), "Kernel Fisher Discriminant Analysis in Gaussian Reproducing Kernel
Hubert Space: Theory," unpublished manuscript, http://www.stat.sinica.edu.tw/syhuang.</mixed-citation>
        </ref>
        <ref id="d774193e498a1310">
          
            <mixed-citation id="d774193e502" publication-type="other">
Jolliffe, I. T. (ed.) (1986), Principal Component Analysis, New York: Springer.</mixed-citation>
        </ref>
        <ref id="d774193e510a1310">
          
            <mixed-citation id="d774193e514" publication-type="other">
Keerthi, S. S., and Lin, C.-J. (2003), "Asymptotic Behaviors of Support Vector Machines With Gaussian Kernel,"
Neural Computation, 15, 1667–1689.</mixed-citation>
        </ref>
        <ref id="d774193e524a1310">
          
            <mixed-citation id="d774193e528" publication-type="other">
Kent, J. T. (1991), Discussion of "Sliced Inverse Regression For Dimension Reduction," by Li, Journal of the
American Statistical Association, 86, 336–337.</mixed-citation>
        </ref>
        <ref id="d774193e538a1310">
          
            <mixed-citation id="d774193e542" publication-type="other">
Kuss, M., and Graepel, T. (2003), "The Geometry of Kernel Canonical Correlation Analysis," Technical report,
Max Planck Institute for Biological Cybernetics, Tübingen, Germany. Available online at http://research.
microsoft, com/~thoreg.</mixed-citation>
        </ref>
        <ref id="d774193e555a1310">
          
            <mixed-citation id="d774193e559" publication-type="other">
Lai, P. L., and Fyfe, C. (2000), "Kernel and Nonlinear Canonical Correlation Analysis," International Journal of
Neural Systems, 10(5), 365–377.</mixed-citation>
        </ref>
        <ref id="d774193e569a1310">
          
            <mixed-citation id="d774193e573" publication-type="other">
Lee, Y J., and Huang, S. Y (2007), "Reduced Support Vector Machines: a Statistical Theory," IEEE Transactions
on Neural Networks, 18, 1–13.</mixed-citation>
        </ref>
        <ref id="d774193e583a1310">
          
            <mixed-citation id="d774193e587" publication-type="other">
Li, K. C. (1991), "Sliced Inverse Regression For Dimension Reduction," Journal of The American Statistical
Association, 86, 316–342.</mixed-citation>
        </ref>
        <ref id="d774193e598a1310">
          
            <mixed-citation id="d774193e602" publication-type="other">
—(1992), "On Principal Hessian Directions for Data Visualization and Dimension Reduction: Another
Application of Stein's Lemma," Journal of the American Statistical Association, 87, 1025–1039.</mixed-citation>
        </ref>
        <ref id="d774193e612a1310">
          
            <mixed-citation id="d774193e616" publication-type="other">
Mika, S., Ratsch, G., Weston, J., Schölkopf, B., and Müller, K.-R. (1999), "Fisher Discriminant Analysis With
Kernels," in Proceedings of IEEE Neural Networks for Signal Processing Workshop.</mixed-citation>
        </ref>
        <ref id="d774193e626a1310">
          
            <mixed-citation id="d774193e630" publication-type="other">
Murphy, P. M., and Aha, D. W. (1993), UCI Repository of Machine Learning Databases. University of California,
Department of Information and Computer Science, Irvine, CA.</mixed-citation>
        </ref>
        <ref id="d774193e640a1310">
          
            <mixed-citation id="d774193e644" publication-type="other">
Pekalska, E., Paclik, P., and Duin, R. P. W. (2001), "A Generalized Kernel Approach to Dissimilarity-based
Classification," Journal of Machine Learning Research, 2, 175–211.</mixed-citation>
        </ref>
        <ref id="d774193e654a1310">
          
            <mixed-citation id="d774193e658" publication-type="other">
Rosipal, R., and Trejo, L. T. (2001), "Kernel Partial Least Squares Regression in Reproducing Kernel Hubert
Space," Journal of Machine Learning Research, 2, 97–123.</mixed-citation>
        </ref>
        <ref id="d774193e668a1310">
          
            <mixed-citation id="d774193e672" publication-type="other">
Roth, V, and Steinhage, V(2000), "Nonlinear Discriminant Analysis Using Kernel Functions," in Advances in
Neural Information Processing Systems, Cambridge, MA: MIT Press, pp. 568–574.</mixed-citation>
        </ref>
        <ref id="d774193e683a1310">
          
            <mixed-citation id="d774193e687" publication-type="other">
Schölkopf, B., and Smola, A. J. (eds.) (2002), Learning With Kernels: Support Vector Machines, Regularization,
Optimization, and Beyond, Cambridge, MA: MIT Press.</mixed-citation>
        </ref>
        <ref id="d774193e697a1310">
          
            <mixed-citation id="d774193e701" publication-type="other">
Schölkopf, B., Smola, A., and Müller, K. R. (1998), "Nonlinear Component Analysis as a Kernel Eigenvalue
Problem," Neural Computation, 10, 1299–1319.</mixed-citation>
        </ref>
        <ref id="d774193e711a1310">
          
            <mixed-citation id="d774193e715" publication-type="other">
—( 1999), "Kernel Principal Component Analysis," in Advances in Kernel Methods: Support Vector Learn-
ing, Cambridge, MA: MIT Press, pp. 327–352.</mixed-citation>
        </ref>
        <ref id="d774193e725a1310">
          
            <mixed-citation id="d774193e729" publication-type="other">
Schölkopf, B., Smola, A., Williamson, R. C, and Bartlett, P. L. (2000), "New Support Vector Algorithms," Neural
Computation, 12, 1207–45.</mixed-citation>
        </ref>
        <ref id="d774193e739a1310">
          
            <mixed-citation id="d774193e743" publication-type="other">
Schölkopf, B., Tsuda, K., and Vert, J.-P. (eds.) (2004), Kernel Methods in Computational Biology, Cambridge,
MA: MIT Press.</mixed-citation>
        </ref>
        <ref id="d774193e753a1310">
          
            <mixed-citation id="d774193e757" publication-type="other">
Smola, A. J., and Schölkopf B. (2000), "Sparse Greedy Matrix Approximation for Machine Learning," in Pro-
ceedings of the 17 th International Conference on Machine Learning, Stanford University, CA, Morgan Kauf-
mann Publishers, pp. 911–918.</mixed-citation>
        </ref>
        <ref id="d774193e771a1310">
          
            <mixed-citation id="d774193e775" publication-type="other">
Tsuda, K. (1999), "Support Vector Classifier With Asymmetric Kernel Functions," in Processing of the Seventh
European Symposium on Artificial Neural Networks (ESANN), pp. 183–188.</mixed-citation>
        </ref>
        <ref id="d774193e785a1310">
          
            <mixed-citation id="d774193e789" publication-type="other">
Vapnik, V (ed.) (1995), The Nature of Statistical Learning Theory, New York: Springer-Verlag.</mixed-citation>
        </ref>
        <ref id="d774193e796a1310">
          
            <mixed-citation id="d774193e800" publication-type="other">
Williams, C, and Seeger, M. (2001), "Using the Nyström Method to Speed up Kernel Machines," in Advances in
Neural Information Processing Systems, 13, eds. T.K. Leen, T. G. Dietterich, and V. Tresp, Cambridge, MA:
MIT Press, pp. 682–688.</mixed-citation>
        </ref>
        <ref id="d774193e813a1310">
          
            <mixed-citation id="d774193e817" publication-type="other">
Wu, H. M., and Lu, H. H.-S. (2004), "Supervised Motion Segmentation by Spatial-Frequential Analysis and
Dynamic Sliced Inverse Regression," Statistica Sinica, 14, 413–430.</mixed-citation>
        </ref>
      </ref-list>
    
  </back>
</article>


