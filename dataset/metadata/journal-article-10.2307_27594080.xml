<?xml version="1.0" encoding="UTF-8"?>

<article xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
         xmlns:xlink="http://www.w3.org/1999/xlink"
         xmlns:mml="http://www.w3.org/1998/Math/MathML"
         dtd-version="1.0"
         article-type="research-article">
   <front>
      <journal-meta>
         <journal-id journal-id-type="jstor">jcompgrapstat</journal-id>
         <journal-id journal-id-type="jstor">j100879</journal-id>
         <journal-title-group>
            <journal-title>Journal of Computational and Graphical Statistics</journal-title>
         </journal-title-group>
         
         <publisher>
            <publisher-name>American Statistical Association, Institute of Mathematical Statistics, and Interface Foundation of North America</publisher-name>
         </publisher>
         <issn pub-type="ppub">10618600</issn>
         <custom-meta-group/>
      </journal-meta>
      <article-meta>
         <volume xmlns:oasis="http://docs.oasis-open.org/ns/oasis-exchange/table">13</volume>
         <issue xmlns:oasis="http://docs.oasis-open.org/ns/oasis-exchange/table">4</issue>
         <issue-id>i27594074</issue-id>
         <article-id pub-id-type="jstor">27594080</article-id>
         <article-id pub-id-type="pub-doi">10.1198/106186004X13064</article-id>
         <title-group>
            <article-title>LOTUS: An Algorithm for Building Accurate and Comprehensible Logistic Regression Trees</article-title>
          </title-group>
         <contrib-group>
            <contrib>
              <string-name>
                  <given-names>Kin-Yee</given-names>
                  <surname>Chan</surname>
              </string-name>
            </contrib>
            <contrib>
              <string-name>
                  <given-names>Wei-Yin</given-names>
                  <surname>Loh</surname>
              </string-name>
            </contrib>
          </contrib-group>
         <pub-date pub-type="ppub">
            <day>1</day>
            <month>12</month>
            <year>2004</year>
         </pub-date>
         <fpage>826</fpage>
         <lpage>852</lpage>
         
         
         <permissions>
            <copyright-statement>Copyright 2004 American Statistical Association, the Institute of Mathematical Statistics, and the Interface Foundation of North America</copyright-statement>
         </permissions>
         <self-uri xlink:href="https://www.jstor.org/stable/27594080"/>
         
         
         <abstract>
            <p>Logistic regression is a powerful technique for fitting models to data with a binary response variable, but the models are difficult to interpret if collinearity, nonlinearity, or interactions are present. Besides, it is hard to judge model adequacy because there are few diagnostics for choosing variable transformations and no true goodness-of-fit test. To overcome these problems, this article proposes to fit a piecewise (multiple or simple) linear logistic regression model by recursively partitioning the data and fitting a different logistic regression in each partition. This allows nonlinear features of the data to be modeled without requiring variable transformations. The binary tree that results from the partitioning process is pruned to minimize a cross-validation estimate of the predicted deviance. This obviates the need for a formal goodness-of-fit test. The resulting model is especially easy to interpret if a simple linear logistic regression is fitted to each partition, because the tree structure and the set of graphs of the fitted functions in the partitions comprise a complete visual description of the model. Trend-adjusted chi-square tests are used to control bias in variable selection at the intermediate nodes. This protects the integrity of inferences drawn from the tree structure. The method is compared with standard stepwise logistic regression on 30 real datasets, with several containing tens to hundreds of thousands of observations. Averaged across the datasets, the results show that the method reduces predicted mean deviance by 9% to 16%. We use an example from the Dutch insurance industry to demonstrate how the method can identify and produce an intelligible profile of prospective customers.</p>
          </abstract>
         <kwd-group>
            <kwd>Piecewise linear logistic regression</kwd>
            <kwd>Recursive partitioning</kwd>
            <kwd>Trend-adjusted chi-square test</kwd>
            <kwd>Unbiased variable selection</kwd>
          </kwd-group>
         <custom-meta-group>
            <custom-meta>
              <meta-name>lang</meta-name>
              <meta-value>eng</meta-value>
            </custom-meta>
          </custom-meta-group>
      </article-meta>
   </front>
   <back>
        
         <ref-list>
            <title>References</title>
            <ref id="d1866969e162a1310">
               
               <mixed-citation id="d1866969e166" publication-type="other">
Akaike, H. (1974), "A New Look at Statistical Model Identification," IEEE Transactions on Automatic Control,
AU-19, 716–722.</mixed-citation>
            </ref>
            <ref id="d1866969e176a1310">
               
               <mixed-citation id="d1866969e180" publication-type="other">
Allison, P. D. (1999), Logistic Regression Using the SAS System: Theory and Application, Cary, NC: SAS Institute,
Inc.</mixed-citation>
            </ref>
            <ref id="d1866969e190a1310">
               
               <mixed-citation id="d1866969e194" publication-type="other">
Armitage, P. (1955), "Tests for Linear Trends in Proportions and Frequencies," Biometrics, 11, 375–386.</mixed-citation>
            </ref>
            <ref id="d1866969e201a1310">
               
               <mixed-citation id="d1866969e205" publication-type="other">
Blake, C., and Merz, C. J. (2000), "UCI Repository of Machine Learning Databases," Technical Report, Department
of Information and Computer Science, University of California, Irvine. Available at http://www.ics.uci.edu/
~mlearn/MLRepository.html.</mixed-citation>
            </ref>
            <ref id="d1866969e219a1310">
               
               <mixed-citation id="d1866969e223" publication-type="other">
Breiman, L., Friedman, J. H., Olshen, R. A., and Stone, C. J. (1984), Classification and Regression Trees, Belmont,
CA: Wadsworth.</mixed-citation>
            </ref>
            <ref id="d1866969e233a1310">
               
               <mixed-citation id="d1866969e237" publication-type="other">
Chaudhuri, P., Huang, M.-C, Loh, W.-Y., and Yao, R. (1994), "Piecewise-Polynomial Regression Trees," Statistica
Sinica, 4, 143–167.</mixed-citation>
            </ref>
            <ref id="d1866969e247a1310">
               
               <mixed-citation id="d1866969e251" publication-type="other">
Chaudhuri, P., Lo, W.-D., Loh, W.-Y, and Yang, C.-C. (1995), "Generalized Regression Trees," Statistica Sinica,
5,641–666.</mixed-citation>
            </ref>
            <ref id="d1866969e261a1310">
               
               <mixed-citation id="d1866969e265" publication-type="other">
Clark, L. A., and Pregibon, D. (1992), "Tree-Based Models," in Statistical Models in S, eds. J. M. Chambers and
T. J. Hastie, Pacific Grove, CA: Wadsworth, pp. 377–419.</mixed-citation>
            </ref>
            <ref id="d1866969e275a1310">
               
               <mixed-citation id="d1866969e279" publication-type="other">
Cochran, W. G. (1954), "Some Methods of Strengthening the Common x2 Tests," Biometrics, 10, 417–451.</mixed-citation>
            </ref>
            <ref id="d1866969e286a1310">
               
               <mixed-citation id="d1866969e290" publication-type="other">
Cook, R. D., and Weisberg, S. (1999), Applied Regression Including Computing and Graphics, New York: Wiley.
Hosmer, D. W., and Lemeshow, S. (1989), Applied Logistic Regression, New York: Wiley.</mixed-citation>
            </ref>
            <ref id="d1866969e301a1310">
               
               <mixed-citation id="d1866969e305" publication-type="other">
Kim, H., and Loh, W.-Y (2001), "Classification Trees with Unbiased Multiway Splits," Journal of the American
Statistical Association, 96, 598–604.</mixed-citation>
            </ref>
            <ref id="d1866969e315a1310">
               
               <mixed-citation id="d1866969e319" publication-type="other">
Le, C. T. (1998), Applied Categorical Data Analysis, New York: Wiley.</mixed-citation>
            </ref>
            <ref id="d1866969e326a1310">
               
               <mixed-citation id="d1866969e330" publication-type="other">
Lim, T.-S., Loh, W.-Y, and Shih, Y-S. (2000), "A Comparison of Prediction Accuracy, Complexity, and Training
Time of Thirty-Three Old and New Classification Algorithms," Machine Learning, 40, 203–228.</mixed-citation>
            </ref>
            <ref id="d1866969e340a1310">
               
               <mixed-citation id="d1866969e344" publication-type="other">
Loh, W.-Y (2002), "Regression Trees with Unbiased Variable Selection and Interaction Detection," Statistica
Sinica, 12, 361–386.</mixed-citation>
            </ref>
            <ref id="d1866969e354a1310">
               
               <mixed-citation id="d1866969e358" publication-type="other">
Loh, W.-Y, and Shih, Y-S. (1997), "Split Selection Methods for Classification Trees," Statistica Sinica, 7, 815-
840.</mixed-citation>
            </ref>
            <ref id="d1866969e368a1310">
               
               <mixed-citation id="d1866969e372" publication-type="other">
McCullagh, P., and Neider, J. A. (1989), Generalized Linear Models (2nd ed.), London: Chapman &amp;amp; Hall.</mixed-citation>
            </ref>
            <ref id="d1866969e380a1310">
               
               <mixed-citation id="d1866969e384" publication-type="other">
Miller, R. G., Jr. (1981), Simultaneous Statistical Inference (2nd ed.), New York: Springer.</mixed-citation>
            </ref>
            <ref id="d1866969e391a1310">
               
               <mixed-citation id="d1866969e395" publication-type="other">
Perlich, C, Provost, F., and Simonoff, J. (2003), "Tree Induction vs. Logistic Regression: A Learning-Curve
Analysis," Journal of Machine Learning Research, 4, 211–255.</mixed-citation>
            </ref>
            <ref id="d1866969e405a1310">
               
               <mixed-citation id="d1866969e409" publication-type="other">
van der Putten, P., de Ruiter, M., and van Someren, M. (2000), "CoIL Challenge 2000 Tasks and Results: Predicting
and Explaining Caravan Policy Ownership," CoIL Challenge 2000: The Insurance Company Case, eds. P.
van der Putten and M. van Someren, Amsterdam: Sentient Machine Research. Also a Leiden Institute of
Advanced Computer Science Technical Report 2000–09, June 22, 2000.</mixed-citation>
            </ref>
            <ref id="d1866969e425a1310">
               
               <mixed-citation id="d1866969e429" publication-type="other">
Quinlan, J. R. (1992), "Learning with Continuous Classes," in Proceedings ofAI'92 Australian National Confer-
ence on Artificial Intelligence, Singapore: World Scientific, pp. 343–348.</mixed-citation>
            </ref>
            <ref id="d1866969e439a1310">
               
               <mixed-citation id="d1866969e443" publication-type="other">
Schafgans, M. M. A. (1998), "Ethnic Wage Differences in Malaysia: Parametric and Semiparametric Estimation
of the Chinese-Malay Wage Gap," Journal of Applied Econometrics, 13, 481–504.</mixed-citation>
            </ref>
            <ref id="d1866969e453a1310">
               
               <mixed-citation id="d1866969e457" publication-type="other">
Steinberg, D., and Cardell, N. S. (1998), "The Hybrid CART-Logit Model in Classification and Data Mining,"
Eighth Annual Advanced Research Techniques Forum, American Marketing Association, Keystone, CO.</mixed-citation>
            </ref>
            <ref id="d1866969e468a1310">
               
               <mixed-citation id="d1866969e472" publication-type="other">
Venables, W. N., and Ripley, B. D. (1999), Modern Applied Statistics with S-Plus (3rd ed.), New York: Springer.</mixed-citation>
            </ref>
            <ref id="d1866969e479a1310">
               
               <mixed-citation id="d1866969e483" publication-type="other">
Weiss, S. M., and Indurkhya, N. (1995), "Rule-Based Machine Learning Methods for Functional Prediction,"
Journal of Artificial Intelligence Research, 3, 383–403.</mixed-citation>
            </ref>
         </ref-list>
      
      </back>
</article>
