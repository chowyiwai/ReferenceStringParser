

<book xmlns:oasis="http://docs.oasis-open.org/ns/oasis-exchange/table"
      xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
      xmlns:xlink="http://www.w3.org/1999/xlink"
      xmlns:mml="http://www.w3.org/1998/Math/MathML"
      dtd-version="0.2"
      xml:lang="eng">
   <collection-meta>
      <collection-id collection-id-type="jstor">j.ctt58dnjw</collection-id>
   </collection-meta>
   <book-meta>
      <book-id book-id-type="doi">10.2307/j.ctvrxk1hs</book-id>
      <subj-group>
         <subject content-type="call-number">QB51.3.E43 I94 2020</subject>
      </subj-group>
      <subj-group>
         <subject content-type="lcsh">Astronomy</subject>
         <subj-group>
            <subject content-type="lcsh">Data processing</subject>
         </subj-group>
      </subj-group>
      <subj-group>
         <subject content-type="lcsh">Statistical astronomy</subject>
      </subj-group>
      <subj-group>
         <subject content-type="lcsh">Python (Computer program language)</subject>
      </subj-group>
      <subj-group subj-group-type="discipline">
         <subject>Astronomy</subject>
         <subject>Physics</subject>
         <subject>Computer Science</subject>
      </subj-group>
      <book-title-group>
         <book-title>Statistics, Data Mining, and Machine Learning in Astronomy</book-title>
         <subtitle>A Practical Python Guide for the Analysis of Survey Data, Updated Edition</subtitle>
      </book-title-group>
      <contrib-group>
         <contrib contrib-type="author" id="contrib1">
            <name name-style="western">
               <surname>Ivezić</surname>
               <given-names>Željko</given-names>
            </name>
         </contrib>
         <contrib contrib-type="author" id="contrib2">
            <name name-style="western">
               <surname>Connolly</surname>
               <given-names>Andrew J.</given-names>
            </name>
         </contrib>
         <contrib contrib-type="author" id="contrib3">
            <name name-style="western">
               <surname>VanderPlas</surname>
               <given-names>Jacob T.</given-names>
            </name>
         </contrib>
         <contrib contrib-type="author" id="contrib4">
            <name name-style="western">
               <surname>Gray</surname>
               <given-names>Alexander</given-names>
            </name>
         </contrib>
      </contrib-group>
      <pub-date>
         <day>03</day>
         <month>12</month>
         <year>2019</year>
      </pub-date>
      <isbn content-type="ppub">9780691198309</isbn>
      <isbn content-type="epub">9780691197050</isbn>
      <isbn content-type="epub">0691197059</isbn>
      <publisher>
         <publisher-name>Princeton University Press</publisher-name>
         <publisher-loc>New Jersey</publisher-loc>
      </publisher>
      <edition>REV - Revised</edition>
      <permissions>
         <copyright-year>2020</copyright-year>
         <copyright-holder>Princeton University Press</copyright-holder>
      </permissions>
      <self-uri xlink:href="https://www.jstor.org/stable/j.ctvrxk1hs"/>
      <abstract abstract-type="short">
         <p> &lt;p&gt;<italic>Statistics, Data Mining, and Machine Learning in Astronomy</italic> is the essential introduction to the statistical methods needed to analyze complex data sets from astronomical surveys such as the Panoramic Survey Telescope and Rapid Response System, the Dark Energy Survey, and the Large Synoptic Survey Telescope. Now fully updated, it presents a wealth of practical analysis problems, evaluates the techniques for solving them, and explains how to use various approaches for different types and sizes of data sets. Python code and sample data sets are provided for all applications described in the book. The supporting data sets have been carefully selected from contemporary astronomical surveys and are easy to download and use. The accompanying Python code is publicly available, well documented, and follows uniform coding standards. Together, the data sets and code enable readers to reproduce all the figures and examples, engage with the different methods, and adapt them to their own fields of interest.  &lt;p&gt;An accessible textbook for students and an indispensable reference for researchers, this updated edition features new sections on deep learning methods, hierarchical Bayes modeling, and approximate Bayesian computation. The chapters have been revised throughout and the astroML code has been brought completely up to date.  &lt;ul&gt; &lt;li&gt;Fully revised and expanded    &lt;li&gt;Describes the most useful statistical and data-mining methods for extracting knowledge from huge and complex astronomical data sets   &lt;li&gt;Features real-world data sets from astronomical surveys   &lt;li&gt;Uses a freely available Python codebase throughout   &lt;li&gt;Ideal for graduate students, advanced undergraduates, and working astronomers        </p>
      </abstract>
      <custom-meta-group>
         <custom-meta>
            <meta-name>
                    lang
                </meta-name>
            <meta-value>eng</meta-value>
         </custom-meta>
      </custom-meta-group>
   </book-meta>
   <body>
      <book-part indexed="true" xlink:type="simple">
         <body>
            <book-part>
               <book-part-meta>
                  <book-part-id book-part-id-type="doi">10.2307/j.ctvrxk1hs.1</book-part-id>
                  <title-group>
                     <title>Front Matter</title>
                  </title-group>
                  <fpage>i</fpage>
               </book-part-meta>
            </book-part>
         </body>
      </book-part>
      <book-part indexed="true" xlink:type="simple">
         <body>
            <book-part>
               <book-part-meta>
                  <book-part-id book-part-id-type="doi">10.2307/j.ctvrxk1hs.2</book-part-id>
                  <title-group>
                     <title>Table of Contents</title>
                  </title-group>
                  <fpage>v</fpage>
               </book-part-meta>
            </book-part>
         </body>
      </book-part>
      <book-part indexed="true" xlink:type="simple">
         <body>
            <book-part>
               <book-part-meta>
                  <book-part-id book-part-id-type="doi">10.2307/j.ctvrxk1hs.3</book-part-id>
                  <title-group>
                     <title>Preface</title>
                  </title-group>
                  <contrib-group>
                     <contrib contrib-type="author" id="contrib1" xlink:type="simple">
                        <collab>The Authors</collab>
                     </contrib>
                  </contrib-group>
                  <fpage>ix</fpage>
               </book-part-meta>
            </book-part>
         </body>
      </book-part>
      <book-part indexed="true" xlink:type="simple">
         <body>
            <book-part>
               <book-part-meta>
                  <book-part-id book-part-id-type="doi">10.2307/j.ctvrxk1hs.4</book-part-id>
                  <title-group>
                     <label>1</label>
                     <title>About the Book and Supporting Material</title>
                  </title-group>
                  <fpage>3</fpage>
                  <abstract>
                     <p>This chapter introduces terminology and nomenclature, reviews a few relevant contemporary books, briefly describes the Python programming language and the Git code management tool, and provides details about data sets used in examples throughout the book.</p>
                     <p>
                        <italic>Data mining</italic>, <italic>machine learning</italic>, and <italic>knowledge discovery</italic> refer to research areas which can all be thought of as outgrowths of multivariate statistics. Their common themes are analysis and interpretation of data, often involving large quantities of data, and even more often resorting to numerical methods. The rapid development of these fields over the last few decades was led by computer scientists, often in collaboration</p>
                  </abstract>
               </book-part-meta>
            </book-part>
         </body>
      </book-part>
      <book-part indexed="true" xlink:type="simple">
         <body>
            <book-part>
               <book-part-meta>
                  <book-part-id book-part-id-type="doi">10.2307/j.ctvrxk1hs.5</book-part-id>
                  <title-group>
                     <label>2</label>
                     <title>Fast Computation on Massive Data Sets</title>
                  </title-group>
                  <fpage>41</fpage>
                  <abstract>
                     <p>This chapter describes basic concepts and tools for tractably performing the computations described in the rest of this book. The need for fast algorithms for such analysis subroutines is becoming increasingly important as modern data sets are approaching billions of objects. With such data sets, even analysis operations whose computational cost is linearly proportional to the size of the data set present challenges, particularly since statistical analyses are inherently interactive processes, requiring that computations complete within some reasonable human attention span. For more sophisticated machine learning algorithms, the often worse-than-linear runtimes of straightforward implementations become quickly unbearable. In this chapter</p>
                  </abstract>
               </book-part-meta>
            </book-part>
         </body>
      </book-part>
      <book-part indexed="true" xlink:type="simple">
         <body>
            <book-part>
               <book-part-meta>
                  <book-part-id book-part-id-type="doi">10.2307/j.ctvrxk1hs.6</book-part-id>
                  <title-group>
                     <label>3</label>
                     <title>Probability and Statistical Distributions</title>
                  </title-group>
                  <fpage>65</fpage>
                  <abstract>
                     <p>The main purpose of this chapter is to review notation and basic concepts in probability and statistics. The coverage of various topics cannot be complete, and it is aimed at concepts needed to understand material covered in the book. For an in-depth discussion of probability and statistics, please refer to numerous readily available textbooks, such as Bar89, Lup93, WJ03, Wass10, mentioned in §1.3.</p>
                     <p>The chapter starts with a brief overview of probability and random variables, then it reviews the most common univariate and multivariate distribution functions, and follows with correlation coefficients. We also summarize the central limit theorem and discuss</p>
                  </abstract>
               </book-part-meta>
            </book-part>
         </body>
      </book-part>
      <book-part indexed="true" xlink:type="simple">
         <body>
            <book-part>
               <book-part-meta>
                  <book-part-id book-part-id-type="doi">10.2307/j.ctvrxk1hs.7</book-part-id>
                  <title-group>
                     <label>4</label>
                     <title>Classical Statistical Inference</title>
                  </title-group>
                  <fpage>115</fpage>
                  <abstract>
                     <p>This chapter introduces the main concepts of <italic>statistical inference</italic>, or drawing conclusions from data. There are three main types of inference:</p>
                     <p>Point estimation: What is the best estimate for a model parameter θ, based on the available data?</p>
                     <p>Confidence estimation: How confident should we be in our point estimate?</p>
                     <p>Hypothesis testing: Are data at hand consistent with a given hypothesis or model?</p>
                     <p>There are two major statistical paradigms which address the statistical inference questions: the classical, or <italic>frequentist</italic> paradigm, and the Bayesian paradigm (despite the often-used adjective “classical,” historically the frequentist paradigm was developed after the Bayesian paradigm). While most</p>
                  </abstract>
               </book-part-meta>
            </book-part>
         </body>
      </book-part>
      <book-part indexed="true" xlink:type="simple">
         <body>
            <book-part>
               <book-part-meta>
                  <book-part-id book-part-id-type="doi">10.2307/j.ctvrxk1hs.8</book-part-id>
                  <title-group>
                     <label>5</label>
                     <title>Bayesian Statistical Inference</title>
                  </title-group>
                  <fpage>165</fpage>
                  <abstract>
                     <p>We have already addressed the main philosophical differences between classical and Bayesian statistical inferences in § 4.1. In this chapter, we introduce the most important aspects of Bayesian statistical inference and techniques for performing such calculations in practice. We first review the basic steps in Bayesian inference in §5.1–5.4, and then illustrate them with several examples in §5.6–5.7. Numerical techniques for solving complex problems are discussed in §5.8, and the last section provides a summary of pros and cons for classical and Bayesian methods.</p>
                     <p>Let us briefly note a few historical facts. The Reverend Thomas Bayes (1702–1761)</p>
                  </abstract>
               </book-part-meta>
            </book-part>
         </body>
      </book-part>
      <book-part indexed="true" xlink:type="simple">
         <body>
            <book-part>
               <book-part-meta>
                  <book-part-id book-part-id-type="doi">10.2307/j.ctvrxk1hs.9</book-part-id>
                  <title-group>
                     <title>[Illustrations]</title>
                  </title-group>
               </book-part-meta>
            </book-part>
         </body>
      </book-part>
      <book-part indexed="true" xlink:type="simple">
         <body>
            <book-part>
               <book-part-meta>
                  <book-part-id book-part-id-type="doi">10.2307/j.ctvrxk1hs.10</book-part-id>
                  <title-group>
                     <label>6</label>
                     <title>Searching for Structure in Point Data</title>
                  </title-group>
                  <fpage>243</fpage>
                  <abstract>
                     <p>We begin the third part of this book by addressing methods for exploring and quantifying structure in amultivariate distribution of points. One name for this kind of activity is <italic>exploratory data analysis</italic> (EDA). Given a sample of N points in D -dimensional space, there are three classes of problems that are frequently encountered in practice: density estimation, cluster finding, and statistical description of the observed structure. The space populated by points in the sample can be real physical space, or a space spanned by the measured quantities (attributes). For example, we can consider the distribution of sources in a multidimensional</p>
                  </abstract>
               </book-part-meta>
            </book-part>
         </body>
      </book-part>
      <book-part indexed="true" xlink:type="simple">
         <body>
            <book-part>
               <book-part-meta>
                  <book-part-id book-part-id-type="doi">10.2307/j.ctvrxk1hs.11</book-part-id>
                  <title-group>
                     <label>7</label>
                     <title>Dimensionality and Its Reduction</title>
                  </title-group>
                  <fpage>281</fpage>
                  <abstract>
                     <p>With the dramatic increase in data available from a new generation of astronomical telescopes and instruments, many analyses must address the question of the complexity as well as size of the data set. For example, with the SDSS imaging data we could measure arbitrary numbers of properties or features for any source detected on an image (e.g., we could measure a series of progressively higher moments of the distribution of fluxes in the pixels that make up the source). From the perspective of efficiency we would clearly rather measure only those properties that are directly correlated with the science we</p>
                  </abstract>
               </book-part-meta>
            </book-part>
         </body>
      </book-part>
      <book-part indexed="true" xlink:type="simple">
         <body>
            <book-part>
               <book-part-meta>
                  <book-part-id book-part-id-type="doi">10.2307/j.ctvrxk1hs.12</book-part-id>
                  <title-group>
                     <label>8</label>
                     <title>Regression and Model Fitting</title>
                  </title-group>
                  <fpage>311</fpage>
                  <abstract>
                     <p>Regression is a special case of the general model fitting and selection procedures discussed in chapters 4 and 5. It can be defined as the relation between a dependent variable, y, and a set of independent variables, x, that describes the expectation value of y given x: E [y |x]. The purpose of obtaining a “best-fit” model ranges from scientific interest in the values of model parameters (e.g., the properties of dark energy, or of a newly discovered planet) to the predictive power of the resulting model (e.g., predicting solar activity). The usage of the word <italic>regression</italic> for this relationship</p>
                  </abstract>
               </book-part-meta>
            </book-part>
         </body>
      </book-part>
      <book-part indexed="true" xlink:type="simple">
         <body>
            <book-part>
               <book-part-meta>
                  <book-part-id book-part-id-type="doi">10.2307/j.ctvrxk1hs.13</book-part-id>
                  <title-group>
                     <label>9</label>
                     <title>Classification</title>
                  </title-group>
                  <fpage>353</fpage>
                  <abstract>
                     <p>In chapter 6 we described techniques for estimating joint probability distributions from multivariate data sets and for identifying the inherent clustering within the properties of sources. We can think of this approach as the <italic>unsupervised classification</italic> of data. If, however, we have labels for some of these data points (e.g., an object is <italic>tall</italic>, <italic>short</italic>, <italic>red</italic>, or <italic>blue</italic>) we can utilize this information to develop a relationship between the label and the properties of a source. We refer to this as <italic>supervised classification</italic>.</p>
                     <p>The motivation for supervised classification comes from the long history of classification in astronomy. Possibly the most</p>
                  </abstract>
               </book-part-meta>
            </book-part>
         </body>
      </book-part>
      <book-part indexed="true" xlink:type="simple">
         <body>
            <book-part>
               <book-part-meta>
                  <book-part-id book-part-id-type="doi">10.2307/j.ctvrxk1hs.14</book-part-id>
                  <title-group>
                     <label>10</label>
                     <title>Time Series Analysis</title>
                  </title-group>
                  <fpage>399</fpage>
                  <abstract>
                     <p>This chapter summarizes the fundamental concepts and tools for analyzing time series data. Time series analysis is a branch of applied mathematics developed mostly in the fields of signal processing and statistics. Contributions to this field, from an astronomical perspective, have predominantly focused on unevenly sampled data, low signal-to-noise data, and heteroscedastic errors. There are more books written about time series analysis than pages in this book and, by necessity, we can only address a few common use cases from contemporary astronomy. Even when limited to astronomical data sets, the diversity of potential applications is enormous. The most common applications</p>
                  </abstract>
               </book-part-meta>
            </book-part>
         </body>
      </book-part>
      <book-part indexed="true" xlink:type="simple">
         <body>
            <book-part>
               <book-part-meta>
                  <book-part-id book-part-id-type="doi">10.2307/j.ctvrxk1hs.15</book-part-id>
                  <title-group>
                     <label>A.</label>
                     <title>An Introduction to Scientific Computing with Python</title>
                  </title-group>
                  <fpage>467</fpage>
               </book-part-meta>
            </book-part>
         </body>
      </book-part>
      <book-part indexed="true" xlink:type="simple">
         <body>
            <book-part>
               <book-part-meta>
                  <book-part-id book-part-id-type="doi">10.2307/j.ctvrxk1hs.16</book-part-id>
                  <title-group>
                     <label>B.</label>
                     <title>AstroML:</title>
                     <subtitle>Machine Learning for Astronomy</subtitle>
                  </title-group>
                  <fpage>505</fpage>
               </book-part-meta>
            </book-part>
         </body>
      </book-part>
      <book-part indexed="true" xlink:type="simple">
         <body>
            <book-part>
               <book-part-meta>
                  <book-part-id book-part-id-type="doi">10.2307/j.ctvrxk1hs.17</book-part-id>
                  <title-group>
                     <label>C.</label>
                     <title>Astronomical Flux Measurements and Magnitudes</title>
                  </title-group>
                  <fpage>509</fpage>
               </book-part-meta>
            </book-part>
         </body>
      </book-part>
      <book-part indexed="true" xlink:type="simple">
         <body>
            <book-part>
               <book-part-meta>
                  <book-part-id book-part-id-type="doi">10.2307/j.ctvrxk1hs.18</book-part-id>
                  <title-group>
                     <label>D.</label>
                     <title>SQL Query for Downloading SDSS Data</title>
                  </title-group>
                  <fpage>513</fpage>
               </book-part-meta>
            </book-part>
         </body>
      </book-part>
      <book-part indexed="true" xlink:type="simple">
         <body>
            <book-part>
               <book-part-meta>
                  <book-part-id book-part-id-type="doi">10.2307/j.ctvrxk1hs.19</book-part-id>
                  <title-group>
                     <label>E.</label>
                     <title>Approximating the Fourier Transform with the FFT</title>
                  </title-group>
                  <fpage>515</fpage>
               </book-part-meta>
            </book-part>
         </body>
      </book-part>
      <book-part indexed="true" xlink:type="simple">
         <body>
            <book-part>
               <book-part-meta>
                  <book-part-id book-part-id-type="doi">10.2307/j.ctvrxk1hs.20</book-part-id>
                  <title-group>
                     <title>Visual Figure Index</title>
                  </title-group>
                  <fpage>521</fpage>
               </book-part-meta>
            </book-part>
         </body>
      </book-part>
      <book-part indexed="true" xlink:type="simple">
         <body>
            <book-part>
               <book-part-meta>
                  <book-part-id book-part-id-type="doi">10.2307/j.ctvrxk1hs.21</book-part-id>
                  <title-group>
                     <title>Index</title>
                  </title-group>
                  <fpage>529</fpage>
               </book-part-meta>
            </book-part>
         </body>
      </book-part>
   </body>
</book>
