<?xml version="1.0" encoding="UTF-8"?>


<article dtd-version="1.0" article-type="research-article">
  <front>
      <journal-meta>
         <journal-id xmlns:xlink="http://www.w3.org/1999/xlink" journal-id-type="jstor">jcompgrapstat</journal-id>
         <journal-id xmlns:xlink="http://www.w3.org/1999/xlink" journal-id-type="jstor">j100879</journal-id>
         <journal-title-group xmlns:xlink="http://www.w3.org/1999/xlink">
            <journal-title>Journal of Computational and Graphical Statistics</journal-title>
         </journal-title-group>
      
         <publisher>
            <publisher-name>American Statistical Association, Institute of Mathematical Statistics, and Interface Foundation of North America</publisher-name>
         </publisher>
         <issn xmlns:xlink="http://www.w3.org/1999/xlink" pub-type="ppub">10618600</issn>
         <custom-meta-group xmlns:xlink="http://www.w3.org/1999/xlink"/>
      </journal-meta>
      <article-meta xmlns:xlink="http://www.w3.org/1999/xlink">
         <volume xmlns:mml="http://www.w3.org/1998/Math/MathML"
                 xmlns:oasis="http://docs.oasis-open.org/ns/oasis-exchange/table">17</volume>
         <issue xmlns:mml="http://www.w3.org/1998/Math/MathML"
                xmlns:oasis="http://docs.oasis-open.org/ns/oasis-exchange/table">3</issue>
         <issue-id>i27594322</issue-id>
         <article-id pub-id-type="jstor">27594328</article-id>
         <article-id pub-id-type="pub-doi">10.1198/106186008X344522</article-id>
         <article-categories>
            <subj-group>
               <subject>Machine Learning and Classification</subject>
            </subj-group>
         </article-categories>
         <title-group>
            <article-title>A Bias Correction Algorithm for the Gini Variable Importance Measure in Classification Trees</article-title>
         </title-group>
         <contrib-group>
            <contrib>
               <string-name>
                  <given-names>Marco</given-names>
                  <surname>Sandri</surname>
               </string-name>
            </contrib>
            <contrib>
               <string-name>
                  <given-names>Paola</given-names>
                  <surname>Zuccolotto</surname>
               </string-name>
            </contrib>
         </contrib-group>
         <pub-date pub-type="ppub">
            <day>1</day>
            <month>9</month>
            <year>2008</year>
         </pub-date>
         <fpage>611</fpage>
         <lpage>628</lpage>
      
      
      
      
      
      
         <permissions>
            <copyright-statement>Copyright 2008 American Statistical Association, the Institute of Mathematical Statistics, and the Interface Foundation of North America</copyright-statement>
         </permissions>
      
         <self-uri xlink:href="https://www.jstor.org/stable/27594328"/>
      
      
         <abstract>
            <p>This article considers a measure of variable importance frequently used in variable-selection methods based on decision trees and tree-based ensemble models. These models include CART, random forests, and gradient boosting machine. The measure of variable importance is defined as the total heterogeneity reduction produced by a given covariate on the response variable when the sample space is recursively partitioned. Despite its popularity, some authors have shown that this measure is biased to the extent that, under certain conditions, there may be dangerous effects on variable selection. Here we present a simple and effective method for bias correction, focusing on the easily generalizable case of the Gini index as a measure of heterogeneity.</p>
         </abstract>
         <kwd-group>
            <kwd>Bias</kwd>
            <kwd>Learning ensemble</kwd>
            <kwd>Variable importance</kwd>
            <kwd>Variable selection</kwd>
         </kwd-group>
         <custom-meta-group>
            <custom-meta>
               <meta-name>lang</meta-name>
               <meta-value>eng</meta-value>
            </custom-meta>
         </custom-meta-group>
      </article-meta>
  </front>
  <back>
    
      <ref-list>
        <title>References</title>
        <ref id="d774026e210a1310">
          
            <mixed-citation id="d774026e214" publication-type="other">
Bell, D. and Wang, H. (2000): "A Formalism for Relevance and its Application in Feature Subset Selection,"
Machine Learning, 4, 2, 175–195.</mixed-citation>
        </ref>
        <ref id="d774026e224a1310">
          
            <mixed-citation id="d774026e228" publication-type="other">
Breiman, L. (2001a), "Random Forests," Machine Learning, 45, 5–32.</mixed-citation>
        </ref>
        <ref id="d774026e235a1310">
          
            <mixed-citation id="d774026e239" publication-type="other">
—(2001b), "Statistical Modeling: The Two Cultures," Statistical Science, 16, 3, 199–231.</mixed-citation>
        </ref>
        <ref id="d774026e246a1310">
          
            <mixed-citation id="d774026e250" publication-type="other">
—(2002), "Manual on Setting Up, Using, and Understanding Random Forests v3.1." Technical Report,
ftp://ftp. stat.berkeley.edu/pub/users/breiman/Using_random_forestS_v3.].pdf.</mixed-citation>
        </ref>
        <ref id="d774026e261a1310">
          
            <mixed-citation id="d774026e265" publication-type="other">
Breiman, L., Friedman, J.H., Olshen, R.A., and Stone, C.J. (1984), Classification and Regression Trees, London:
Chapman &amp;amp; Hall.</mixed-citation>
        </ref>
        <ref id="d774026e275a1310">
          
            <mixed-citation id="d774026e279" publication-type="other">
Breiman, L., Cutler, A., Liaw, A., and Wiener, M. (2006), Breiman and Cutler's Random Forests for Classification
and Regression. R package version 4.5-18 http://cran.r-project.org/doc/packages/randomForest.pdf.</mixed-citation>
        </ref>
        <ref id="d774026e289a1310">
          
            <mixed-citation id="d774026e293" publication-type="other">
Bureau, A., Dupuis, J., Hayward, B., Falls, K., and Van Eerdewegh, P. (2003), "Mapping Complex Traits using
Random Forests," BMC Genetics, 4(Suppl.l): S64, http://www.biomedcentral.com/1471-2156/4/sl/S64.</mixed-citation>
        </ref>
        <ref id="d774026e303a1310">
          
            <mixed-citation id="d774026e307" publication-type="other">
De'ath, G. (2007), "Boosted Trees for Ecological Modeling and Prediction," Ecology, 88, 1, 243–251.</mixed-citation>
        </ref>
        <ref id="d774026e314a1310">
          
            <mixed-citation id="d774026e318" publication-type="other">
Diaz-Uriarte, R., and Alvarez de Andrés, S. (2006), "Gene Selection and Classification of Microarray Data using
Random Forest," BMC Genetics, 7:3, http://www.biomedcentral.com/1471 -2105/7/3.</mixed-citation>
        </ref>
        <ref id="d774026e328a1310">
          
            <mixed-citation id="d774026e332" publication-type="other">
Dobra, A., and Gehrke, J. (2001), "Bias Correction in Classification Tree Construction," in Proceedings of the
Seventeenth International Conference on Machine Learning, eds. C.E. Brodley and A. P. Danyluk, Williams
College, Williamstown, MA, 90–97.</mixed-citation>
        </ref>
        <ref id="d774026e346a1310">
          
            <mixed-citation id="d774026e350" publication-type="other">
Friedman, J.H. (2001), "Greedy Function Approximation: A Gradient Boosting Machine," The Annals of Statis-
tics, 29, 1189–1232.</mixed-citation>
        </ref>
        <ref id="d774026e360a1310">
          
            <mixed-citation id="d774026e364" publication-type="other">
—(2002), "Tutorial: Getting Started with MART in R," Technical Report, Standford University. Available
online at http://www-stat.Stanford. edu/~jhf/r- mart/tutorial/tutorial.pdf.</mixed-citation>
        </ref>
        <ref id="d774026e374a1310">
          
            <mixed-citation id="d774026e378" publication-type="other">
Friedman, J.H., and Meulman, J.J. (2003), "Multiple Additive Regression Trees with Application in Epidemiol-
ogy," Statistics in Medicine, 22, 1365–1381.</mixed-citation>
        </ref>
        <ref id="d774026e388a1310">
          
            <mixed-citation id="d774026e392" publication-type="other">
Gong, G. (1986), "Cross-Validation, the Jackknife, and the Bootstrap: Excess Error Estimation in Forward Lo-
gistic Regression," Journal of the American Statistical Association, 81, 108–113.</mixed-citation>
        </ref>
        <ref id="d774026e402a1310">
          
            <mixed-citation id="d774026e406" publication-type="other">
Guglielmi, A., Ruzzenente, A., Sandri, M., Kind, R., Lombardo, F., Rodella, L., Catalano, F., De Manzoni, G.
and Cordiano, C. (2002), "Risk Assessment and Prediction of Rebleeding in Bleeding Gastroduodenal Ulcer,"
Endoscopy, 34, 771–779.</mixed-citation>
        </ref>
        <ref id="d774026e419a1310">
          
            <mixed-citation id="d774026e423" publication-type="other">
Guha, R., and Jurs, P.C. (2004), "Development of Linear, Ensemble, and Nonlinear Models for the Prediction
and Interpretation of the Biological Activity of a Set of PDGFR Inhibitors," Journal of Chemical Inference
and Computer Science, 44, 2179–2189.</mixed-citation>
        </ref>
        <ref id="d774026e437a1310">
          
            <mixed-citation id="d774026e441" publication-type="other">
Hothorn, T., Hornik, K., and Zeileis, A. (2006a), "Unbiased Recursive Partitioning: A Conditional Inference
Framework," Journal of Computational and Graphical Statistics, 15, 651–674.</mixed-citation>
        </ref>
        <ref id="d774026e451a1310">
          
            <mixed-citation id="d774026e455" publication-type="other">
Hothorn, T., Hornik, K., and Zeileis, A. (2006b), "party: A Laboratory for Recursive Part(y)itioning," R pack-
age version 0.9-11. Available online at http://cran.r-project.org/doc/vignettes/party/party.pdf.</mixed-citation>
        </ref>
        <ref id="d774026e465a1310">
          
            <mixed-citation id="d774026e469" publication-type="other">
John, G.H., Kohavi, R., and Pfleger, K. (1994), "Irrelevant Features and the Subset Selection Problem," in
Proceedings of the 11th International Conference on Machine Learning, eds. W. W. Cohen and H. Hirsch,
New Brunswick, NJ: Morgan Kaufmann, 121–129.</mixed-citation>
        </ref>
        <ref id="d774026e482a1310">
          
            <mixed-citation id="d774026e486" publication-type="other">
Kim, H., and Loh, W. (2001), "Classification Trees with Unbiased Multiway Splits," Journal of the American
Statistical Association, 96, 589–604.</mixed-citation>
        </ref>
        <ref id="d774026e496a1310">
          
            <mixed-citation id="d774026e500" publication-type="other">
Kononenko, I. (1995), "On Biases in Estimating Multi-Valued Attributes," in Proceedings of the Fourteenth
International Joint Conference on Artificial Intelligence, ed. C. Mellish, Montréal, Canada, 1034–1040.</mixed-citation>
        </ref>
        <ref id="d774026e510a1310">
          
            <mixed-citation id="d774026e514" publication-type="other">
Loh, W.-Y., Shih, Y.-S. (1997), "Split Selection Methods for Classification Trees," Statististica Sinica, 7, 815–
840.</mixed-citation>
        </ref>
        <ref id="d774026e525a1310">
          
            <mixed-citation id="d774026e529" publication-type="other">
Lunetta, K.L., Hayward, B.L., Segal, J., and Van Eerdewegh, P. (2004), "Screening Large-Scale Association
Study Data: Exploiting Interactions using Random Forests," BMC Genetics, 5, 32. Available online at http:
//www.biomedcentral.com/1471-2156/5/32.</mixed-citation>
        </ref>
        <ref id="d774026e542a1310">
          
            <mixed-citation id="d774026e546" publication-type="other">
Menze, B.H., Petrich, W., and Hamprecht F.A. (2007), "Multivariate Feature Selection and Hierarchical Classifi-
cation for Infrared Spectroscopy: Serum-Based Detection of Bovine Spongiform Encephalopathy," Analytical
and Bioanalytical Chemistry, doi:10.1007/s00216-006-1070-5.</mixed-citation>
        </ref>
        <ref id="d774026e559a1310">
          
            <mixed-citation id="d774026e563" publication-type="other">
Pearl, J. (1988), Probabilistic Reasoning in Intelligent Systems: Networks of Plausible Inference, San Francisco,
CA: Morgan Kaufmann Publishers, Inc.</mixed-citation>
        </ref>
        <ref id="d774026e573a1310">
          
            <mixed-citation id="d774026e577" publication-type="other">
Ridgeway, G. (2007), "Generalized Boosted Models: A Guide to the gbm Package," Available online at http:
//i-pensieri. com/gregr/papers/gbm-vignette.pdf.</mixed-citation>
        </ref>
        <ref id="d774026e587a1310">
          
            <mixed-citation id="d774026e591" publication-type="other">
Schonlau, M. (2005), "Boosted Regression (Boosting): A Tutorial and a Stata Plugin," The Stata Journal, 5, 3,
330–354.</mixed-citation>
        </ref>
        <ref id="d774026e601a1310">
          
            <mixed-citation id="d774026e605" publication-type="other">
Strobl, C. (2005), "Statistical Sources of Variable Selection Bias in Classification Trees Based on the Gini Index,"
Technical Report, SFB 386, http://epub.ub.uni-muenchen.de/archive/00001789/01/paper_420.pdf.</mixed-citation>
        </ref>
        <ref id="d774026e616a1310">
          
            <mixed-citation id="d774026e620" publication-type="other">
Strobl, C, Boulesteix, A.-L., Zeileis, A., and Hothorn, T. (2007a), "Bias in Random Forest Variable Importance
Measures: Illustrations, Sources and a Solution," BMCBioinformatics, 8, 25, doi: 10.1186/1471-2105-8-25.</mixed-citation>
        </ref>
        <ref id="d774026e630a1310">
          
            <mixed-citation id="d774026e634" publication-type="other">
Strobl, C, Boulesteix, A.-L., and Augustin, T. (2007b), "Unbiased Split Selection for Classification Trees Based
on the Gini Index," Computational Statistics &amp;amp; Data Analysis, doi: 10.1016/j.csda.2006.12.030.</mixed-citation>
        </ref>
        <ref id="d774026e644a1310">
          
            <mixed-citation id="d774026e648" publication-type="other">
Svetnik, V, Wang, T., Tong, C, Liaw, A., Sheridan, R.P., and Song Q. (2005), "Boosting: An Ensemble Learning
Tool for Compound Classification and QSAR Modeling," Journal of Chemical Information and Modeling, 45,
786–799</mixed-citation>
        </ref>
        <ref id="d774026e661a1310">
          
            <mixed-citation id="d774026e665" publication-type="other">
White, A.P., and Liu, W.Z. (1994), "Bias in Information-Based Measures in Decision Tree Induction," Machine
Learning, 15,321–329.</mixed-citation>
        </ref>
        <ref id="d774026e675a1310">
          
            <mixed-citation id="d774026e679" publication-type="other">
Wu, Y, Boos, D.D., and Stefanski, L.A. (2007), "Controlling Variable Selection by the Addition of Pseudovari-
ables," Journal of the American Statistical Association, 102, 235–243.</mixed-citation>
        </ref>
        <ref id="d774026e689a1310">
          
            <mixed-citation id="d774026e693" publication-type="other">
Yu, L., and Liu, H. (2004), "Efficient Feature Selection via Analysis of Relevance and Redundancy," Journal of
Machine Learning Research, 5, 1205–1224.</mixed-citation>
        </ref>
      </ref-list>
    
  </back>
</article>


