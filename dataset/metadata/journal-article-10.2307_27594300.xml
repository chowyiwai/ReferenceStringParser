<?xml version="1.0" encoding="UTF-8"?>


<article dtd-version="1.0" article-type="research-article">
  <front>
      <journal-meta>
         <journal-id xmlns:xlink="http://www.w3.org/1999/xlink" journal-id-type="jstor">jcompgrapstat</journal-id>
         <journal-id xmlns:xlink="http://www.w3.org/1999/xlink" journal-id-type="jstor">j100879</journal-id>
         <journal-title-group xmlns:xlink="http://www.w3.org/1999/xlink">
            <journal-title>Journal of Computational and Graphical Statistics</journal-title>
         </journal-title-group>
      
         <publisher>
            <publisher-name>American Statistical Association, Institute of Mathematical Statistics, and Interface Foundation of North America</publisher-name>
         </publisher>
         <issn xmlns:xlink="http://www.w3.org/1999/xlink" pub-type="ppub">10618600</issn>
         <custom-meta-group xmlns:xlink="http://www.w3.org/1999/xlink"/>
      </journal-meta>
      <article-meta xmlns:xlink="http://www.w3.org/1999/xlink">
         <volume xmlns:mml="http://www.w3.org/1998/Math/MathML"
                 xmlns:oasis="http://docs.oasis-open.org/ns/oasis-exchange/table">17</volume>
         <issue xmlns:mml="http://www.w3.org/1998/Math/MathML"
                xmlns:oasis="http://docs.oasis-open.org/ns/oasis-exchange/table">1</issue>
         <issue-id>i27594287</issue-id>
         <article-id pub-id-type="jstor">27594300</article-id>
         <article-id pub-id-type="pub-doi">10.1198/106186008X285573</article-id>
         <title-group>
            <article-title>Sliced Coordinate Analysis for Effective Dimension Reduction and Nonlinear Extensions</article-title>
         </title-group>
         <contrib-group>
            <contrib>
               <string-name>
                  <given-names>Zhihua</given-names>
                  <surname>Zhang</surname>
               </string-name>
            </contrib>
            <contrib>
               <string-name>
                  <given-names>Dit-Yan</given-names>
                  <surname>Yeung</surname>
               </string-name>
            </contrib>
            <contrib>
               <string-name>
                  <given-names>James T.</given-names>
                  <surname>Kwok</surname>
               </string-name>
            </contrib>
            <contrib>
               <string-name>
                  <given-names>Edward Y.</given-names>
                  <surname>Chang</surname>
               </string-name>
            </contrib>
         </contrib-group>
         <pub-date pub-type="ppub">
            <day>1</day>
            <month>3</month>
            <year>2008</year>
         </pub-date>
         <fpage>225</fpage>
         <lpage>242</lpage>
         <permissions>
            <copyright-statement>Copyright 2008 American Statistical Association, the Institute of Mathematical Statistics, and the Interface Foundation of North America</copyright-statement>
         </permissions>
      
         <self-uri xlink:href="https://www.jstor.org/stable/27594300"/>
      
      
         <abstract>
            <p>Sliced inverse regression (SIR) is an important method for reducing the dimensionality of input variables. Its goal is to estimate the effective dimension reduction directions. In classification settings, SIR is closely related to Fisher discriminant analysis. Motivated by reproducing kernel theory, we propose a notion of nonlinear effective dimension reduction and develop a nonlinear extension of SIR called kernel SIR (KSIR). Both SIR and KSIR are based on principal component analysis. Alternatively, based on principal coordinate analysis, we propose the dual versions of SIR and KSIR, which we refer to as sliced coordinate analysis (SCA) and kernel sliced coordinate analysis (KSCA), respectively. In the classification setting, we also call them discriminant coordinate analysis and kernel discriminant coordinate analysis. The computational complexities of SIR and KSIR rely on the dimensionality of the input vector and the number of input vectors, respectively, while those of SCA and KSCA both rely on the number of slices in the output. Thus, SCA and KSCA are very efficient dimension reduction methods.</p>
         </abstract>
         <kwd-group>
            <kwd>Nonlinear effective dimension reduction</kwd>
            <kwd>Sliced inverse regression</kwd>
            <kwd>Reproducing kernels</kwd>
         </kwd-group>
         <custom-meta-group>
            <custom-meta>
               <meta-name>lang</meta-name>
               <meta-value>eng</meta-value>
            </custom-meta>
         </custom-meta-group>
      </article-meta>
  </front>
  <back>
    
      <ref-list>
        <title>References</title>
        <ref id="d1449666e167a1310">
          
            <mixed-citation id="d1449666e171" publication-type="other">
Baudat, G., and Anouar, F. (2000), "Generalized Discriminant Analysis using a Kernel Approach," Neural
Computation, 12, 2385–2404.</mixed-citation>
        </ref>
        <ref id="d1449666e181a1310">
          
            <mixed-citation id="d1449666e185" publication-type="other">
Belhumeur, P., Hespanha, J., and Kriegman, D. (1997), "Eigenfaces vs. Fisherfaces: Recognition Using Class
Specific Linear Projection," IEEE Transactions PAMI, 19, 711–720.</mixed-citation>
        </ref>
        <ref id="d1449666e195a1310">
          
            <mixed-citation id="d1449666e199" publication-type="other">
Cook, R. D. (1998), "Principal Hessian Directions Revisited," Journal of the American Statistical Association,
93, 84–94.</mixed-citation>
        </ref>
        <ref id="d1449666e209a1310">
          
            <mixed-citation id="d1449666e213" publication-type="other">
Cook, R. D., and Lee, H. (1999), "Dimension Reduction in Regression with a Binary Response," Journal of the
American Statistical Association, 94, 1187–1200.</mixed-citation>
        </ref>
        <ref id="d1449666e224a1310">
          
            <mixed-citation id="d1449666e228" publication-type="other">
Cook, R. D., and Weisberg (1991), Discussion of "Sliced Inverse Regression for Dimension Reduction," by Li,
Journal of the American Statistical Association, 86, 328–332.</mixed-citation>
        </ref>
        <ref id="d1449666e238a1310">
          
            <mixed-citation id="d1449666e242" publication-type="other">
Cook, R. D., and Yin, X. (2001), "Dimension Reduction and Visualization in Discriminant Analysis," Australian
&amp;amp; New Zealand Journal of Statistics, 43, 147–199.</mixed-citation>
        </ref>
        <ref id="d1449666e252a1310">
          
            <mixed-citation id="d1449666e256" publication-type="other">
Duda, R. O., Hart, P. E., and Stork, D. G. (2001), Pattern Classification (2nd ed.), New York: Wiley.</mixed-citation>
        </ref>
        <ref id="d1449666e263a1310">
          
            <mixed-citation id="d1449666e267" publication-type="other">
Golub, G. H., and Loan, C. F. V. (1996), Matrix Computations (3rd ed.), Baltimore: The Johns Hopkins University
Press.</mixed-citation>
        </ref>
        <ref id="d1449666e277a1310">
          
            <mixed-citation id="d1449666e281" publication-type="other">
Gower, J. C. (1966), "Some Distance Properties of Latent Root and Vector Methods used in Multivariate Data
Analysis," Biometrika, 53, 315–328.</mixed-citation>
        </ref>
        <ref id="d1449666e291a1310">
          
            <mixed-citation id="d1449666e295" publication-type="other">
—(1968), "Adding a Point to Vector Diagrams in Multivariate Analysis," Biometrika, 55, 582–585.</mixed-citation>
        </ref>
        <ref id="d1449666e303a1310">
          
            <mixed-citation id="d1449666e307" publication-type="other">
Gower, J. C, and Krzanowski, W. J. (1999), "Analysis of Distance for Structured Multivariate Data and Exten-
sions to Multivariate Analysis of Variance," Journal of the Royal Statistial Society, Series C, 48, 505–519.</mixed-citation>
        </ref>
        <ref id="d1449666e317a1310">
          
            <mixed-citation id="d1449666e321" publication-type="other">
Hastie, T., Tibshirani, R., and Friedman, J. (2001), The Elements of Statistical Learning: Data Mining, Inference,
and Prediction, New York: Springer-Verlag.</mixed-citation>
        </ref>
        <ref id="d1449666e331a1310">
          
            <mixed-citation id="d1449666e335" publication-type="other">
Howland, P., Jeon, M., and Park, H. (2003), "Structure Preserving Dimension Reduction for Clustered Text Data
Based on the Generalized Singular Value Decomposition," SIAM Journal on Matrix Analysis and Applications,
25, 165–179.</mixed-citation>
        </ref>
        <ref id="d1449666e348a1310">
          
            <mixed-citation id="d1449666e352" publication-type="other">
Jolliffe, I. (2002), Principal Component Analysis (2nd ed.), New York: Springer.</mixed-citation>
        </ref>
        <ref id="d1449666e359a1310">
          
            <mixed-citation id="d1449666e363" publication-type="other">
Li, K. C. (1991), "Sliced Inverse Regression for Dimension Reduction" (with discussion), Journal of the Ameri-
can Statistical Association, 86, 316–342.</mixed-citation>
        </ref>
        <ref id="d1449666e373a1310">
          
            <mixed-citation id="d1449666e377" publication-type="other">
—(1992), "On Principal Hessian Direction for Data Visualization and Dimension Reduction: Another
Application of Stein's Lemma," Journal of the American Statistical Association, 87, 1025–1039.</mixed-citation>
        </ref>
        <ref id="d1449666e388a1310">
          
            <mixed-citation id="d1449666e392" publication-type="other">
Mardia, K. V, Kent, J. T., and Bibby, J. M. (1979), Multivariate Analysis, New York: Academic Press.</mixed-citation>
        </ref>
        <ref id="d1449666e399a1310">
          
            <mixed-citation id="d1449666e403" publication-type="other">
Mika, S., Ratsch, G., Weston, J., Schölkopf, B., Smola, A., and Müller, K. R. (2000), "Invariant Feature Extraction
and Classification in Kernel Space," in Advances in Neural Information Processing Systems 12, volume 12,
pp. 526–532.</mixed-citation>
        </ref>
        <ref id="d1449666e416a1310">
          
            <mixed-citation id="d1449666e420" publication-type="other">
Paige, C. C, and Saunders, M. A. (1981), "Towards a Generalized Singular Value Decomposition," SIAM Journal
on Numerical Analysis, 18, 398–405.</mixed-citation>
        </ref>
        <ref id="d1449666e430a1310">
          
            <mixed-citation id="d1449666e434" publication-type="other">
Park, C. H., and Park, H. (2005), "Nonlinear Discriminant Analysis using Kernel Functions and the Generalized
Singular Value Decomposition," SIAM Journal on Matrix Analysis and Applications, 27, 87–102.</mixed-citation>
        </ref>
        <ref id="d1449666e444a1310">
          
            <mixed-citation id="d1449666e448" publication-type="other">
Roth, V, and Steinhage, V (2000), "Nonlinear Discriminant Analysis using Kernel Functions," in Advances in
Neural Information Processing Systems 12, vol. 12, pp. 568–574.</mixed-citation>
        </ref>
        <ref id="d1449666e458a1310">
          
            <mixed-citation id="d1449666e462" publication-type="other">
Schölkopf, B., and Smola, A. (2002), Learning with Kernels, Cambridge, MA: The MIT Press.</mixed-citation>
        </ref>
        <ref id="d1449666e470a1310">
          
            <mixed-citation id="d1449666e474" publication-type="other">
Schölkopf, B., Smola, A., and Müller, K.-R. (1998), "Nonlinear Component Analysis as a Kernel Eigenvalue
Problem," Neural Computation, 10, 1299–1319.</mixed-citation>
        </ref>
        <ref id="d1449666e484a1310">
          
            <mixed-citation id="d1449666e488" publication-type="other">
Schott, J. R. (1994), "Determining the Dimensionality in Sliced Inverse Regression," Journal of the American
Statistical Association, 89, 141–148.</mixed-citation>
        </ref>
        <ref id="d1449666e498a1310">
          
            <mixed-citation id="d1449666e502" publication-type="other">
Shawe-Taylor, J., and Cristianini, N. (2004), Kernel Methods for Pattern Analysis, New York: Cambridge Uni-
versity Press.</mixed-citation>
        </ref>
        <ref id="d1449666e512a1310">
          
            <mixed-citation id="d1449666e516" publication-type="other">
Tong, S., and Chang, E. (2001), "Support Vector Machine Active Learning for Image Retrieval," in Proceedings
of ACMInternational Conference on Multimedia, pp. 107–118.</mixed-citation>
        </ref>
        <ref id="d1449666e526a1310">
          
            <mixed-citation id="d1449666e530" publication-type="other">
Turk, M., and Pentland, A. (1991), "Eigenfaces for Recognition," Journal of Cognitive Neuroscience, 3, 71–86.</mixed-citation>
        </ref>
        <ref id="d1449666e537a1310">
          
            <mixed-citation id="d1449666e541" publication-type="other">
Vempala, S., and Wang, G. (2002), "A Spectral Algorithm for Learning Mixtures of Distributions," in Proceedings
of the 43rd IEEE Foundations of Computer Science.</mixed-citation>
        </ref>
      </ref-list>
    
  </back>
</article>


