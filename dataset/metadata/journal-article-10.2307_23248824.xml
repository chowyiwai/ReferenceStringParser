<?xml version="1.0" encoding="UTF-8"?>


<article dtd-version="1.0" article-type="research-article">
  <front>
      <journal-meta>
         <journal-id xmlns:xlink="http://www.w3.org/1999/xlink" journal-id-type="jstor">jcompgrapstat</journal-id>
         <journal-id xmlns:xlink="http://www.w3.org/1999/xlink" journal-id-type="jstor">j100879</journal-id>
         <journal-title-group xmlns:xlink="http://www.w3.org/1999/xlink">
            <journal-title>Journal of Computational and Graphical Statistics</journal-title>
         </journal-title-group>
      
         <publisher>
            <publisher-name>American Statistical Association, Institute of Mathematical Statistics, and Interface Foundation of North America</publisher-name>
         </publisher>
         <issn xmlns:xlink="http://www.w3.org/1999/xlink" pub-type="ppub">10618600</issn>
         <custom-meta-group xmlns:xlink="http://www.w3.org/1999/xlink"/>
      </journal-meta>
      <article-meta xmlns:xlink="http://www.w3.org/1999/xlink">
         <volume xmlns:mml="http://www.w3.org/1998/Math/MathML"
                 xmlns:oasis="http://docs.oasis-open.org/ns/oasis-exchange/table">21</volume>
         <issue xmlns:mml="http://www.w3.org/1998/Math/MathML"
                xmlns:oasis="http://docs.oasis-open.org/ns/oasis-exchange/table">1</issue>
         <issue-id>i23241031</issue-id>
         <article-id pub-id-type="jstor">23248824</article-id>
         <title-group>
            <article-title>LASSO Isotone for High-Dimensional Additive Isotonic Regression</article-title>
         </title-group>
         <contrib-group>
            <contrib>
               <string-name>
                  <given-names>Zhou</given-names>
                  <surname>Fang</surname>
               </string-name>
            </contrib>
            <contrib>
               <string-name>
                  <given-names>Nicolai</given-names>
                  <surname>Meinshausen</surname>
               </string-name>
            </contrib>
         </contrib-group>
         <pub-date pub-type="ppub">
            <day>1</day>
            <month>3</month>
            <year>2012</year>
         </pub-date>
         <fpage>72</fpage>
         <lpage>91</lpage>
      
      
      
      
      
      
      
      
         <permissions>
            <copyright-statement>© 2012 American Statistical Association, the Institute of Mathematical Statistics, and the Interface Foundation of North America</copyright-statement>
         </permissions>
      
         <self-uri xlink:href="https://www.jstor.org/stable/23248824"/>
      
      
         <abstract>
            <p>Additive isotonic regression attempts to determine the relationship between a multi-dimensional observation variable and a response, under the constraint that the estimate is the additive sum of univariate component effects that are monotonically increasing. In this article, we present a new method for such regression called LASSO Isotone (LISO). LISO adapts ideas from sparse linear modeling to additive isotonic regression. Thus, it is viable in many situations with high-dimensional predictor variables, where selection of significant versus insignificant variables is required. We suggest an algorithm involving a modification of the backfitting algorithm CPAV. We give a numerical convergence result, and finally examine some of its properties through simulations. We also suggest some possible extensions that improve performance, and allow calculation to be carried out when the direction of the monotonicity is unknown. Supplemental materials are available online for this article.</p>
         </abstract>
         <custom-meta-group>
            <custom-meta>
               <meta-name>lang</meta-name>
               <meta-value>eng</meta-value>
            </custom-meta>
         </custom-meta-group>
      </article-meta>
  </front>
  <back>
    
      <ref-list>
        <title>REFERENCES</title>
        <ref id="d1620553e207a1310">
          
            <mixed-citation id="d1620553e211" publication-type="other">
Avalos, M., Grandvalet, Y., and Ambroise, C. (2003), "Regularization Methods for Additive Models," in Pro-
ceedings of the 5th International Symposium on Intelligent Data Analysis, Berlin: Springer, pp. 509-520.
[74]</mixed-citation>
        </ref>
        <ref id="d1620553e224a1310">
          
            <mixed-citation id="d1620553e228" publication-type="other">
Ayer, M„ Brunk, H. D., Ewing, G. M., Reid, W. T., and Silverman, E. (1955), "An Empirical Distribution Function
for Sampling With Incomplete Information," The Annals of Mathematical Statistics, 26, 641-647. [73,76]</mixed-citation>
        </ref>
        <ref id="d1620553e238a1310">
          
            <mixed-citation id="d1620553e242" publication-type="other">
Bacchetti, P. (1989), "Additive Isotonic Models," Journal of the American Statistical Association, 84, 289-294.
[73,74]</mixed-citation>
        </ref>
        <ref id="d1620553e252a1310">
          
            <mixed-citation id="d1620553e256" publication-type="other">
Barlow, R. E., Bartholomew, D. J., Bremner, J. M., and Brunk, H. D. (1972), Statistical Inference Under Order
Restrictions: The Theory and Application of Isotonic Regression, Hoboken, NJ: Wiley. [73,76]</mixed-citation>
        </ref>
        <ref id="d1620553e267a1310">
          
            <mixed-citation id="d1620553e271" publication-type="other">
Breiman, L. (1996), "Bagging Predictors," Machine Learning, 24, 123-140. [81]</mixed-citation>
        </ref>
        <ref id="d1620553e278a1310">
          
            <mixed-citation id="d1620553e282" publication-type="other">
-—— (2001), "Random Forests," Machine Learning, 45, 5-32. [87]</mixed-citation>
        </ref>
        <ref id="d1620553e289a1310">
          
            <mixed-citation id="d1620553e293" publication-type="other">
Donoho, D. (2006), "For Most Large Underdetermined Systems of Linear Equations the Minimal 11 -Norm Solu-
tion Is Also the Sparsest Solution," Communications on Pure and Applied Mathematics, 59, 797-829. [74]</mixed-citation>
        </ref>
        <ref id="d1620553e303a1310">
          
            <mixed-citation id="d1620553e307" publication-type="other">
Efron, B., Hastie, T., Johnstone, I., and Tibshirani, R. (2004), "Least Angle Regression," The Annals of Statistics,
32, 407^99. [76]</mixed-citation>
        </ref>
        <ref id="d1620553e317a1310">
          
            <mixed-citation id="d1620553e321" publication-type="other">
Friedman, J. (1991), "Multiple Adaptive Regression Splines," The Annals of Statistics, 19, 1-141. [87]</mixed-citation>
        </ref>
        <ref id="d1620553e328a1310">
          
            <mixed-citation id="d1620553e332" publication-type="other">
Friedman, J., Hastie, T., Hofling, H„ and Tibshirani, R. (2007), "Pathwise Coordinate Optimization," The Annals
of Applied Statistics, 1, 302-332. [74,81]</mixed-citation>
        </ref>
        <ref id="d1620553e343a1310">
          
            <mixed-citation id="d1620553e347" publication-type="other">
Harrison, D., and Rubinfeld, D. L. (1978), "Hedonic Housing Prices and the Demand for Clean Air," Journal of
Environmental Economics and Management, 5, 81-102. [83]</mixed-citation>
        </ref>
        <ref id="d1620553e357a1310">
          
            <mixed-citation id="d1620553e361" publication-type="other">
Hastie, T. J., and Tibshirani, R. J. (1990), "Additive Models," in Generalized Additive Models. Monographs on
Statistics and Applied Probability, London: Chapman &amp; Hall, chapter 4, pp. 82-95. [72]</mixed-citation>
        </ref>
        <ref id="d1620553e371a1310">
          
            <mixed-citation id="d1620553e375" publication-type="other">
Hastie, T., Tibshirani, R., and Friedman, J. (2003), The Elements of Statistical Learning, Berlin: Springer. [83]</mixed-citation>
        </ref>
        <ref id="d1620553e382a1310">
          
            <mixed-citation id="d1620553e386" publication-type="other">
Huang. J., Horowitz, J. L., and Wei, F. (2009), "Variable Selection in Nonparametric Additive Models," technical
report, The University of Iowa. [81 ]</mixed-citation>
        </ref>
        <ref id="d1620553e396a1310">
          
            <mixed-citation id="d1620553e400" publication-type="other">
Ito, K. (1993), "Functions of Bounded Variation," in Encyclopedic Dictionary of Mathematics, Cambridge, MA:
MIT Press, chapter 166, pp. 642-643. [82]</mixed-citation>
        </ref>
        <ref id="d1620553e410a1310">
          
            <mixed-citation id="d1620553e414" publication-type="other">
Leng, C., Lin, Y., and Wahba, G. (2006), "A Note on the Lasso and Related Procedures in Model Selection,"
Statistica Sinica, 16, 1273-1284. [80]</mixed-citation>
        </ref>
        <ref id="d1620553e425a1310">
          
            <mixed-citation id="d1620553e429" publication-type="other">
Mammen, E., and van de Geer, S. (1997), "Locally Adaptive Regression Splines," The Annals of Statistics, 25,
387-413. [74,76]</mixed-citation>
        </ref>
        <ref id="d1620553e439a1310">
          
            <mixed-citation id="d1620553e443" publication-type="other">
Mammen, E., and Yu, K. (2007), "Additive Isotone Regression," in Asymptotics: Particles, Processes and Inverse
Problems. IMS Lecture Notes—Monograph Series, Vol. 55, Beachwood, OH: IMS, pp. 179-195. [73]</mixed-citation>
        </ref>
        <ref id="d1620553e453a1310">
          
            <mixed-citation id="d1620553e457" publication-type="other">
Meier, L., van de Geer, S„ and Biihlmann, P. (2009), "High-Dimensional Additive Modeling," The Annals of
Statistics, 37, 3779-3821. [74,87]</mixed-citation>
        </ref>
        <ref id="d1620553e467a1310">
          
            <mixed-citation id="d1620553e471" publication-type="other">
Meinshausen, N., and Biihlmann, P. (2006), "High-Dimensional Graphs and Variable Selection With the Lasso,"
The Annals of Statistics, 34, 1436—1462. [80]</mixed-citation>
        </ref>
        <ref id="d1620553e481a1310">
          
            <mixed-citation id="d1620553e485" publication-type="other">
Osborne, M. R., Presnell, B„ and Turlach, B. A. (1998), "Knot Selection for Regression Splines via the Lasso,"
in Dimension Reduction, Computational Complexity, and Information, Proceedings of the 30th Symposium
on the Interface, Interface 98, ed. S. Weisberg, Fairfax Station, VA: Interface Foundation of North America,
pp. 44-49. [74,76]</mixed-citation>
        </ref>
        <ref id="d1620553e501a1310">
          
            <mixed-citation id="d1620553e505" publication-type="other">
-(2000), "A New Approach to Variable Selection in Least Squares Problems," IMA Journal of Numerical
Analysis, 20, 389-404. [74,76,83]</mixed-citation>
        </ref>
        <ref id="d1620553e516a1310">
          
            <mixed-citation id="d1620553e520" publication-type="other">
Ravikumar, P., Liu. H., Lafferty, J., and Wasserman, L. (2007), "Spam: Sparse Additive Models," in Advances in
Neural Information Processing Systems (NIPS). Cambridge, MA: MIT Press. [74,83,84,87]</mixed-citation>
        </ref>
        <ref id="d1620553e530a1310">
          
            <mixed-citation id="d1620553e534" publication-type="other">
Tibshirani, R. (1996), "Regression Shrinkage and Selection via the Lasso," Journal of the Royal Statistical Soci-
ety, Ser. B, 58, 267-288. [74,80]</mixed-citation>
        </ref>
        <ref id="d1620553e544a1310">
          
            <mixed-citation id="d1620553e548" publication-type="other">
Tibshirani, R. J.. Hoefling, H., and Tibshirani, R. (2010), "Nearly-Isotonic Regression," unpublished manuscript,
Stanford University, Palo Alto, CA. [82]</mixed-citation>
        </ref>
        <ref id="d1620553e558a1310">
          
            <mixed-citation id="d1620553e562" publication-type="other">
Tibshirani, R., Sanders, M., Rosset, S., Zhu, J., and Knight, K. (2005), "Sparsity and Smoothness via the Fused
Lasso," Journal of the Royal Statistical Society, Ser. B, 67, 91-108. [74]</mixed-citation>
        </ref>
        <ref id="d1620553e572a1310">
          
            <mixed-citation id="d1620553e576" publication-type="other">
Tseng, P. (2001), "Convergence of a Block Coordinate Descent Method for Nondifferentiable Minimization,"
Journal of Optimization Theory and Applications, 109, 475^194. [78]</mixed-citation>
        </ref>
        <ref id="d1620553e586a1310">
          
            <mixed-citation id="d1620553e590" publication-type="other">
Wainwright, M. (2009), "Sharp Thresholds for High-Dimensional and Noisy Recovery of Sparsity," IEEE Trans-
actions on Information Theory, 55, 2183-2201. [85]</mixed-citation>
        </ref>
        <ref id="d1620553e601a1310">
          
            <mixed-citation id="d1620553e605" publication-type="other">
Yuan, M., and Lin, Y. (2006), "Model Selection and Estimation in Regression With Grouped Variables," Journal
of the Royal Statistical Society, Ser. B, 68, 49-67. [81]</mixed-citation>
        </ref>
        <ref id="d1620553e615a1310">
          
            <mixed-citation id="d1620553e619" publication-type="other">
Zou, H. (2006), "The Adaptive Lasso and Its Oracle Properties," Journal of the American Statistical Association,
101, 1418-1429. [81]</mixed-citation>
        </ref>
        <ref id="d1620553e629a1310">
          
            <mixed-citation id="d1620553e633" publication-type="other">
Zou, H., and Li, R. (2008), "One-Step Sparse Estimates in Nonconcave Penalized Likelihood Methods." The
Annals of Statistics, 36, 1509-1533. [81,82]</mixed-citation>
        </ref>
      </ref-list>
    
  </back>
</article>


