<?xml version="1.0" encoding="UTF-8"?>


<article dtd-version="1.0" article-type="research-article">
  <front>
      <journal-meta>
         <journal-id xmlns:xlink="http://www.w3.org/1999/xlink" journal-id-type="jstor">jcompgrapstat</journal-id>
         <journal-id xmlns:xlink="http://www.w3.org/1999/xlink" journal-id-type="jstor">j100879</journal-id>
         <journal-title-group xmlns:xlink="http://www.w3.org/1999/xlink">
            <journal-title>Journal of Computational and Graphical Statistics</journal-title>
         </journal-title-group>
      
         <publisher>
            <publisher-name>American Statistical Association, Institute of Mathematical Statistics, and Interface Foundation of North America</publisher-name>
         </publisher>
         <issn xmlns:xlink="http://www.w3.org/1999/xlink" pub-type="ppub">10618600</issn>
         <custom-meta-group xmlns:xlink="http://www.w3.org/1999/xlink"/>
      </journal-meta>
      <article-meta xmlns:xlink="http://www.w3.org/1999/xlink">
         <volume xmlns:mml="http://www.w3.org/1998/Math/MathML"
                 xmlns:oasis="http://docs.oasis-open.org/ns/oasis-exchange/table">24</volume>
         <issue xmlns:mml="http://www.w3.org/1998/Math/MathML"
                xmlns:oasis="http://docs.oasis-open.org/ns/oasis-exchange/table">3</issue>
         <issue-id>i24736739</issue-id>
         <article-id pub-id-type="jstor">24737290</article-id>
         <article-categories>
            <subj-group>
               <subject>Regression Methods</subject>
            </subj-group>
         </article-categories>
         <title-group>
            <article-title>An Algorithm for Nonlinear, Nonparametric Model Choice and Prediction</article-title>
         </title-group>
         <contrib-group>
            <contrib>
               <string-name>
                  <given-names>Frédéric</given-names>
                  <surname>Ferraty</surname>
               </string-name>
            </contrib>
            <contrib>
               <string-name>
                  <given-names>Peter</given-names>
                  <surname>Hall</surname>
               </string-name>
            </contrib>
         </contrib-group>
         <pub-date pub-type="ppub">
            <day>1</day>
            <month>9</month>
            <year>2015</year>
         </pub-date>
         <fpage>695</fpage>
         <lpage>714</lpage>
      
      
      
      
      
      
      
      
      
      
      
      
         <permissions>
            <copyright-statement>© 2015 American Statistical Association, the Institute of Mathematical Statistics, and the Interface Foundation of North America</copyright-statement>
         </permissions>
         <self-uri xlink:href="https://www.jstor.org/stable/24737290"/>
      
      
         <abstract>
            <p>We introduce an algorithm which, in the context of nonlinear regression on vector-valued explanatory variables, aims to choose those combinations of vector components that provide best prediction. The algorithm is constructed specifically so that it devotes attention to components that might be of relatively little predictive value by themselves, and so might be ignored by more conventional methodology for model choice, but which, in combination with other difficult-to-find components, can be particularly beneficial for prediction. The design of the algorithm is also motivated by a desire to choose vector components that become redundant once appropriate combinations of other, more relevant components are selected. Our theoretical arguments show these goals are met in the sense that, with probability converging to 1 as sample size increases, the algorithm correctly determines a small, fixed number of variables on which the regression mean, g say, depends, even if dimension diverges to infinity much faster than n. Moreover, the estimated regression mean based on those variables approximates g with an error that, to first order, equals the error which would arise if we were told in advance the correct variables. In this sense, the estimator achieves oracle performance. Our numerical work indicates that the algorithm is suitable for very high dimensional problems, where it keeps computational labor in check by using a novel sequential argument, and also for more conventional prediction problems, where dimension is relatively low.</p>
         </abstract>
         <custom-meta-group>
            <custom-meta>
               <meta-name>lang</meta-name>
               <meta-value>eng</meta-value>
            </custom-meta>
         </custom-meta-group>
      </article-meta>
  </front>
  <back>
    
      <ref-list>
        <title>REFERENCES</title>
        <ref id="d1425e251a1310">
          
            <mixed-citation id="d1425e255" publication-type="other">
Bühlmann, P., and Meier, L. (2008), Discussion of "One-Step Sparse Estimates in Nonconcave Penalized Likeli-
hood Models" by H. Zou and R. Li, The Annals of Statistics, 36, 1534-1541. [696]</mixed-citation>
        </ref>
        <ref id="d1425e265a1310">
          
            <mixed-citation id="d1425e269" publication-type="other">
Bühlmann, P., and van de Geer, S. (2011), Statistics for High-Dimensional Data: Methods, Theory and Applica-
tions, Berlin: Springer. [696]</mixed-citation>
        </ref>
        <ref id="d1425e279a1310">
          
            <mixed-citation id="d1425e283" publication-type="other">
Bushel, P., Wolfinger, R. D., and Gibson, G. (2007), "Simultaneous Clustering of Gene Expression Data With
Clinical Chemistry and Pathological Evaluations Reveals Phenotypic Prototypes," BMC Systems Biology, 1.
doi: 10.1186/1752-0509-1-15. [700]</mixed-citation>
        </ref>
        <ref id="d1425e296a1310">
          
            <mixed-citation id="d1425e300" publication-type="other">
Candès, E., and Tao, T. (2007), "The Dantzig Selector: Statistical Estimation When p is Much Larger Than n,"
The Annals of Statistics, 35, 2313-2351. [696]</mixed-citation>
        </ref>
        <ref id="d1425e311a1310">
          
            <mixed-citation id="d1425e315" publication-type="other">
Dejean, S., Gonzalez, I., Le Cao, K.-A., Monget, P., and Coquery, J. (2011), "mixOmics: Omics Data Integration
Project. R package version 3.0," available at http://CRAN.R-project.org/package=mixOmics. [700]</mixed-citation>
        </ref>
        <ref id="d1425e325a1310">
          
            <mixed-citation id="d1425e329" publication-type="other">
Efron, B., Hastie, T., Johnstone, I., and Tibshirani, R. (2004), "Least Angle Regression," The Annals of Statistics,
32,407-499. [696]</mixed-citation>
        </ref>
        <ref id="d1425e339a1310">
          
            <mixed-citation id="d1425e343" publication-type="other">
Fan, J., and Gijbels, I. (1996), Local Polynomial Modeling and its Applications, London: Chapman and Hall.
[700]</mixed-citation>
        </ref>
        <ref id="d1425e353a1310">
          
            <mixed-citation id="d1425e357" publication-type="other">
Fan, J., and Li, R. (2001), "Variable Selection via Nonconcave Penalized Likelihood and its Oracle Properties,"
Journal of the American Statistical Association, 96, 1348-1360. [696]</mixed-citation>
        </ref>
        <ref id="d1425e367a1310">
          
            <mixed-citation id="d1425e371" publication-type="other">
Fan, J., and Lv, J. (2010), "A Selective Overview of Variable Selection in High Dimensional Feature Space"
(invited review article), Statistica Sinica, 20, 101-148. [696]</mixed-citation>
        </ref>
        <ref id="d1425e381a1310">
          
            <mixed-citation id="d1425e385" publication-type="other">
Ferraty, F., Hall, P., and Vieu, P. (2010), "Most-Predictive Design Points for Functional Data Predictors,"
Biometrika, 97, 807-824. [696]</mixed-citation>
        </ref>
        <ref id="d1425e396a1310">
          
            <mixed-citation id="d1425e400" publication-type="other">
Friedman, J., Hastie, T., Höfling, H„ and Tibshirani, R. (2007), "Pathwise Coordinate Optimization," Annals of
Applied Statistics, 1, 302-332. [696]</mixed-citation>
        </ref>
        <ref id="d1425e410a1310">
          
            <mixed-citation id="d1425e414" publication-type="other">
Fu, W. (1998), "Penalized Regressions: The Bridge versus the Lasso," Journal of Computational and Graphical
Statistics, 1, 397-416. [696]</mixed-citation>
        </ref>
        <ref id="d1425e424a1310">
          
            <mixed-citation id="d1425e428" publication-type="other">
Geladi, P., and Kowalski, B. R. (1986), "Partial Least Squares Regression: A Tutorial," Analytica Chimica Acta,
185, 1-17. [702]</mixed-citation>
        </ref>
        <ref id="d1425e438a1310">
          
            <mixed-citation id="d1425e442" publication-type="other">
Hastie, T., Tibshirani, R., and Friedman, J. (2009), The Elements of Statistical Learning: Data Mining, Inference,
and Prediction (2nd ed.), New York: Springer. [696]</mixed-citation>
        </ref>
        <ref id="d1425e452a1310">
          
            <mixed-citation id="d1425e456" publication-type="other">
Huang, J., Horowitz, J. L., and Wei, F. (2010), "Variable Selection via Nonparametric Additive Models," The
Annals of Statistics, 38, 2282-2313. [696]</mixed-citation>
        </ref>
        <ref id="d1425e466a1310">
          
            <mixed-citation id="d1425e470" publication-type="other">
Lê Cao, K. A., Rossouw, D., Robert-Granié, C., and Besse, P. (2008), "A Sparse PLS for Variable Selection When
Integrating Omics Data," Statistical Applications in Genetics and Molecular Biology, 7, 35. [702]</mixed-citation>
        </ref>
        <ref id="d1425e481a1310">
          
            <mixed-citation id="d1425e485" publication-type="other">
Martens, H., and Naes, T. (1989), Multivariate Calibration, New York: Wiley. [702]</mixed-citation>
        </ref>
        <ref id="d1425e492a1310">
          
            <mixed-citation id="d1425e496" publication-type="other">
Meier, L„ van de Geer, S„ and Biihlmann, P. (2009), "High-Dimensional Additive Modeling," The Annals of
Statistics, 37, 3779-3821. [696]</mixed-citation>
        </ref>
        <ref id="d1425e506a1310">
          
            <mixed-citation id="d1425e510" publication-type="other">
Meinshausen, N. (2007), "Relaxed Lasso," Computational Statistics and Data Analysis, 52, 374-393. [696]</mixed-citation>
        </ref>
        <ref id="d1425e517a1310">
          
            <mixed-citation id="d1425e521" publication-type="other">
R Development Core Team (2011), R: A Language and Environment for Statistical Computing, R Foundation for
Statistical Computing, Vienna, Austria. ISBN 3-900051-07-0. Available at http://www.R-project.org/. [699]</mixed-citation>
        </ref>
        <ref id="d1425e531a1310">
          
            <mixed-citation id="d1425e535" publication-type="other">
Ravikumar, P., Lafferty, J., Liu, H., and Wasserman, L. (2009), "Sparse Additive Models," Journal of the Royal
Statistical Society, Series B, 71, 1009-1030. [696]</mixed-citation>
        </ref>
        <ref id="d1425e545a1310">
          
            <mixed-citation id="d1425e549" publication-type="other">
Revolution Analytics (2011), "doSNOW: Foreach Parallel Adaptor for the Snow Package. R Package Version
1.0.5," available at http://CRAN.R-project.org/package=doSNOW. [699]</mixed-citation>
        </ref>
        <ref id="d1425e560a1310">
          
            <mixed-citation id="d1425e564" publication-type="other">
Tibshirani, R. (1996), "Regression Analysis and Selection via the Lasso," Journal of the Royal Statistical Society,
Series B, 58, 267-288. [696]</mixed-citation>
        </ref>
        <ref id="d1425e574a1310">
          
            <mixed-citation id="d1425e578" publication-type="other">
Wold, H. (1966), "Estimation of Principal Components and Related Models by Iterative Least Squares." in
Multivariate Analysis, ed. P. R. Krishnaiah, New York: Academic Press, pp. 391-420. [702]</mixed-citation>
        </ref>
        <ref id="d1425e588a1310">
          
            <mixed-citation id="d1425e592" publication-type="other">
Yuan, M., and Lin, Y. (2006), "Model Selection and Estimation in Regression With Grouped Variables," Journal
of the Royal Statistical Society, Series B, 68,49-67. [696]</mixed-citation>
        </ref>
        <ref id="d1425e602a1310">
          
            <mixed-citation id="d1425e606" publication-type="other">
Zou, H. (2006), "The Adaptive Lasso and its Oracle Properties," Journal of the American Statistical Association,
101, 1418-1429. [696]</mixed-citation>
        </ref>
        <ref id="d1425e616a1310">
          
            <mixed-citation id="d1425e620" publication-type="other">
Zou, H., and Hastie, T. (2005), "Regularization and Variable Selection via the Elastic Net," Journal of the Royal
Statistical Society, Series B, 67, 301-320. [696]</mixed-citation>
        </ref>
      </ref-list>
    
  </back>
</article>


