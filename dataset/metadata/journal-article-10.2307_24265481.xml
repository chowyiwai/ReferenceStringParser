<?xml version="1.0" encoding="UTF-8"?>


<article dtd-version="1.0" article-type="research-article">
  <front>
      <journal-meta>
         <journal-id xmlns:xlink="http://www.w3.org/1999/xlink" journal-id-type="jstor">computermusicj</journal-id>
         <journal-id xmlns:xlink="http://www.w3.org/1999/xlink" journal-id-type="jstor">j100532</journal-id>
         <journal-title-group xmlns:xlink="http://www.w3.org/1999/xlink">
            <journal-title>Computer Music Journal</journal-title>
         </journal-title-group>
      
         <publisher>
            <publisher-name>MIT Press</publisher-name>
         </publisher>
         <issn xmlns:xlink="http://www.w3.org/1999/xlink" pub-type="ppub">01489267</issn>
         <issn xmlns:xlink="http://www.w3.org/1999/xlink" pub-type="epub">15315169</issn>
         <custom-meta-group xmlns:xlink="http://www.w3.org/1999/xlink"/>
      </journal-meta>
      <article-meta xmlns:xlink="http://www.w3.org/1999/xlink">
         <volume xmlns:mml="http://www.w3.org/1998/Math/MathML"
                 xmlns:oasis="http://docs.oasis-open.org/ns/oasis-exchange/table">38</volume>
         <issue xmlns:mml="http://www.w3.org/1998/Math/MathML"
                xmlns:oasis="http://docs.oasis-open.org/ns/oasis-exchange/table">3</issue>
         <issue-id>i24265475</issue-id>
         <article-id pub-id-type="jstor">24265481</article-id>
         <article-categories>
            <subj-group>
               <subject>Mapping</subject>
            </subj-group>
         </article-categories>
         <title-group>
            <article-title>Mapping Through Listening</article-title>
         </title-group>
         <contrib-group>
            <contrib>
               <string-name>
                  <given-names>Baptiste</given-names>
                  <surname>Caramiaux</surname>
               </string-name>
            </contrib>
            <contrib>
               <string-name>
                  <given-names>Jules</given-names>
                  <surname>Françoise</surname>
               </string-name>
            </contrib>
            <contrib>
               <string-name>
                  <given-names>Norbert</given-names>
                  <surname>Schnell</surname>
               </string-name>
            </contrib>
            <contrib>
               <string-name>
                  <given-names>Frédéric</given-names>
                  <surname>Bevilacqua</surname>
               </string-name>
            </contrib>
         </contrib-group>
         <pub-date pub-type="ppub">
            <day>1</day>
            <month>10</month>
            <year>2014</year>
         </pub-date>
         <fpage>34</fpage>
         <lpage>48</lpage>
      
      
      
      
      
      
         <permissions>
            <copyright-statement>©2014 Massachusetts Institute of Technology</copyright-statement>
         </permissions>
         <self-uri xlink:href="https://www.jstor.org/stable/24265481"/>
      
      
         <abstract>
            <p>Gesture-to-sound mapping is generally defined as the association between gestural and sound parameters. This article describes an approach that brings forward the perception–action loop as a fundamental design principle for gesture–sound mapping in digital music instrument. Our approach considers the processes of listening as the foundation—and the first step—in the design of action–sound relationships. In this design process, the relationship between action and sound is derived from actions that can be perceived in the sound. Building on previous work on listening modes and gestural descriptions, we propose to distinguish between three mapping strategies: instantaneous, temporal, and metaphorical. Our approach makes use of machine-learning techniques for building prototypes, from digital music instruments to interactive installations. Four different examples of scenarios and prototypes are described and discussed.</p>
         </abstract>
         <custom-meta-group>
            <custom-meta>
               <meta-name>lang</meta-name>
               <meta-value>eng</meta-value>
            </custom-meta>
         </custom-meta-group>
      </article-meta>
  </front>
  <back>
    
      <ref-list>
        <title>References</title>
        <ref id="d356186e214a1310">
          
            <mixed-citation id="d356186e218" publication-type="other">
Arfib, D., et al. 2002. "Strategies of Mapping Between
Gesture Data and Synthesis Model Parameters Using
Perceptual Spaces." Organised Sound 7(02): 127-144.</mixed-citation>
        </ref>
        <ref id="d356186e231a1310">
          
            <mixed-citation id="d356186e235" publication-type="other">
Bevilacqua, F., et al. 2010. "Continuous Realtime Ges-
ture Following and Recognition." In S. Kopp and
I. Wachsmuth, eds. Gesture in Embodied Commu-
nication and Human-Computer Interaction. Berlin:
Springer, pp. 73-84.</mixed-citation>
        </ref>
        <ref id="d356186e254a1310">
          
            <mixed-citation id="d356186e258" publication-type="other">
Bevilacqua, F., et al. 2011. "Online Gesture Analysis and
Control of Audio Processing." In J. Solis and K. Ng, eds.
Musical Robots and Interactive Multimodal Systems.
Berlin: Springer, pp. 127-142.</mixed-citation>
        </ref>
        <ref id="d356186e274a1310">
          
            <mixed-citation id="d356186e278" publication-type="other">
Cadoz, C. 1988. "Instrumental Gesture and Musical
Composition." In Proceedings of the International
Computer Music Conference, pp. 1-12.</mixed-citation>
        </ref>
        <ref id="d356186e292a1310">
          
            <mixed-citation id="d356186e296" publication-type="other">
Caramiaux, B., F. Bevilacqua, and N. Schnell. 2010a.
"Analysing Gesture and Sound Similarities with a
HMM-Based Divergence Measure." In Proceedings of
the Sound and Music Computing Conference. Available
online at smcnetwork.org/files/proceedings/2010/9.pdf.
Accessed March 2014.</mixed-citation>
        </ref>
        <ref id="d356186e319a1310">
          
            <mixed-citation id="d356186e323" publication-type="other">
Caramiaux, B.( F. Bevilacqua, and N. Schnell. 2010b.
"Towards a Gesture-Sound Cross-Modal Analysis." In
S. Kopp and I. Wachsmuth, eds. Gesture in Embod-
ied Communication and Human-Computer. Berlin:
Springer Verlag, pp. 158-170.</mixed-citation>
        </ref>
        <ref id="d356186e342a1310">
          
            <mixed-citation id="d356186e346" publication-type="other">
Caramiaux, B., and A. Tanaka. 2013. "Machine Learn-
ing of Musical Gestures." In Proceedings of the
Conference on New Interfaces for Musical Expres-
sion. Available online at baptistecaramiaux.com/blog/
wp-con tent/uploads/2013/05/nime2013 _mlrev.pdf.
Accessed March 2014.</mixed-citation>
        </ref>
        <ref id="d356186e369a1310">
          
            <mixed-citation id="d356186e373" publication-type="other">
Caramiaux, B., et al. 2014. "The Role of Sound Source
Perception in Gestural Sound Description." ACM
Transactions on Applied Perception 11(1): 1—19.</mixed-citation>
        </ref>
        <ref id="d356186e386a1310">
          
            <mixed-citation id="d356186e390" publication-type="other">
Caramiaux, B., et al. In press. "Adaptive Gesture Recogni-
tion with Variation Estimation for Interactive Systems."
ACM Transactions on Iterative Intelligent Systems.</mixed-citation>
        </ref>
        <ref id="d356186e403a1310">
          
            <mixed-citation id="d356186e407" publication-type="other">
Castagne, N., et al. 2004. "Haptics in Computer Music:
A Paradigm Shift." In Proceedings of the Eurohaptics
Meeting, pp. 174-181.</mixed-citation>
        </ref>
        <ref id="d356186e421a1310">
          
            <mixed-citation id="d356186e425" publication-type="other">
Chion, M. 1983. Guide des objets sonores: Pierre Schaeffer
et la recherche musicale. Paris: Buchet/Chastel.</mixed-citation>
        </ref>
        <ref id="d356186e435a1310">
          
            <mixed-citation id="d356186e439" publication-type="other">
Fadiga, L., et al. 2002. "Speech Listening Specifically
Modulates the Excitability of Tongue Muscles: A TMS
Study." European Journal of Neuroscience 15(2):399-
402.</mixed-citation>
        </ref>
        <ref id="d356186e455a1310">
          
            <mixed-citation id="d356186e459" publication-type="other">
Fiebrink, R. A. 2011. "Real-Time Human Interaction with
Supervised Learning Algorithms for Music Composition
and Performance." PhD Thesis, Princeton University,
Department of Computer Science.</mixed-citation>
        </ref>
        <ref id="d356186e475a1310">
          
            <mixed-citation id="d356186e479" publication-type="other">
Fiebrink, R. A., P. R. Cook, and D. Trueman. 2011.
"Human Model Evaluation in Interactive Supervised
Learning." In Proceedings of the SIGCHI Conference
on Human Factors in Computing Systems, pp. 147-156.</mixed-citation>
        </ref>
        <ref id="d356186e495a1310">
          
            <mixed-citation id="d356186e499" publication-type="other">
Franijoise, J., B. Caramiaux, and F. Bevilacqua. 2011.
"Realtime Segmentation and Recognition of Gestures
Using Hierarchical Markov Models." Master's Thesis,
Universite Pierre et Marie Curie (Paris VI).</mixed-citation>
        </ref>
        <ref id="d356186e515a1310">
          
            <mixed-citation id="d356186e519" publication-type="other">
Frangoise, J., B. Caramiaux, and F. Bevilacqua. 2012.
"A Hierarchical Approach for the Design of Gesture-
to-Sound Mappings." In Proceedings of the Sound
and Music Computing Conference. Available online
at smcnetwork.org/system/files/smc2012-203.pdf.
Accessed March 2014.</mixed-citation>
        </ref>
        <ref id="d356186e543a1310">
          
            <mixed-citation id="d356186e547" publication-type="other">
Franijoise, J., N. Schnell, and F. Bevilacqua. 2013. "A
Multimodal Probabilistic Model for Gesture-Based
Control of Sound Synthesis." In Proceedings of the ACM
International Conference on Multimedia, pp. 705-708.</mixed-citation>
        </ref>
        <ref id="d356186e563a1310">
          
            <mixed-citation id="d356186e567" publication-type="other">
Frey, A., et al. 2009. "Temporal Semiotic Units as Minimal
Meaningful Units in Music? An Electrophysiological
Approach." Music Perception 26(3):247-256.</mixed-citation>
        </ref>
        <ref id="d356186e580a1310">
          
            <mixed-citation id="d356186e584" publication-type="other">
Gaver, W. W. 1993a. "How Do We Hear in the World?
Explorations in Ecological Acoustics." Ecological
Psychology 5(4):285-313.</mixed-citation>
        </ref>
        <ref id="d356186e597a1310">
          
            <mixed-citation id="d356186e601" publication-type="other">
Gaver, W. W. 1993b. "What in the World Do We Hear? An
Ecological Approach to Auditory Event Perception."
Ecological Psychology 5{ 1): 1—2,9.</mixed-citation>
        </ref>
        <ref id="d356186e614a1310">
          
            <mixed-citation id="d356186e618" publication-type="other">
Gillian, N. 2011. "Gesture Recognition for Musician
Computer Interaction." PhD Thesis, Queen's Univer-
sity Belfast, School of Music and Sonic Arts.</mixed-citation>
        </ref>
        <ref id="d356186e631a1310">
          
            <mixed-citation id="d356186e635" publication-type="other">
Godoy, R. I. 2006. "Gestural-Sonorous Objects: Embodied
Extensions of Schaeffer's Conceptual Apparatus."
Organised Sound 11(2): 149—157.</mixed-citation>
        </ref>
        <ref id="d356186e649a1310">
          
            <mixed-citation id="d356186e653" publication-type="other">
God0y, R. I., E. Haga, and A. R. Jensenius. 2006. "Ex-
ploring Music-Related Gestures by Sound-Tracing: A
Preliminary Study." In Proceedings of the International
Symposium on Gesture Interfaces for Multimedia
Systems, pp. 27-33.</mixed-citation>
        </ref>
        <ref id="d356186e672a1310">
          
            <mixed-citation id="d356186e676" publication-type="other">
Haueisen, J., and T. R. Knosche. 2001. "Involuntary Motor
Activity in Pianists Evoked by Music Perception."
Journal of Cognitive Neuroscience 13(6):786-792.</mixed-citation>
        </ref>
        <ref id="d356186e689a1310">
          
            <mixed-citation id="d356186e693" publication-type="other">
Hunt, A., and R. Kirk. 2000. "Mapping Strategies for Mu-
sical Performance." In M. M. Wanderley and M. Battier,
eds. Trends in Gestural Control of Music. Paris: Institut
de Recherche et Coordination Acoustique/Musique,
pp. 231-258.</mixed-citation>
        </ref>
        <ref id="d356186e712a1310">
          
            <mixed-citation id="d356186e716" publication-type="other">
Huron, D. 2002. "A Six-Component Theory of Auditory-
Evoked Emotion." In Proceedings of the International
Conference on Music Perception and Cognition,
pp. 673-676.</mixed-citation>
        </ref>
        <ref id="d356186e732a1310">
          
            <mixed-citation id="d356186e736" publication-type="other">
Kiissner, M. 2013. "Music and Shape." Literary and
Linguistic Computing 28(3): 1-8.</mixed-citation>
        </ref>
        <ref id="d356186e746a1310">
          
            <mixed-citation id="d356186e750" publication-type="other">
Lahav, A., et al. 2005. "The Power of Listening: Auditory-
Motor Interactions in Musical Training." Annals of the
New York Academy of Sciences 1060(1): 189-194.</mixed-citation>
        </ref>
        <ref id="d356186e764a1310">
          
            <mixed-citation id="d356186e768" publication-type="other">
Large, E. W. 2000. "On Synchronizing Movements to
Music." Human Movement Science 19(4):527—566.</mixed-citation>
        </ref>
        <ref id="d356186e778a1310">
          
            <mixed-citation id="d356186e782" publication-type="other">
Large, E. W., and C. Palmer. 2002. "Perceiving Temporal
Regularity in Music." Cognitive Science 26(1): 1-37.</mixed-citation>
        </ref>
        <ref id="d356186e792a1310">
          
            <mixed-citation id="d356186e796" publication-type="other">
Leman, M. 2007. Embodied Music Cognition and Me-
diation Technology. Cambridge, Massachusetts: MIT
Press.</mixed-citation>
        </ref>
        <ref id="d356186e809a1310">
          
            <mixed-citation id="d356186e813" publication-type="other">
Leman, M., et al. 2009. "Sharing Musical Expression
Through Embodied Listening: A Case Study Based on
Chinese Guqin Music." Music Perception 26(3):263—
278.</mixed-citation>
        </ref>
        <ref id="d356186e829a1310">
          
            <mixed-citation id="d356186e833" publication-type="other">
Liberman, A. M., and I. G. Mattingly. 1985. "The Motor
Theory of Speech Perception Revised." Cognition
21(1): 1—36.</mixed-citation>
        </ref>
        <ref id="d356186e846a1310">
          
            <mixed-citation id="d356186e850" publication-type="other">
Maes, P.-J. 2012. "An Empirical Study of Embodied
Music Listening and Its Applications in Mediation
Technology." PhD Dissertation, Ghent University.</mixed-citation>
        </ref>
        <ref id="d356186e864a1310">
          
            <mixed-citation id="d356186e868" publication-type="other">
Merer, A. 2011. "Caracterisation acoustique et perceptive
du mouvement evoque les sons pour le controle de la
synthese." PhD Dissertation, Universite de Provence
Aix-Marseille 1.</mixed-citation>
        </ref>
        <ref id="d356186e884a1310">
          
            <mixed-citation id="d356186e888" publication-type="other">
Merleau-Ponty, M. 1945. La Phenomenologie de la
Perception. Paris: Gallimard.</mixed-citation>
        </ref>
        <ref id="d356186e898a1310">
          
            <mixed-citation id="d356186e902" publication-type="other">
Miranda, E., and M. Wanderley. 2006. New Digital
Musical Instruments: Control and Interaction beyond
the Keyboard. Middleton, Wisconsin: A-R Editions.</mixed-citation>
        </ref>
        <ref id="d356186e915a1310">
          
            <mixed-citation id="d356186e919" publication-type="other">
Mitchell, T. M. 2006. "The Discipline of Machine Learn-
ing." Technical Report CMU-ML-06-108. Pittsburgh,
Pennsylvania: Carnegie Mellon University, School of
Computer Science, Machine Learning Department.</mixed-citation>
        </ref>
        <ref id="d356186e935a1310">
          
            <mixed-citation id="d356186e939" publication-type="other">
Momeni, A., and C. Henry. 2006. "Dynamic Independent
Mapping Layers for Concurrent Control of Audio and
Video Synthesis." Computer Music Journal 30(l):49-66.</mixed-citation>
        </ref>
        <ref id="d356186e952a1310">
          
            <mixed-citation id="d356186e956" publication-type="other">
Noe, A. 2005. Action in Perception. Cambridge, Mas-
sachusetts: MIT Press.</mixed-citation>
        </ref>
        <ref id="d356186e967a1310">
          
            <mixed-citation id="d356186e971" publication-type="other">
Nymoen, K., et al. 2011. "Analyzing Sound Tracings: A
Multimodal Approach to Music Information Retrieval."
In Proceedings of the International ACM Workshop on
Music Information Retrieval with User-centered and
Multimodal Strategies, pp. 39-44.</mixed-citation>
        </ref>
        <ref id="d356186e990a1310">
          
            <mixed-citation id="d356186e994" publication-type="other">
Rasamimanana, N., et al. 2011. "Modular Musical
Objects Towards Embodied Control of Digital Music."
In Proceedings of the International Conference on
Tangible, Embedded, and Embodied Interaction,
pp. 9-12.</mixed-citation>
        </ref>
        <ref id="d356186e1013a1310">
          
            <mixed-citation id="d356186e1017" publication-type="other">
Rovan, J., et al. 1997. "Instrumental Gestural Mapping
Strategies as Expressivity Determinants in Computer
Music Performance." In Proceedings of Kansei: The
Technology of Emotion Workshop, pp. 68-73.</mixed-citation>
        </ref>
        <ref id="d356186e1033a1310">
          
            <mixed-citation id="d356186e1037" publication-type="other">
Schaeffer, P. 1966. Traite des Objets Musicaux. Paris:
Editions du Seuil.</mixed-citation>
        </ref>
        <ref id="d356186e1047a1310">
          
            <mixed-citation id="d356186e1051" publication-type="other">
Schwarz, D., N. Schnell, and S. Gulluni. 2009. "Scalability
in Content-Based Navigation of Sound Databases." In
Proceedings of the International Computer Music
Conference, -pp. 13-16.</mixed-citation>
        </ref>
        <ref id="d356186e1067a1310">
          
            <mixed-citation id="d356186e1071" publication-type="other">
Tuuri, K., and T. Eerola. 2012. "Formulating a Revised
Taxonomy for Modes of Listening." Journal of New
Music Research 41(2): 137—152,.</mixed-citation>
        </ref>
        <ref id="d356186e1085a1310">
          
            <mixed-citation id="d356186e1089" publication-type="other">
Van Nort, D. 2009. "Instrumental Listening: Sonic Gesture
as Design Principle." Oganised Sound 14(02): 177—
187.</mixed-citation>
        </ref>
        <ref id="d356186e1102a1310">
          
            <mixed-citation id="d356186e1106" publication-type="other">
Van Nort, D., M. M. Wanderley, and P. Depalle. 2004.
"On the Choice of Mappings Based on Geometric
Properties." In Proceedings of the Conference on New
Interfaces for Musical Expression, pp. 87-91.</mixed-citation>
        </ref>
        <ref id="d356186e1122a1310">
          
            <mixed-citation id="d356186e1126" publication-type="other">
Varela, E, E. Thompson, and E. Rosch. 1991. The Embod-
ied Mind: Cognitive Science and Human Experience.
Cambridge, Massachusetts: MIT Press.</mixed-citation>
        </ref>
        <ref id="d356186e1139a1310">
          
            <mixed-citation id="d356186e1143" publication-type="other">
Wanderley, M. M. 2002. "Mapping Strategies in Real-Time
Computer Music." Organised Sound 7(2):83-84.</mixed-citation>
        </ref>
        <ref id="d356186e1153a1310">
          
            <mixed-citation id="d356186e1157" publication-type="other">
Wanderley, M. M., and P. Depalle. 2004. "Gestural
Control of Sound Synthesis." Proceedings of the IEEE
92(4):632-644.</mixed-citation>
        </ref>
        <ref id="d356186e1170a1310">
          
            <mixed-citation id="d356186e1174" publication-type="other">
Wanderley M. M., and N. Orio. 2002. "Evaluation of Input
Devices for Musical Expression: Borrowing Tools from
HCI." Computer Music Journal 26(3):62-76.</mixed-citation>
        </ref>
        <ref id="d356186e1188a1310">
          
            <mixed-citation id="d356186e1192" publication-type="other">
Zatorre, R. J., J. L. Chen, and V. B. Penhune. 2007. "When
the Brain Plays Music: Auditory-Motor Interactions
in Music Perception and Production." Nature Reviews
Neuroscience 8(7):547—558.</mixed-citation>
        </ref>
      </ref-list>
    
  </back>
</article>


