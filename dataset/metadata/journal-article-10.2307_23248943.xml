<?xml version="1.0" encoding="UTF-8"?>


<article dtd-version="1.0" article-type="research-article">
  <front>
      <journal-meta>
         <journal-id xmlns:xlink="http://www.w3.org/1999/xlink" journal-id-type="jstor">jcompgrapstat</journal-id>
         <journal-id xmlns:xlink="http://www.w3.org/1999/xlink" journal-id-type="jstor">j100879</journal-id>
         <journal-title-group xmlns:xlink="http://www.w3.org/1999/xlink">
            <journal-title>Journal of Computational and Graphical Statistics</journal-title>
         </journal-title-group>
      
         <publisher>
            <publisher-name>JCGS Management Committee of the American Statistical Association, Institute of Mathematical Statistics, and Interface Foundation of North America</publisher-name>
         </publisher>
         <issn xmlns:xlink="http://www.w3.org/1999/xlink" pub-type="ppub">10618600</issn>
         <custom-meta-group xmlns:xlink="http://www.w3.org/1999/xlink"/>
      </journal-meta>
      <article-meta xmlns:xlink="http://www.w3.org/1999/xlink">
         <volume xmlns:mml="http://www.w3.org/1998/Math/MathML"
                 xmlns:oasis="http://docs.oasis-open.org/ns/oasis-exchange/table">20</volume>
         <issue xmlns:mml="http://www.w3.org/1998/Math/MathML"
                xmlns:oasis="http://docs.oasis-open.org/ns/oasis-exchange/table">4</issue>
         <issue-id>i23241032</issue-id>
         <article-id pub-id-type="jstor">23248943</article-id>
         <article-categories>
            <subj-group>
               <subject>Mining High-Dimensional Data</subject>
            </subj-group>
         </article-categories>
         <title-group>
            <article-title>A Framework for Unbiased Model Selection Based on Boosting</article-title>
         </title-group>
         <contrib-group>
            <contrib>
               <string-name>
                  <given-names>Benjamin</given-names>
                  <surname>Hofner</surname>
               </string-name>
            </contrib>
            <contrib>
               <string-name>
                  <given-names>Torsten</given-names>
                  <surname>Hothorn</surname>
               </string-name>
            </contrib>
            <contrib>
               <string-name>
                  <given-names>Thomas</given-names>
                  <surname>Kneib</surname>
               </string-name>
            </contrib>
            <contrib>
               <string-name>
                  <given-names>Matthias</given-names>
                  <surname>Schmid</surname>
               </string-name>
            </contrib>
         </contrib-group>
         <pub-date pub-type="ppub">
            <day>1</day>
            <month>12</month>
            <year>2011</year>
         </pub-date>
         <fpage>956</fpage>
         <lpage>971</lpage>
      
      
      
      
      
         <permissions>
            <copyright-statement>© 2011 American Statistical Association, the Institute of Mathematical Statistics, and the Interface Foundation of North America</copyright-statement>
         </permissions>
      
         <self-uri xlink:href="https://www.jstor.org/stable/23248943"/>
      
      
         <abstract>
            <p>Variable and model selection are of major concern in many statistical applications, especially in high-dimensional regression models. Boosting is a convenient statistical method that combines model fitting with intrinsic model selection. We investigate the impact of base-learner specification on the performance of boosting as a model selection procedure. We show that variable selection may be biased if the covariates are of different nature. Important examples are models combining continuous and categorical covariates, especially if the number of categories is large. In this case, least squares base-learners offer increased flexibility for the categorical covariate and lead to a preference even if the categorical covariate is noninformative. Similar difficulties arise when comparing linear and nonlinear base-learners for a continuous covariate. The additional flexibility in the nonlinear base-learner again yields a preference of the more complex modeling alternative. We investigate these problems from a theoretical perspective and suggest a framework for bias correction based on a general class of penalized least squares base-learners. Making all base-learners comparable in terms of their degrees of freedom strongly reduces the selection bias observed in naive boosting specifications. The importance of unbiased model selection is demonstrated in simulations. Supplemental materials including an application to forest health models, additional simulation results, additional theorems, and proofs for the theorems are available online.</p>
         </abstract>
         <custom-meta-group>
            <custom-meta>
               <meta-name>lang</meta-name>
               <meta-value>eng</meta-value>
            </custom-meta>
         </custom-meta-group>
      </article-meta>
  </front>
  <back>
    
      <ref-list>
        <title>REFERENCES</title>
        <ref id="d980007e240a1310">
          
            <mixed-citation id="d980007e244" publication-type="other">
Binder, H„ and Schumacher, M. (2008), "Allowing for Mandatory Covariates in Boosting Estimation of Sparse
High-Dimensional Survival Models," BMC Bioinformatics, 9. 14. [968]</mixed-citation>
        </ref>
        <ref id="d980007e254a1310">
          
            <mixed-citation id="d980007e258" publication-type="other">
Breiman, L. (2001), "Random Forests," Machine Learning, 45, 5-32. [957]</mixed-citation>
        </ref>
        <ref id="d980007e265a1310">
          
            <mixed-citation id="d980007e269" publication-type="other">
Breiman, L., Friedman, J. H., Olshen, R. A., and Stone, C. J. (1984), Classification and Regression Trees, Cali-
fornia: Wadsworth. [957]</mixed-citation>
        </ref>
        <ref id="d980007e279a1310">
          
            <mixed-citation id="d980007e283" publication-type="other">
Biihlmann, P., and Hothorn, T. (2007), "Boosting Algorithms: Regularization, Prediction and Model Fitting,"
Statistical Science, 22, 477-505. [958]</mixed-citation>
        </ref>
        <ref id="d980007e294a1310">
          
            <mixed-citation id="d980007e298" publication-type="other">
Biihlmann, P., and Yu, B. (2003), "Boosting With the L2 Loss: Regression and Classification," Journal of the
American Statistical Association, 98, 324-339. [957,959]</mixed-citation>
        </ref>
        <ref id="d980007e308a1310">
          
            <mixed-citation id="d980007e312" publication-type="other">
Buja, A.. Hastie, T., and Tibshirani, R. (1989), "Linear Smoothers and Additive Models" (with discussion), The
Annals of Statistics, 17,453-555. [961]</mixed-citation>
        </ref>
        <ref id="d980007e322a1310">
          
            <mixed-citation id="d980007e326" publication-type="other">
Eilers, P. H. C., and Marx, B. D. (1996), "Flexible Smoothing With B-Splines and Penalties," Statistical Science,
11,89-121. [959,961]</mixed-citation>
        </ref>
        <ref id="d980007e336a1310">
          
            <mixed-citation id="d980007e340" publication-type="other">
Fahrmeir, L., Kneib, T., and Lang, S. (2004), "Penalized Structured Additive Regression: A Bayesian Perspec-
tive," Statistica Sinica, 14, 731-761. [958]</mixed-citation>
        </ref>
        <ref id="d980007e350a1310">
          
            <mixed-citation id="d980007e354" publication-type="other">
Freund, Y„ and Schapire, R. (1996), "Experiments With a New Boosting Algorithm," in Proceedings of the
Thirteenth International Conference on Machine Learning, San Francisco, CA: Morgan Kaufmann, pp. 148—
156. [957]</mixed-citation>
        </ref>
        <ref id="d980007e367a1310">
          
            <mixed-citation id="d980007e371" publication-type="other">
Friedman, J., Hastie, T., and Tibshirani, R. (2000), "Additive Logistic Regression: A Statistical View of Boosting"
(with discussion), The Annals of Statistics, 28, 337-407. [957]</mixed-citation>
        </ref>
        <ref id="d980007e382a1310">
          
            <mixed-citation id="d980007e386" publication-type="other">
Gertheiss, J., and Tutz, G. (2009), "Penalized Regression With Ordinal Predictors," International Statistical Re-
view, 77, 345-365. [959]</mixed-citation>
        </ref>
        <ref id="d980007e396a1310">
          
            <mixed-citation id="d980007e400" publication-type="other">
Hastie, T., and Tibshirani, R. (1986), "Generalized Additive Models," Statistical Science, 1, 297-310. [958]</mixed-citation>
        </ref>
        <ref id="d980007e407a1310">
          
            <mixed-citation id="d980007e411" publication-type="other">
— (1990), Generalized Additive Models, London: Chapman &amp; Hall/CRC. [958]</mixed-citation>
        </ref>
        <ref id="d980007e418a1310">
          
            <mixed-citation id="d980007e422" publication-type="other">
Hoerl, A. E., and Kennard, R. W. (1970), "Ridge Regression: Biased Estimation for Nonorthogonal Problems,"
Technometrics, 12, 55-67. [959]</mixed-citation>
        </ref>
        <ref id="d980007e432a1310">
          
            <mixed-citation id="d980007e436" publication-type="other">
Hofner, B., Hothorn, T„ and Kneib, T. (2008), "Variable Selection and Model Choice in Structured Survival Mod-
els," Technical Report 43, Department of Statistics, Ludwig-Maximilans-Universitat Munchen. Available at
http://epub.ub.uni-muenchen.de/7901/. [968]</mixed-citation>
        </ref>
        <ref id="d980007e449a1310">
          
            <mixed-citation id="d980007e453" publication-type="other">
Hothorn, T., Biihlmann, P., Dudoit, S., Molinaro, A., and van der Laan, M. J. (2006), "Survival Ensembles,"
Biostatistics, 7, 355-373. [957]</mixed-citation>
        </ref>
        <ref id="d980007e464a1310">
          
            <mixed-citation id="d980007e468" publication-type="other">
Hothorn, T., Biihlmann, P., Kneib. T., Schmid, M., and Hofner, B. (2009), "mboost: Model-Based Boosting,"
R package version 1.1-4. Available at http://CRAN.R-project.org/package=mboost. [969]</mixed-citation>
        </ref>
        <ref id="d980007e478a1310">
          
            <mixed-citation id="d980007e482" publication-type="other">
— (2010), "Model-Based Boosting 2.0," Journal of Machine Learning Research, 11, 2109-2113. [969]</mixed-citation>
        </ref>
        <ref id="d980007e489a1310">
          
            <mixed-citation id="d980007e493" publication-type="other">
Hothorn, T„ Hornik, K., and Zeileis, A. (2006), "Unbiased Recursive Partitioning: A Conditional Inference
Framework," Journal of Computational and Graphical Statistics, 15, 651-674. [957]</mixed-citation>
        </ref>
        <ref id="d980007e503a1310">
          
            <mixed-citation id="d980007e507" publication-type="other">
Kim, H., and Loh, W. Y. (2003), "Classification Trees With Bivariate Linear Discriminant Node Models," Journal
of Computational and Graphical Statistics, 12, 512-530. [957]</mixed-citation>
        </ref>
        <ref id="d980007e517a1310">
          
            <mixed-citation id="d980007e521" publication-type="other">
Kneib, T., Hothorn, T., and Tutz, G. (2009), "Variable Selection and Model Choice in Geoadditive Regression
Models," Biometrics, 65, 626-634. [957,959,961,968]</mixed-citation>
        </ref>
        <ref id="d980007e531a1310">
          
            <mixed-citation id="d980007e535" publication-type="other">
Loll, W. Y. (2002), "Regression Trees With Unbiased Variable Selection and Interaction Detection," Statistica
Sinica, 12, 361-386. [957]</mixed-citation>
        </ref>
        <ref id="d980007e546a1310">
          
            <mixed-citation id="d980007e550" publication-type="other">
Loh, W. Y., and Vanichsetakul, N. (1988), "Tree-Structured Classification via Generalized Discriminant Analysis"
(with discussion), Journal of the American Statistical Association, 83, 715-725. [957]</mixed-citation>
        </ref>
        <ref id="d980007e560a1310">
          
            <mixed-citation id="d980007e564" publication-type="other">
R Development Core Team (2009), R: A Language and Environment for Statistical Computing, Vienna, Austria:
R Foundation for Statistical Computing. Available at http://www.R-project.org. [969]</mixed-citation>
        </ref>
        <ref id="d980007e574a1310">
          
            <mixed-citation id="d980007e578" publication-type="other">
Ruppert, D. (2002), "Selecting the Number of Knots for Penalized Splines," Journal of Computational and
Graphical Statistics, 11, 735-757. [962]</mixed-citation>
        </ref>
        <ref id="d980007e588a1310">
          
            <mixed-citation id="d980007e592" publication-type="other">
Schmid, M., and Hothorn, T. (2008a), "Boosting Additive Models Using Component-Wise P-splines," Computa-
tional Statistics &amp; Data Analysis, 53, 298-311. [959]</mixed-citation>
        </ref>
        <ref id="d980007e602a1310">
          
            <mixed-citation id="d980007e606" publication-type="other">
— (2008b), "Flexible Boosting of Accelerated Failure Time Models," BMC Bioinformatics, 9, 269. [957]</mixed-citation>
        </ref>
        <ref id="d980007e613a1310">
          
            <mixed-citation id="d980007e617" publication-type="other">
Strobl, C„ Boulesteix, A. L., Zeileis, A., and Hothorn, T. (2007), "Bias in Random Forest Variable Importance
Measures: Illustrations, Sources and a Solution," BMC Bioinformatics, 8, 25. [957]</mixed-citation>
        </ref>
        <ref id="d980007e628a1310">
          
            <mixed-citation id="d980007e632" publication-type="other">
Tutz, G., and Binder, H. (2006), "Generalized Additive Modelling With Implicit Variable Selection by Likelihood-
Based Boosting," Biometrics, 62, 961-971. [968]</mixed-citation>
        </ref>
        <ref id="d980007e642a1310">
          
            <mixed-citation id="d980007e646" publication-type="other">
Vapnik, V. (1995), The Nature of Statistical Learning Theory, New York: Springer-Verlag. [957]</mixed-citation>
        </ref>
      </ref-list>
    
  </back>
</article>


