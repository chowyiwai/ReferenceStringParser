<?xml version="1.0" encoding="UTF-8"?>


<article dtd-version="1.0" article-type="research-article">
  <front>
      <journal-meta>
         <journal-id xmlns:xlink="http://www.w3.org/1999/xlink" journal-id-type="jstor">jcompgrapstat</journal-id>
         <journal-id xmlns:xlink="http://www.w3.org/1999/xlink" journal-id-type="jstor">j100879</journal-id>
         <journal-title-group xmlns:xlink="http://www.w3.org/1999/xlink">
            <journal-title>Journal of Computational and Graphical Statistics</journal-title>
         </journal-title-group>
      
         <publisher>
            <publisher-name>JCGS Management Committee of the American Statistical Association, Institute of Mathematical Statistics, and Interface Foundation of North America</publisher-name>
         </publisher>
         <issn xmlns:xlink="http://www.w3.org/1999/xlink" pub-type="ppub">10618600</issn>
         <custom-meta-group xmlns:xlink="http://www.w3.org/1999/xlink"/>
      </journal-meta>
      <article-meta xmlns:xlink="http://www.w3.org/1999/xlink">
         <volume xmlns:mml="http://www.w3.org/1998/Math/MathML"
                 xmlns:oasis="http://docs.oasis-open.org/ns/oasis-exchange/table">20</volume>
         <issue xmlns:mml="http://www.w3.org/1998/Math/MathML"
                xmlns:oasis="http://docs.oasis-open.org/ns/oasis-exchange/table">1</issue>
         <issue-id>i23110420</issue-id>
         <article-id pub-id-type="jstor">23113379</article-id>
         <article-categories>
            <subj-group>
               <subject>Bayesian Computing</subject>
            </subj-group>
         </article-categories>
         <title-group>
            <article-title>Particle Learning of Gaussian Process Models for Sequential Design and Optimization</article-title>
         </title-group>
         <contrib-group>
            <contrib>
               <string-name>
                  <given-names>Robert B.</given-names>
                  <surname>Gramacy</surname>
               </string-name>
            </contrib>
            <contrib>
               <string-name>
                  <given-names>Nicholas G.</given-names>
                  <surname>Polson</surname>
               </string-name>
            </contrib>
         </contrib-group>
         <pub-date pub-type="ppub">
            <day>1</day>
            <month>3</month>
            <year>2011</year>
         </pub-date>
         <fpage>102</fpage>
         <lpage>118</lpage>
      
      
      
      
      
      
         <permissions>
            <copyright-statement>© 2011 American Statistical Association, the Institute of Mathematical Statistics, and the Interface Foundation of North America</copyright-statement>
         </permissions>
      
         <self-uri xlink:href="https://www.jstor.org/stable/23113379"/>
      
      
         <abstract>
            <p>We develop a simulation-based method for the online updating of Gaussian process regression and classification models. Our method exploits sequential Monte Carlo to produce a fast sequential design algorithm for these models relative to the established MCMC alternative. The latter is less ideal for sequential design since it must be restarted and iterated to convergence with the inclusion of each new design point. We illustrate some attractive ensemble aspects of our SMC approach, and show how active learning heuristics may be implemented via particles to optimize a noisy function or to explore classification boundaries online. Supplemental material, including an R package, is available online.</p>
         </abstract>
         <custom-meta-group>
            <custom-meta>
               <meta-name>lang</meta-name>
               <meta-value>eng</meta-value>
            </custom-meta>
         </custom-meta-group>
      </article-meta>
  </front>
  <back>
    
      <ref-list>
        <title>REFERENCES</title>
        <ref id="d2253675e197a1310">
          
            <mixed-citation id="d2253675e201" publication-type="other">
Abrahamsen, P. ( 1997), "A Review of Gaussian Random Fields and Correlation Functions," Technical Report 917,
Norwegian Computing Center, Box 114 Blindem, N-0314 Oslo, Norway. [103]</mixed-citation>
        </ref>
        <ref id="d2253675e211a1310">
          
            <mixed-citation id="d2253675e215" publication-type="other">
Broderick, T., and Gramacy, R. (2010), "Classification and Categorical Inputs With Treed Gaussian Process
Models," technical report, University of Cambridge. Available at ArXiv:0904.4891. [111–113]</mixed-citation>
        </ref>
        <ref id="d2253675e225a1310">
          
            <mixed-citation id="d2253675e229" publication-type="other">
Carvalho, C., Johannes, M., Lopes, H., and Poison, N. (2010), "Particle Learning and Smoothing," Statistical
Science, 25 (1), 88–106. [105]</mixed-citation>
        </ref>
        <ref id="d2253675e239a1310">
          
            <mixed-citation id="d2253675e243" publication-type="other">
Cohn, D. A. (1996), "Neural Network Exploration Using Optimal Experimental Design," Neural Networks, 9 (6),
1071–1083. [103]</mixed-citation>
        </ref>
        <ref id="d2253675e254a1310">
          
            <mixed-citation id="d2253675e258" publication-type="other">
Escobar, L. Α., and Moser, Ε. Β. (1993), "A Note on the Updating of Regression Estimates," The American
Statistician, 47 (3), 192–194. [107]</mixed-citation>
        </ref>
        <ref id="d2253675e268a1310">
          
            <mixed-citation id="d2253675e272" publication-type="other">
Gilks, W., and Berzuini, C. (2001), "Following a Moving Target: Monte Carlo Inference for Dynamic Bayesian
Models," Journal of the Royal Statistical Society, Ser. Β, 63, 127–146. [107]</mixed-citation>
        </ref>
        <ref id="d2253675e282a1310">
          
            <mixed-citation id="d2253675e286" publication-type="other">
Gramacy, R. B. (2005), "Bayesian Treed Gaussian Process Models," Ph.D. thesis, University of California, Santa
Cruz. [104,108]</mixed-citation>
        </ref>
        <ref id="d2253675e296a1310">
          
            <mixed-citation id="d2253675e300" publication-type="other">
— (2007), "tgp: An R Package for Bayesian Nonstationary, Semiparametric Nonlinear Regression and
Design by Treed Gaussian Process Models," Journal of Statistical Software, 19, 9. [108]</mixed-citation>
        </ref>
        <ref id="d2253675e310a1310">
          
            <mixed-citation id="d2253675e314" publication-type="other">
— (2010), "plgp: Particle Learning of Gaussian Processes," R package version 1.0. [103]</mixed-citation>
        </ref>
        <ref id="d2253675e321a1310">
          
            <mixed-citation id="d2253675e325" publication-type="other">
Gramacy, R. B„ and Lee, H. K. H. (2008), "Bayesian Treed Gaussian Process Models With an Application to
Computer Modeling," Journal of the American Statistical Association, 103, 1119–1130. [108]</mixed-citation>
        </ref>
        <ref id="d2253675e336a1310">
          
            <mixed-citation id="d2253675e340" publication-type="other">
— (2009), "Adaptive Design and Analysis of Supercomputer Experiment," Technometrics, 51 (2), 130-145.
[103]</mixed-citation>
        </ref>
        <ref id="d2253675e350a1310">
          
            <mixed-citation id="d2253675e354" publication-type="other">
Gramacy, R. B., and Taddy, M. A. (2010), "Categorical Inputs, Sensitivity Analysis, Optimization and Importance
Tempering With tgp Version 2, an R Package for Treed Gaussian Process Models," Journal of Statistical
Software, 33 (6). [114]</mixed-citation>
        </ref>
        <ref id="d2253675e367a1310">
          
            <mixed-citation id="d2253675e371" publication-type="other">
Higdon, D. (2002), "Space and Space-Time Modeling Using Process Convolutions," in Quantitative Methods for
Current Environmental Issues, eds. C. Anderson, V. Barnett, P. C. Chatwin, and A. H. El-Shaarawi, London:
Springer-Verlag, pp. 37–56. [108]</mixed-citation>
        </ref>
        <ref id="d2253675e384a1310">
          
            <mixed-citation id="d2253675e388" publication-type="other">
Jones, D., Schonlau, M., and Welch, W. J. (1998), "Efficient Global Optimization of Expensive Black Box Func-
tions," Journal of Global Optimization, 13,455–492. [103,113]</mixed-citation>
        </ref>
        <ref id="d2253675e398a1310">
          
            <mixed-citation id="d2253675e402" publication-type="other">
Joshi, Α., Porikli, F., and Papanikolopoulos, N. (2009), "Multi-Class Active Learning for Image Classification,"
in Computer Vision and Pattern Recognition, IEEE Computer Society Conference on, Los Alamitos, CA:
IEEE Computer Society, pp. 2372–2379. [103,115]</mixed-citation>
        </ref>
        <ref id="d2253675e415a1310">
          
            <mixed-citation id="d2253675e419" publication-type="other">
Knaus, J., Porzelius, C., Binder, H., and Schwarzer, G. (2009), "Easier Parallel Computing in R With snowfall
and sfCluster," The R Journal, 1 (1), 47–53. [117]</mixed-citation>
        </ref>
        <ref id="d2253675e430a1310">
          
            <mixed-citation id="d2253675e434" publication-type="other">
Kong, Α., Liu, J., and Wong, W. (1994), "Sequential Imputations and Bayesian Missing Data Problems," Journal
of the American Statistical Association, 89, 278–288. [105]</mixed-citation>
        </ref>
        <ref id="d2253675e444a1310">
          
            <mixed-citation id="d2253675e448" publication-type="other">
Liu, J., and Chen, R. (1995), "Blind Deconvolution via Sequential Imputations," Journal of the American Statis-
tical Association, 90 (430), 567–576. [105]</mixed-citation>
        </ref>
        <ref id="d2253675e458a1310">
          
            <mixed-citation id="d2253675e462" publication-type="other">
— (1998), "Sequential Monte Carlo Methods for Dynamic Systems," Journal of the American Statistical
Association, 93, 1032–1044. [105]</mixed-citation>
        </ref>
        <ref id="d2253675e472a1310">
          
            <mixed-citation id="d2253675e476" publication-type="other">
MacEachern, S., Clyde, M., and Liu, J. (1999), "Sequential Importance Sampling for Nonparametric Bayes Mod-
els: The Next Generation," Canadian Journal of Statistics, 27, 251–267. [107]</mixed-citation>
        </ref>
        <ref id="d2253675e486a1310">
          
            <mixed-citation id="d2253675e490" publication-type="other">
MacKay, D. J. C. (1992), "Information-Based Objective Functions for Active Data Selection," Neural Computa-
tion, 4 (4), 589–603. [102]</mixed-citation>
        </ref>
        <ref id="d2253675e500a1310">
          
            <mixed-citation id="d2253675e504" publication-type="other">
Müller, P., Sansó, Β., and de Iorio, M. (2004), "Optimal Bayesian Design by Inhomogeneous Markov Chain
Simulation," Journal of the American Statistical Association, 99 (467), 788–798. [102]</mixed-citation>
        </ref>
        <ref id="d2253675e515a1310">
          
            <mixed-citation id="d2253675e519" publication-type="other">
Neal, R. M. (1998), "Regression and Classification Using Gaussian Process Priors" (with discussion), in Bayesian
Statistics, Vol. 6, eds. J. M. Bernardo et al., Oxford, U.K.: Oxford University Press, pp. 476–501. [102,104,
111,112]</mixed-citation>
        </ref>
        <ref id="d2253675e532a1310">
          
            <mixed-citation id="d2253675e536" publication-type="other">
Pitt, M., and Shephard, N. (1999), "Filtering via Simulation: Auxiliary Particle Filters," Journal of the American
Statistical Association, 94, 590–599. [105]</mixed-citation>
        </ref>
        <ref id="d2253675e546a1310">
          
            <mixed-citation id="d2253675e550" publication-type="other">
Rasmussen, C. E., and Williams, C. K. I. (2006), Gaussian Processes for Machine Learning, Cambridge, MA:
The MIT Press. [102]</mixed-citation>
        </ref>
        <ref id="d2253675e560a1310">
          
            <mixed-citation id="d2253675e564" publication-type="other">
Santner, T. J., Williams, B. J., and Notz, W. I. (2003), The Design and Analysis of Computer Experiments, New
York: Springer-Verlag. [102,108,112]</mixed-citation>
        </ref>
        <ref id="d2253675e574a1310">
          
            <mixed-citation id="d2253675e578" publication-type="other">
Seo, S., Wallat, M., Graepel, T., and Obermayer, Κ. (2000), "Gaussian Process Regression: Active Data Selection
and Test Point Rejection," in Proceedings of the International Joint Conference on Neural Networks, Vol. III,
IEEE, pp. 241–246. [103]</mixed-citation>
        </ref>
        <ref id="d2253675e591a1310">
          
            <mixed-citation id="d2253675e595" publication-type="other">
Stein, M. L. (1999), Interpolation of Spatial Data, New York: Springer. [103]</mixed-citation>
        </ref>
        <ref id="d2253675e603a1310">
          
            <mixed-citation id="d2253675e607" publication-type="other">
Taddy, M., Lee, H. K. H., Gray, G. Α., and Griffin, J. D. (2009), "Bayesian Guided Pattern Search for Robust
Local Optimization," Technometrics, 51, 389–101. [103,114]</mixed-citation>
        </ref>
        <ref id="d2253675e617a1310">
          
            <mixed-citation id="d2253675e621" publication-type="other">
Warnes, J., and Ripley, B. (1987), "Problems With Likelihood Estimation of Covariance Functions of Spatial
Gaussian Processes," Biometrika, 74 (3), 640-642. [104]</mixed-citation>
        </ref>
        <ref id="d2253675e631a1310">
          
            <mixed-citation id="d2253675e635" publication-type="other">
Williams, B., Santner, T., and Notz, W. (2000), "Sequential Design of Computer Experiments to Minimize Inte-
grated Response Functions," Statistica Sinica, 10, 1133–1152. [114]</mixed-citation>
        </ref>
      </ref-list>
    
  </back>
</article>


