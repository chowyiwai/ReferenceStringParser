<?xml version="1.0" encoding="UTF-8"?>


<article dtd-version="1.0" article-type="research-article">
  <front>
      <journal-meta>
         <journal-id xmlns:xlink="http://www.w3.org/1999/xlink" journal-id-type="jstor">jcompgrapstat</journal-id>
         <journal-id xmlns:xlink="http://www.w3.org/1999/xlink" journal-id-type="jstor">j100879</journal-id>
         <journal-title-group xmlns:xlink="http://www.w3.org/1999/xlink">
            <journal-title>Journal of Computational and Graphical Statistics</journal-title>
         </journal-title-group>
      
         <publisher>
            <publisher-name>American Statistical Association, Institute of Mathematical Statistics, and Interface Foundation of North America</publisher-name>
         </publisher>
         <issn xmlns:xlink="http://www.w3.org/1999/xlink" pub-type="ppub">10618600</issn>
         <custom-meta-group xmlns:xlink="http://www.w3.org/1999/xlink"/>
      </journal-meta>
      <article-meta xmlns:xlink="http://www.w3.org/1999/xlink">
         <volume xmlns:mml="http://www.w3.org/1998/Math/MathML"
                 xmlns:oasis="http://docs.oasis-open.org/ns/oasis-exchange/table">16</volume>
         <issue xmlns:mml="http://www.w3.org/1998/Math/MathML"
                xmlns:oasis="http://docs.oasis-open.org/ns/oasis-exchange/table">4</issue>
         <issue-id>i27594271</issue-id>
         <article-id pub-id-type="jstor">27594274</article-id>
         <article-id pub-id-type="pub-doi">10.1198/106186007X238846</article-id>
         <title-group>
            <article-title>Continuous Generalized Gradient Descent</article-title>
         </title-group>
         <contrib-group>
            <contrib>
               <string-name>
                  <given-names>Cun-Hui</given-names>
                  <surname>Zhang</surname>
               </string-name>
            </contrib>
         </contrib-group>
         <pub-date pub-type="ppub">
            <day>1</day>
            <month>12</month>
            <year>2007</year>
         </pub-date>
         <fpage>761</fpage>
         <lpage>781</lpage>
      
         <permissions>
            <copyright-statement>Copyright 2007 American Statistical Association, the Institute of Mathematical Statistics, and the Interface Foundation of North America</copyright-statement>
         </permissions>
      
         <self-uri xlink:href="https://www.jstor.org/stable/27594274"/>
      
      
         <abstract>
            <p>This article derives characterizations and computational algorithms for continuous general gradient descent trajectories in high-dimensional parameter spaces for statistical model selection, prediction, and classification. Examples include proportional gradient shrinkage as an extension of LASSO and LARS, threshold gradient descent with right-continuous variable selectors, threshold ridge regression, and many more with proper combinations of variable selectors and functional forms of a kernel. In all these problems, general gradient descent trajectories are continuous piecewise analytic vector-valued curves as solutions to matrix differential equations. We show the monotonicity and convergence of the proposed algorithms in the loss or negative likelihood functions. We prove that approximations of continuous solutions via infinite series expansions are computationally more efficient and accurate compared with discretization methods. We demonstrate the applicability of our algorithms through numerical experiments with real and simulated datasets.</p>
         </abstract>
         <kwd-group>
            <kwd>Classification</kwd>
            <kwd>Matrix differential equation</kwd>
            <kwd>Regression</kwd>
            <kwd>Regularized optimization</kwd>
         </kwd-group>
         <custom-meta-group>
            <custom-meta>
               <meta-name>lang</meta-name>
               <meta-value>eng</meta-value>
            </custom-meta>
         </custom-meta-group>
      </article-meta>
  </front>
  <back>
    
      <ref-list>
        <title>References</title>
        <ref id="d201392e143a1310">
          
            <mixed-citation id="d201392e147" publication-type="other">
Breiman, L. (1996), "Heuristics of Instability and Stabilization in Model Selection," The Annals of Statistics, 24,
2350–2383.</mixed-citation>
        </ref>
        <ref id="d201392e157a1310">
          
            <mixed-citation id="d201392e161" publication-type="other">
Efron, B., Hastie, T., Johnstone, I., and Tibshirani, R. (2004), "Least Angle Regression" (with discussion), The
Annals of Statistics, 32, 407–199.</mixed-citation>
        </ref>
        <ref id="d201392e171a1310">
          
            <mixed-citation id="d201392e175" publication-type="other">
Freund, Y., and Schapire, R.E. (1996), Experiments with a New Boosting Algorithm. Machine Learning: Proceed-
ings of the Thirteenth International Conference, San Francisco: Morgan Kauffmann, pp. 148–156.</mixed-citation>
        </ref>
        <ref id="d201392e185a1310">
          
            <mixed-citation id="d201392e189" publication-type="other">
—(1997), "A Decision-Theoretic Generalization of On-line Learning and an Application to Boosting," Jour-
nal of Computer System Science, 55, 119—139.</mixed-citation>
        </ref>
        <ref id="d201392e200a1310">
          
            <mixed-citation id="d201392e204" publication-type="other">
Friedman, J. (2001), "Greedy Function Approximation: A Gradient Boosting Machine," The Annals of Statistics,
29, 1189–1232.</mixed-citation>
        </ref>
        <ref id="d201392e214a1310">
          
            <mixed-citation id="d201392e218" publication-type="other">
Friedman, J. H., and Popescu, B. E. (2004), "Gradient Directed Regularization for Linear Regression and Classi-
fication," Stanford University, Department of Statistics, Technical Report.</mixed-citation>
        </ref>
        <ref id="d201392e228a1310">
          
            <mixed-citation id="d201392e232" publication-type="other">
Friedman, J., Hastie, T., and Tibshirani, R. (2000), "Additive Logistic Regression: A Statistical View of Boosting,"
(with discussion), The Annals of Statistics, 28, 337–307.</mixed-citation>
        </ref>
        <ref id="d201392e242a1310">
          
            <mixed-citation id="d201392e246" publication-type="other">
Genkin, A., Lewis, D., and Madigan, D. (2007), "Large-Scale Bayesian Logistic Regression for Text Categoriza-
tion," Technometrics, 49, 291–304.</mixed-citation>
        </ref>
        <ref id="d201392e256a1310">
          
            <mixed-citation id="d201392e260" publication-type="other">
Hastie, T., Rosset, S., Tibshirani, R., and Zhu, J. (2004), "The Entire Regularization Path for the Support Vector
Machine," Journal of Machine Learning Research, 5, 1391–1415.</mixed-citation>
        </ref>
        <ref id="d201392e270a1310">
          
            <mixed-citation id="d201392e274" publication-type="other">
Hoerl, A. E., and Kennard, R. (1970), "Ridge Regression: Biased Estimation for Nonorthogonal Problems," Tech-
nometrics, 12, 55–67.</mixed-citation>
        </ref>
        <ref id="d201392e285a1310">
          
            <mixed-citation id="d201392e289" publication-type="other">
Osborne, M., Presnell, B., and Turlach, B. (2000a), "A New Approach to Variable Selection in Least Squares
Problems," IMA Journal of Numerical Analysis, 20, 389–404.</mixed-citation>
        </ref>
        <ref id="d201392e299a1310">
          
            <mixed-citation id="d201392e303" publication-type="other">
–(2000b), "On the Lasso and its Dual," Journal of Computational and Graphical Statistics, 9, 319–337.
Park, M.Y., and Hastie, T. (2007), "LI Regularization Path Algorithm for Generalized Linear Models," Journal
of the Royal statistical Society, Ser. B, 659–677.</mixed-citation>
        </ref>
        <ref id="d201392e316a1310">
          
            <mixed-citation id="d201392e320" publication-type="other">
Rosset, S. (2004), "Tracking Curved Regularized Optimization Solution Paths," NIPS 2004.</mixed-citation>
        </ref>
        <ref id="d201392e327a1310">
          
            <mixed-citation id="d201392e331" publication-type="other">
Rosset, S., and Zhu, J. (2007), "Piecewise Linear Regularized Solution Paths," The Annals of Statistics, 35, 1012–
1030.</mixed-citation>
        </ref>
        <ref id="d201392e341a1310">
          
            <mixed-citation id="d201392e345" publication-type="other">
Schapire, R. E. (1990), "The Strength of Weak Learnability," Machine Learning, 5, 197–227.</mixed-citation>
        </ref>
        <ref id="d201392e352a1310">
          
            <mixed-citation id="d201392e356" publication-type="other">
Tibshirani, R. (1996), "Regression Shrinkage and Selection via the Lasso," Journal of the Royal Statistical Soci-
ety, Series B, 58, 267–288.</mixed-citation>
        </ref>
        <ref id="d201392e367a1310">
          
            <mixed-citation id="d201392e371" publication-type="other">
Vapnik, V. (1996), The Nature of Statistical Learning, New York: Springer-Verlag.</mixed-citation>
        </ref>
        <ref id="d201392e378a1310">
          
            <mixed-citation id="d201392e382" publication-type="other">
Wahba, G., Lin, Y, and Zhang, H. (2000), "Gacv for Support Vector Machines," in Advances in Large Margin
Classifiers, eds. A.J. Smola, PL. Bartlett, B. Schölkopf, and D. Schuurmans, Cambridge, MA: MIT Press,
pp. 297–311.</mixed-citation>
        </ref>
        <ref id="d201392e395a1310">
          
            <mixed-citation id="d201392e399" publication-type="other">
Zhao, P., and Yu, B. (2004), "Boosted Lasso," Tech Report, Statistics Department, U. C. Berkeley.</mixed-citation>
        </ref>
      </ref-list>
    
  </back>
</article>


