<?xml version="1.0" encoding="UTF-8"?>


<article dtd-version="1.0" article-type="research-article">
  <front>
      <journal-meta>
         <journal-id xmlns:xlink="http://www.w3.org/1999/xlink" journal-id-type="jstor">machtran</journal-id>
         <journal-id xmlns:xlink="http://www.w3.org/1999/xlink" journal-id-type="jstor">j50000036</journal-id>
         <journal-title-group xmlns:xlink="http://www.w3.org/1999/xlink">
            <journal-title>Machine Translation</journal-title>
         </journal-title-group>
      
         <publisher>
            <publisher-name>Springer</publisher-name>
         </publisher>
         <issn xmlns:xlink="http://www.w3.org/1999/xlink" pub-type="ppub">09226567</issn>
         <issn xmlns:xlink="http://www.w3.org/1999/xlink" pub-type="epub">15730573</issn>
         <custom-meta-group xmlns:xlink="http://www.w3.org/1999/xlink"/>
      </journal-meta>
      <article-meta xmlns:xlink="http://www.w3.org/1999/xlink">
         <volume xmlns:mml="http://www.w3.org/1998/Math/MathML"
                 xmlns:oasis="http://docs.oasis-open.org/ns/oasis-exchange/table">31</volume>
         <issue xmlns:mml="http://www.w3.org/1998/Math/MathML"
                xmlns:oasis="http://docs.oasis-open.org/ns/oasis-exchange/table">1/2</issue>
         <issue-id>i40211882</issue-id>
         <article-id pub-id-type="jstor">44987836</article-id>
         <title-group>
            <article-title>A comparison of discriminative training criteria for continuous space translation models</article-title>
         </title-group>
         <contrib-group>
            <contrib>
               <string-name>
                  <given-names>Alexandre</given-names>
                  <surname>Allauzen</surname>
               </string-name>
            </contrib>
            <contrib>
               <string-name>
                  <given-names>Quoc Khanh</given-names>
                  <surname>Do</surname>
               </string-name>
            </contrib>
            <contrib>
               <string-name>
                  <given-names>François</given-names>
                  <surname>Yvon</surname>
               </string-name>
            </contrib>
         </contrib-group>
         <pub-date pub-type="ppub">
            <day>1</day>
            <month>6</month>
            <year>2017</year>
         </pub-date>
         <fpage>19</fpage>
         <lpage>33</lpage>
      
      
      
      
      
      
         <permissions>
            <copyright-statement>© Springer Science+Business Media 2017</copyright-statement>
         </permissions>
         <self-uri xlink:href="https://www.jstor.org/stable/44987836"/>
      
      
         <abstract>
            <p>This paper explores a new discriminative training procedure for continuous-space translation models (CTMs) which correlates better with translation quality than conventional training methods. The core of the method lays in the definition of a novel objective function which enables us to effectively integrate the CTM with the rest of the translation system through N-best rescoring. Using a fixed architecture, where we iteratively retrain the CTM parameters and the log-linear coefficients, we compare various ways to define and combine training criteria for each of these steps, drawing inspirations both from max-margin and learning-to-rank techniques. We experimentally show that a recently introduced loss function, which combines these two techniques, outperforms several objective functions from the literature. We also show that ensuring the consistency of the losses used to train these two sets of parameters is beneficial to the overall performance.</p>
         </abstract>
         <custom-meta-group>
            <custom-meta>
               <meta-name>lang</meta-name>
               <meta-value>eng</meta-value>
            </custom-meta>
         </custom-meta-group>
      </article-meta>
  </front>
  <back>
    
      <fn-group xmlns:xlink="http://www.w3.org/1999/xlink">
        <title>[Footnotes]</title>
        <fn id="d3447e193a1310">
            <label>3</label>
          
            <p>
               <mixed-citation id="d3447e200" publication-type="other">
Crego et al. (2011).</mixed-citation>
            </p>
        </fn>
        <fn id="d3447e207a1310">
            <label>4</label>
          
            <p>
               <mixed-citation id="d3447e214" publication-type="other">
http://www.statmt.org/moses/.</mixed-citation>
            </p>
            <p>
               <mixed-citation id="d3447e220" publication-type="other">
Cherry and Foster
2012</mixed-citation>
            </p>
        </fn>
        <fn id="d3447e230a1310">
            <label>6</label>
          
            <p>
               <mixed-citation id="d3447e237" publication-type="other">
Rosti et al. (2010)</mixed-citation>
            </p>
        </fn>
        <fn id="d3447e244a1310">
            <label>7</label>
          
            <p>
               <mixed-citation id="d3447e251" publication-type="other">
http://workshop2014.iwslt.org/.</mixed-citation>
            </p>
        </fn>
        <fn id="d3447e259a1310">
            <label>8</label>
          
            <p>
               <mixed-citation id="d3447e266" publication-type="other">
http://ncode.limsi.fr/.</mixed-citation>
            </p>
        </fn>
      </fn-group>
    
    
      <ref-list>
        <title>References</title>
        <ref id="d3447e282a1310">
          
            <mixed-citation id="d3447e286" publication-type="other">
Allauzen A, Pécheux N, Do QK, Dinareiii M, Lavergne T, Max A, Le H, Yvon F (2013) LIMSI @ WMT13.
In: Proceedings of the workshop on statistical machine translation, Sofìa, Bulgaria, pp 62-69</mixed-citation>
        </ref>
        <ref id="d3447e296a1310">
          
            <mixed-citation id="d3447e300" publication-type="other">
Auli M, Gao J (2014a) Decoder integration and expected BLEU training for recurrent neural network lan-
guage models. In: Proceedings of the annual meeting of the Association for Computational Linguistics
(ACL), pp 136-142</mixed-citation>
        </ref>
        <ref id="d3447e313a1310">
          
            <mixed-citation id="d3447e317" publication-type="other">
Auli M, Gao J (2014b) Decoder integration and expected BLEU training for recurrent neural network
language models. In: Proceedings of the 52nd annual meeting of the Association for Computational
Linguistics (ACL' 14), pp 136-142</mixed-citation>
        </ref>
        <ref id="d3447e330a1310">
          
            <mixed-citation id="d3447e334" publication-type="other">
Auli M, Galley M, Quirk C, Zweig G (2013) Joint language and translation modeling with recurrent neural
networks. In: Proceedings of the conference on empirical methods in natural language processing
(EMNLP), pp 1044-1054</mixed-citation>
        </ref>
        <ref id="d3447e348a1310">
          
            <mixed-citation id="d3447e352" publication-type="other">
Bahdanau D, Cho K, Bengio Y (2014) Neural machine translation by jointly learning to align and translate.
arXiv preprint arXiv: 1409.0473</mixed-citation>
        </ref>
        <ref id="d3447e362a1310">
          
            <mixed-citation id="d3447e366" publication-type="other">
Bengio Y, Ducharme R, Vincent P, Janvin C (2003) A neural probabilistic language model. J Mach Learn
Res 3:1137-1155</mixed-citation>
        </ref>
        <ref id="d3447e376a1310">
          
            <mixed-citation id="d3447e380" publication-type="other">
Blunsom P, Osborne M (2008) Probabilistic inference for machine translation. In: Proceedings of the
conference on empirical methods in natural language processing, pp 215-223</mixed-citation>
        </ref>
        <ref id="d3447e390a1310">
          
            <mixed-citation id="d3447e394" publication-type="other">
Blunsom P, Cohn T, Osborne M (2008) A discriminative latent variable model for statistical machine
translation. In: ACL, pp 200-208</mixed-citation>
        </ref>
        <ref id="d3447e404a1310">
          
            <mixed-citation id="d3447e408" publication-type="other">
Casacuberta F, Vidal E (2004) Machine translation with inferred stochastic finite-state transducers. Comput
Linguist 30(3):205-225</mixed-citation>
        </ref>
        <ref id="d3447e418a1310">
          
            <mixed-citation id="d3447e422" publication-type="other">
Cherry C, Foster G (2012) Batch tuning strategies for statistical machine translation. In: Proceedings
of the North American chapter of the Association for Computational Linguistics: human language
technologies (NAACL-HLT), pp 427-436</mixed-citation>
        </ref>
        <ref id="d3447e436a1310">
          
            <mixed-citation id="d3447e440" publication-type="other">
Chiang D, Marton Y, Resnik P (2008) Online large-margin training of syntactic and structural translation
features. In: Proceedings of the conference on empirical methods in natural language processing, pp
224-233</mixed-citation>
        </ref>
        <ref id="d3447e453a1310">
          
            <mixed-citation id="d3447e457" publication-type="other">
Cho K, van Merrienboer B, Gulcehre C, Bahdanau D, Bougares F, Schwenk H, Bengio Y (2014) Learning
phrase representations using RNN encoder-decoder for statistical machine translation. In: Proceedings
of the 2014 conference on empirical methods in natural language processing (EMNLP), Doha, Qatar,
pp 1724-1734</mixed-citation>
        </ref>
        <ref id="d3447e473a1310">
          
            <mixed-citation id="d3447e477" publication-type="other">
Collins M (2002) Discriminative training methods for hidden Markov models: theory and experiments with
perceptron algorithms. In: Proceedings of the conference on empirical methods in natural language
processing (EMNLP), pp 1-8</mixed-citation>
        </ref>
        <ref id="d3447e490a1310">
          
            <mixed-citation id="d3447e494" publication-type="other">
Collobert R, Weston J (2008) A unified architecture for natural language processing: deep neural networks
with multitask learning. In: Proceedings of the 25th international conference on machine learning.
ACM, New York, pp 160-167</mixed-citation>
        </ref>
        <ref id="d3447e507a1310">
          
            <mixed-citation id="d3447e511" publication-type="other">
Collobert R, Weston J, Bottou L, Karlen M, Kavukcuoglu K, Kuksa P (201 1) Natural language processing
(almost) from scratch. J Mach Learn Res 12:2493-2537</mixed-citation>
        </ref>
        <ref id="d3447e521a1310">
          
            <mixed-citation id="d3447e525" publication-type="other">
Crammer K, Singer Y (2003) Ultraconservative online algorithms for multiclass problems. J Mach Learn
Res 3:951-991</mixed-citation>
        </ref>
        <ref id="d3447e536a1310">
          
            <mixed-citation id="d3447e540" publication-type="other">
Crego JM, Marino JB (2006) Improving statistical MT by coupling reordering and decoding. Mach Transi
20(3): 199-215</mixed-citation>
        </ref>
        <ref id="d3447e550a1310">
          
            <mixed-citation id="d3447e554" publication-type="other">
Crego JM, Yvon F, Mariño JB (201 1) N-code: an open-source bilingual N-gram SMT toolkit. Prague Bull
Math Linguist 96:49-58</mixed-citation>
        </ref>
        <ref id="d3447e564a1310">
          
            <mixed-citation id="d3447e568" publication-type="other">
Devlin J, Zbib R, Huang Z, Lamar T, Schwartz R, Makhoul J (2014) Fast and robust neural network
joint models for statistical machine translation. In: Proceedings of the 52nd annual meeting of the
Association for Computational Linguistics. Long papers, vol 1, Baltimore, MD, pp 1370-1380</mixed-citation>
        </ref>
        <ref id="d3447e581a1310">
          
            <mixed-citation id="d3447e585" publication-type="other">
Do QK (2016) Discriminative training of continuous space translation models. PhD Thesis, Université
Paris-Sud and Université Paris-Saclay</mixed-citation>
        </ref>
        <ref id="d3447e595a1310">
          
            <mixed-citation id="d3447e599" publication-type="other">
Do Q-K, Allauzen A, Yvon F (2014) Discriminative adaptation of continuous space translation models. In:
International workshop on spoken language translation (IWSLT 2014), Lake Tahoe, USA</mixed-citation>
        </ref>
        <ref id="d3447e609a1310">
          
            <mixed-citation id="d3447e613" publication-type="other">
Do Q-K, Allauzen A, Yvon F (2015a) Apprentissage discriminant des modèles continus de traduction. In:
Actes de la 22e conférence sur le Traitement Automatique des Langues Naturelles, Caen, France.
Association pour le Traitement Automatique des Langues, pp 267-278</mixed-citation>
        </ref>
        <ref id="d3447e627a1310">
          
            <mixed-citation id="d3447e631" publication-type="other">
Do QK, Allauzen A, Yvon F (2015b) A discriminative training procedure for continuous translation models.
In: Conference on empirical methods in natural language processing (EMNLP 201 5), Lisboa, Portugal,
P7</mixed-citation>
        </ref>
        <ref id="d3447e644a1310">
          
            <mixed-citation id="d3447e648" publication-type="other">
Dyer C, Resnik P (2010) Context-free reordering, finite-state translation. In: Proceedings of the North
American chapter of the Association for Computational Linguistics: human language technologies
(NAACL-HLT), pp 858-866</mixed-citation>
        </ref>
        <ref id="d3447e661a1310">
          
            <mixed-citation id="d3447e665" publication-type="other">
Freund Y, Schapire RE (1999) Large margin classification using the perceptron algorithm. Mach Learn
37(3):277-296</mixed-citation>
        </ref>
        <ref id="d3447e675a1310">
          
            <mixed-citation id="d3447e679" publication-type="other">
Gao J, He X (2013) Training MRF-based phrase translation models using gradient ascent. In: Proceedings
of the North American chapter of the Association for Computational Linguistics: human language
technologies (NAACL-HLT), Atlanta, pp 450-459</mixed-citation>
        </ref>
        <ref id="d3447e692a1310">
          
            <mixed-citation id="d3447e696" publication-type="other">
Gao J, He X, Yih W-t, Deng L (2014) Learning continuous phrase representations for translation model-
ing. In: Proceedings of the 52nd annual meeting of the Association for Computational Linguistics,
Baltimore, MD</mixed-citation>
        </ref>
        <ref id="d3447e709a1310">
          
            <mixed-citation id="d3447e713" publication-type="other">
Gutmann M, Hyvärinen A (2010) Noise-contrastive estimation: a new estimation principle for unnormalized
statistical models. In: Teh YW, Titterington M (eds) Proceedings of the international conference on
artificial intelligence and statistics (AISTATS), vol 9, pp 297-304</mixed-citation>
        </ref>
        <ref id="d3447e727a1310">
          
            <mixed-citation id="d3447e731" publication-type="other">
He X, Deng L (2012) Maximum expected BLEU training of phrase and lexicon translation models. In:
Proceedings of the 50th annual meeting of the Association for Computational Linguistics: long papers,
vol 1, pp 292-301</mixed-citation>
        </ref>
        <ref id="d3447e744a1310">
          
            <mixed-citation id="d3447e748" publication-type="other">
Hopkins M, May J (2011) Tuning as ranking. In: Proceedings of the 2011 conference on empirical methods
in natural language processing, Edinburgh, Scotland, UK, pp 1352-1362</mixed-citation>
        </ref>
        <ref id="d3447e758a1310">
          
            <mixed-citation id="d3447e762" publication-type="other">
Lavergne T, Crego JM, Allauzen A, Yvon F (2011) From n-gram-based to CRF-based translation models.
In: Proceedings of the sixth workshop on statistical machine translation, pp 542-553</mixed-citation>
        </ref>
        <ref id="d3447e772a1310">
          
            <mixed-citation id="d3447e776" publication-type="other">
Lavergne T, Allauzen A, Yvon F (2013) Un cadre d'apprentissage intégralement discriminant pour la
traduction statistique. TALN-RÉCITAL 2013, p 450</mixed-citation>
        </ref>
        <ref id="d3447e786a1310">
          
            <mixed-citation id="d3447e790" publication-type="other">
Le H-S, Oparin I, Allauzen A, Gauvain J-L, Yvon F (201 1) Structured output layer neural network language
model. In: Proceedings of the international conference on audio, speech and signal processing, pp
5524-5527</mixed-citation>
        </ref>
        <ref id="d3447e803a1310">
          
            <mixed-citation id="d3447e807" publication-type="other">
Le H-S, Allauzen A, Yvon F (2012) Continuous space translation models with neural networks. In: Pro-
ceedings of the North American chapter of the Association for Computational Linguistics: human
language technologies (NAACL-HLT), Montréal, Canada, pp 39-48</mixed-citation>
        </ref>
        <ref id="d3447e821a1310">
          
            <mixed-citation id="d3447e825" publication-type="other">
Liang P, Bouchard-Côté A, Klein D, Taškař B (2006) An end-to-end discriminative approach to machine
translation. In: Proceedings of the annual meeting of the Association for Computational Linguistics
(ACL), pp 761-768</mixed-citation>
        </ref>
        <ref id="d3447e838a1310">
          
            <mixed-citation id="d3447e842" publication-type="other">
Mariño JB, Banchs RE, Crego JM, de Gispert A, Lambert P, Fonollosa JA, Costa-Jussà MR (2006) N-gram-
based machine translation. Comput Linguist 32(4):527-549</mixed-citation>
        </ref>
        <ref id="d3447e852a1310">
          
            <mixed-citation id="d3447e856" publication-type="other">
McDonald R, Crammer K, Pereira F (2005) Online large-margin training of dependency parsers. In: Pro-
ceedings of the annual meeting of the Association for Computational Linguistics (ACL), pp 91-98</mixed-citation>
        </ref>
        <ref id="d3447e866a1310">
          
            <mixed-citation id="d3447e870" publication-type="other">
Mnih A, Hinton GE (2008) A scalable hierarchical distributed language model. In: Koller D, Schuurmans D,
Bengio Y, Bottou L (eds) Advances in neural information processing Systems 21, vol 21, pp 1081-1088</mixed-citation>
        </ref>
        <ref id="d3447e880a1310">
          
            <mixed-citation id="d3447e884" publication-type="other">
Mnih A, Teh YW (2012) A fast and simple algorithm for training neural probabilistic language models. In:
Proceedings of the international conference of machine learning (ICML)</mixed-citation>
        </ref>
        <ref id="d3447e894a1310">
          
            <mixed-citation id="d3447e898" publication-type="other">
Neubig G, Watanabe T (2016) Optimization for statistical machine translation: a survey. Comput Linguist
42(1): 1-54</mixed-citation>
        </ref>
        <ref id="d3447e909a1310">
          
            <mixed-citation id="d3447e913" publication-type="other">
Niehues J, Waibel A (2012) Continuous space language models using restricted Boltzmann machines. In:
Proceedings of international workshop on spoken language translation (IWSLT), Hong-Kong, China,
pp 164-170</mixed-citation>
        </ref>
        <ref id="d3447e926a1310">
          
            <mixed-citation id="d3447e930" publication-type="other">
Och FJ (2003) Minimum error rate training in statistical machine translation. In: Proceedings of the annual
meeting of the Association for Computational Linguistics (ACL). Association for Computational
Linguistics, pp 160-167</mixed-citation>
        </ref>
        <ref id="d3447e943a1310">
          
            <mixed-citation id="d3447e947" publication-type="other">
Papineni K, Roukos S, Ward T, Zhu W-J (2002) BLEU: a method for automatic evaluation of machine
translation. In: Proceedings of the annual meeting of the Association for Computational Linguistics
(ACL), pp 311-318</mixed-citation>
        </ref>
        <ref id="d3447e960a1310">
          
            <mixed-citation id="d3447e964" publication-type="other">
Rosti A-V, Zhang B, Matsoukas S, Schwartz R (2010) BBN system description for WMT10 system
combination task. In: Proceedings of the joint fifth workshop on statistical machine translation and
MetricsMATR, Uppsala, Sweden. Association for Computational Linguistics, pp 321-326</mixed-citation>
        </ref>
        <ref id="d3447e977a1310">
          
            <mixed-citation id="d3447e981" publication-type="other">
Schwenk H (2007) Continuous space language models. Comput Speech Lang 21(3):492-518</mixed-citation>
        </ref>
        <ref id="d3447e988a1310">
          
            <mixed-citation id="d3447e992" publication-type="other">
Schwenk H, Costa-Jussa MR, Fonollosa JAR (2007) Smooth bilingual w-gram translation. In: Proceedings
of the conference on empirical methods in natural language processing (EMNLP), Prague, Czech
Republic, pp 430-438</mixed-citation>
        </ref>
        <ref id="d3447e1006a1310">
          
            <mixed-citation id="d3447e1010" publication-type="other">
Shen L, Joshi AK (2005) Ranking and reranking with perceptron. Mach Learn 60(l-3):73-96</mixed-citation>
        </ref>
        <ref id="d3447e1017a1310">
          
            <mixed-citation id="d3447e1021" publication-type="other">
Shen L, Sarkar A, Och FJ (2004) Discriminative reranking for machine translation. In: HLT-NAACL, pp
177-184</mixed-citation>
        </ref>
        <ref id="d3447e1031a1310">
          
            <mixed-citation id="d3447e1035" publication-type="other">
Shen S, Cheng Y, He Z, He W, Wu H, Sun M, Liu Y (2015) Minimum risk training for neural machine
translation. CoRR. arXiv : 1512.02433</mixed-citation>
        </ref>
        <ref id="d3447e1045a1310">
          
            <mixed-citation id="d3447e1049" publication-type="other">
Simianer P, Riezler S, Dyer C (2012) Joint feature selection in distributed stochastic learning for large-
scale discriminative training in SMT. In: Proceedings of the annual meeting of the Association for
Computational Linguistics (ACL), pp 11-21</mixed-citation>
        </ref>
        <ref id="d3447e1062a1310">
          
            <mixed-citation id="d3447e1066" publication-type="other">
Socher R, Bauer J, Manning CD, Andrew YN (2013) Parsing with compositional vector grammars. In:
Proceedings of the annual meeting of the Association for Computational Linguistics (ACL), Sofia,
Bulgaria, pp 455-465</mixed-citation>
        </ref>
        <ref id="d3447e1079a1310">
          
            <mixed-citation id="d3447e1083" publication-type="other">
Sutskever I, Vinyals O, Le QV (2014) Sequence to sequence learning with neural networks. In: Advances
in neural information processing systems, NIPS*27, Montréal, Canada, pp 3104-31 12</mixed-citation>
        </ref>
        <ref id="d3447e1094a1310">
          
            <mixed-citation id="d3447e1098" publication-type="other">
Vaswani A, Zhao Y, Fossum V, Chiang D (201 3) Decoding with large-scale neural language models improves
translation. In: Proceedings of the conference on empirical methods in natural language Processing
(EMNLP), Seattle, Washington, USA, pp 1387-1392</mixed-citation>
        </ref>
        <ref id="d3447e1111a1310">
          
            <mixed-citation id="d3447e1115" publication-type="other">
Watanabe T, Suzuki J, Tsukada H, Isozaki H (2007) Online large-margin training for statistical machine
translation. In: Proceedings of the 2007 joint conference on empirical methods in natural language
processing and computational natural language learning (EMNLP-CoNLL), Prague, Czech Republic,
pp 764-773</mixed-citation>
        </ref>
        <ref id="d3447e1131a1310">
          
            <mixed-citation id="d3447e1135" publication-type="other">
Yang N, Liu S, Li M, Zhou M, Yu N (2013) Word alignment modeling with context dependent deep neural
networks. In: Proceedings of the annual meeting of the Association for Computational Linguistics
(ACL), Sofia, Bulgaria, pp 166-175</mixed-citation>
        </ref>
        <ref id="d3447e1148a1310">
          
            <mixed-citation id="d3447e1152" publication-type="other">
Zens R, Och FJ, Ney H (2002) Phrase-based statistical machine translation. In: KI 02: proceedings of the
25th annual German conference on AI. Springer, London, pp 18-32</mixed-citation>
        </ref>
        <ref id="d3447e1162a1310">
          
            <mixed-citation id="d3447e1166" publication-type="other">
Zens R, Hasan S, Ney H (2007) A systematic comparison of training criteria for statistical machine transla-
tion. In: Proceedings of the 2007 joint conference on empirical methods in natural language processing
and computational natural language learning (EMNLP-CoNLL), Prague, Czech Republic, pp 524-532</mixed-citation>
        </ref>
      </ref-list>
    
  </back>
</article>


