<?xml version="1.0" encoding="UTF-8"?>


<article dtd-version="1.0" article-type="research-article">
  <front>
      <journal-meta>
         <journal-id xmlns:xlink="http://www.w3.org/1999/xlink" journal-id-type="jstor">jcompgrapstat</journal-id>
         <journal-id xmlns:xlink="http://www.w3.org/1999/xlink" journal-id-type="jstor">j100879</journal-id>
         <journal-title-group xmlns:xlink="http://www.w3.org/1999/xlink">
            <journal-title>Journal of Computational and Graphical Statistics</journal-title>
         </journal-title-group>
      
         <publisher>
            <publisher-name>JCGS Management Committee of the American Statistical Association, Institute of Mathematical Statistics, and Interface Foundation of North America</publisher-name>
         </publisher>
         <issn xmlns:xlink="http://www.w3.org/1999/xlink" pub-type="ppub">10618600</issn>
         <custom-meta-group xmlns:xlink="http://www.w3.org/1999/xlink"/>
      </journal-meta>
      <article-meta xmlns:xlink="http://www.w3.org/1999/xlink">
         <volume xmlns:mml="http://www.w3.org/1998/Math/MathML"
                 xmlns:oasis="http://docs.oasis-open.org/ns/oasis-exchange/table">20</volume>
         <issue xmlns:mml="http://www.w3.org/1998/Math/MathML"
                xmlns:oasis="http://docs.oasis-open.org/ns/oasis-exchange/table">4</issue>
         <issue-id>i23241032</issue-id>
         <article-id pub-id-type="jstor">23248944</article-id>
         <article-categories>
            <subj-group>
               <subject>Mining High-Dimensional Data</subject>
            </subj-group>
         </article-categories>
         <title-group>
            <article-title>A Homotopy Algorithm for the Quantile Regression Lasso and Related Piecewise Linear Problems</article-title>
         </title-group>
         <contrib-group>
            <contrib>
               <string-name>
                  <given-names>M. R.</given-names>
                  <surname>Osborne</surname>
               </string-name>
            </contrib>
            <contrib>
               <string-name>
                  <given-names>B. A.</given-names>
                  <surname>Turlach</surname>
               </string-name>
            </contrib>
         </contrib-group>
         <pub-date pub-type="ppub">
            <day>1</day>
            <month>12</month>
            <year>2011</year>
         </pub-date>
         <fpage>972</fpage>
         <lpage>987</lpage>
      
      
      
      
         <permissions>
            <copyright-statement>© 2011 American Statistical Association, the Institute of Mathematical Statistics, and the Interface Foundation of North America</copyright-statement>
         </permissions>
      
         <self-uri xlink:href="https://www.jstor.org/stable/23248944"/>
      
      
         <abstract>
            <p>We show that the homotopy algorithm of Osborne, Presnell, and Turlach (2000), which has proved such an effective optimal path following method for implementing Tibshirani's "lasso" for variable selection in least squares estimation problems, can be extended to polyhedral objectives in examples such as the quantile regression lasso. The new algorithm introduces the novel feature that it requires two homotopy sequences involving continuation steps with respect to both the constraint bound and the Lagrange multiplier to be performed consecutively. Performance is illustrated by application to several standard datasets, and these results are compared to calculations made with the original lasso homotopy program. This permits an assessment of the computational complexity to be made both for the new method and for the closely related linear programming post-optimality procedures as these generate essentially identical solution trajectories. This comparison strongly favors the least squares selection method. However, the new method still provides an effective computational procedure, plus it has distinct implementation advantages over the linear programming approaches to the polyhedral objective problem. The computational difficulty is explained and the problem that needs to be resolved in order to improve performance identified. An online supplement to the article contains proofs and R code to implement the algorithm.</p>
         </abstract>
         <custom-meta-group>
            <custom-meta>
               <meta-name>lang</meta-name>
               <meta-value>eng</meta-value>
            </custom-meta>
         </custom-meta-group>
      </article-meta>
  </front>
  <back>
    
      <ref-list>
        <title>REFERENCES</title>
        <ref id="d980179e183a1310">
          
            <mixed-citation id="d980179e187" publication-type="other">
Bondell, H. D., and Reich, B. J. (2007), "Simultaneous Regression Shrinkage, Variable Selection, and Supervising
Clustering of Predictors With OSCAR," Biometrics, 64, 115-123. [973]</mixed-citation>
        </ref>
        <ref id="d980179e197a1310">
          
            <mixed-citation id="d980179e201" publication-type="other">
Candes, E., and Tao, T. (2007), "The Dantzig Selector: Statistical Estimation When p Is Much Larger Than n,"
The Annals of Statistics, 35 (6), 2313-2351. [974]</mixed-citation>
        </ref>
        <ref id="d980179e211a1310">
          
            <mixed-citation id="d980179e215" publication-type="other">

              Donoho, D. L., and Tsaig, Y. (2008), "Fast Solution of l
              
              -Norm Minimization Problems When the Solution May
            
Be Sparse," IEEE Transactions on Information Theory, 54, 4789-4812. [973]</mixed-citation>
        </ref>
        <ref id="d980179e228a1310">
          
            <mixed-citation id="d980179e232" publication-type="other">
Draper, N. H., and Smith, H. (1998), Applied Regression Analysis (3rd ed.), New York: Wiley. [982]</mixed-citation>
        </ref>
        <ref id="d980179e240a1310">
          
            <mixed-citation id="d980179e244" publication-type="other">
Hastie, T. J., Rosset, S., Tibshirani, R. J., and Zhu, J. (2004), "The Entire Regularization Path for the Support
Vector Machine," Journal of Machine Learning Research, 5, 1391-1415. [986]</mixed-citation>
        </ref>
        <ref id="d980179e254a1310">
          
            <mixed-citation id="d980179e258" publication-type="other">
Hettsmansperger, T. P., and McKean, J. W. (1998), Robust Nonparametric Statistical Methods, Chichester: Wiley.
[973]</mixed-citation>
        </ref>
        <ref id="d980179e268a1310">
          
            <mixed-citation id="d980179e272" publication-type="other">

              Kato, K. (2010), "Solving l
              
              Regularization Problems With Piecewise Linear Losses," Journal of Computational
            
and Graphical Statistics, 19, 1024-1040. [975]</mixed-citation>
        </ref>
        <ref id="d980179e285a1310">
          
            <mixed-citation id="d980179e289" publication-type="other">
Koenker, R. (2005), Quantile Regression, New York: Cambridge University Press. [973]</mixed-citation>
        </ref>
        <ref id="d980179e296a1310">
          
            <mixed-citation id="d980179e300" publication-type="other">

              Li, Y., and Zhu, J. (2008), "L
              
              -Norm Quantile Regression," Journal of Computational and Graphical Statistics,
            
17, 163-185. [973,975,979]</mixed-citation>
        </ref>
        <ref id="d980179e313a1310">
          
            <mixed-citation id="d980179e317" publication-type="other">
Ong, C.-J., Shao, S., and Yang, J. (2010), "An Improved Algorithm for the Solution of the Regularization Path of
Support Vector Machine," IEEE Transactions on Neural Networks, 21 (3), 451-462. [986]</mixed-citation>
        </ref>
        <ref id="d980179e328a1310">
          
            <mixed-citation id="d980179e332" publication-type="other">
Osborne, M. R. (2001), Simplicial Algorithms for Minimizing Polyhedral Functions, New York: Cambridge Uni-
versity Press. [974]</mixed-citation>
        </ref>
        <ref id="d980179e342a1310">
          
            <mixed-citation id="d980179e346" publication-type="other">
Osborne, M. R., Presnell, B., and Turlach, B. A. (1998), "Knot Selection for Regression Splines via the LASSO,"
in Dimension Reduction, Computational Complexity, and Information, Proceedings of the 30th Symposium
on the Interface, Interface 98, ed. S. Weisburg, Fairfax Station, VA: Interface Foundation of North America,
pp. 44-49. [975]</mixed-citation>
        </ref>
        <ref id="d980179e362a1310">
          
            <mixed-citation id="d980179e366" publication-type="other">
— (2000), "A New Approach to Variable Selection in Least Squares Problems," IMA Journal of Numerical
Analysis, 20, 389^103. [972,973,976,981]</mixed-citation>
        </ref>
        <ref id="d980179e376a1310">
          
            <mixed-citation id="d980179e380" publication-type="other">
Rosset, S., and Zhu, J. (2007), "Piecewise Linear Regularised Solution Paths," The Annals of Statistics, 35 (3),
1012-1030. [973,974,976]</mixed-citation>
        </ref>
        <ref id="d980179e390a1310">
          
            <mixed-citation id="d980179e394" publication-type="other">
Tibshirani, R. (1996), "Regression Shrinkage and Selection via the Lasso," Journal of the Royal Statistical Soci-
ety, Ser. B, 58 (1), 267-288. [973]</mixed-citation>
        </ref>
        <ref id="d980179e404a1310">
          
            <mixed-citation id="d980179e408" publication-type="other">
Turlach, B. A., Venables, W. N., and Wright, S. J. (2005), "Simultaneous Variable Selection," Technometrics,
47 (3), 349-363. [973,974]</mixed-citation>
        </ref>
        <ref id="d980179e419a1310">
          
            <mixed-citation id="d980179e423" publication-type="other">
Yao, Y., and Lee, Y. (2007), "Another Look at Linear Programming for Feature Selection via Methods of Regu-
larization," Technical Report 800, Department of Statistics, Ohio State University. [973]</mixed-citation>
        </ref>
        <ref id="d980179e433a1310">
          
            <mixed-citation id="d980179e437" publication-type="other">

              Yuan, M„ and Zou, H. (2009), "Efficient Global Approximation of Generalised Nonlinear i
              
              Regularised Solution
            
Paths and Its Applications," Journal of the American Statistical Association, 104 (488), 1562-1573. [973,
974]</mixed-citation>
        </ref>
        <ref id="d980179e453a1310">
          
            <mixed-citation id="d980179e457" publication-type="other">

              Zhu, J„ Hastie, T„ Rosset, S., and Tibshirani, R. (2004),"
              
              -Norm Support Vector Machines," Advances in Neural
            
Information Processing Systems, 16, 49-56. [973]</mixed-citation>
        </ref>
        <ref id="d980179e470a1310">
          
            <mixed-citation id="d980179e474" publication-type="other">
Zou, H., and Hastie, T. (2005), "Regularization and Variable Selection via the Elastic Net," Journal of the Royal
Statististical Society, Ser. B, 67, 301-320. [973]</mixed-citation>
        </ref>
        <ref id="d980179e484a1310">
          
            <mixed-citation id="d980179e488" publication-type="other">
Zou, H„ and Yuan, M. (2008), "Regularised Simultaneous Model Selection in Multiple Quantiles Regression,"
Computational Statistics &amp; Data Analysis, 52, 5296-5304. [973]</mixed-citation>
        </ref>
      </ref-list>
    
  </back>
</article>


