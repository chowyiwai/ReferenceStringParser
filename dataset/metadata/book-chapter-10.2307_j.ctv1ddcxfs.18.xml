

<book xmlns:mml="http://www.w3.org/1998/Math/MathML"
      xmlns:oasis="http://docs.oasis-open.org/ns/oasis-exchange/table"
      xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
      xmlns:xlink="http://www.w3.org/1999/xlink"
      dtd-version="0.2"
      xml:lang="eng">
   <collection-meta>
      <collection-id collection-id-type="jstor">books</collection-id>
   </collection-meta>
   <book-meta>
      <book-id book-id-type="doi">10.2307/j.ctv1ddcxfs</book-id>
      <subj-group subj-group-type="discipline">
         <subject>Computer Science</subject>
      </subj-group>
      <book-title-group>
         <book-title>Scientific Parallel Computing</book-title>
      </book-title-group>
      <contrib-group>
         <contrib contrib-type="author" id="contrib1">
            <name name-style="western">
               <surname>Scott</surname>
               <given-names>L. Ridgway</given-names>
            </name>
         </contrib>
         <contrib contrib-type="author" id="contrib2">
            <name name-style="western">
               <surname>Clark</surname>
               <given-names>Terry</given-names>
            </name>
         </contrib>
         <contrib contrib-type="author" id="contrib3">
            <name name-style="western">
               <surname>Bagheri</surname>
               <given-names>Babak</given-names>
            </name>
         </contrib>
      </contrib-group>
      <pub-date>
         <day>09</day>
         <month>03</month>
         <year>2021</year>
      </pub-date>
      <isbn content-type="ppub">9780691119359</isbn>
      <isbn content-type="epub">9780691227658</isbn>
      <isbn content-type="epub">0691227659</isbn>
      <publisher>
         <publisher-name>Princeton University Press</publisher-name>
         <publisher-loc>PRINCETON; OXFORD</publisher-loc>
      </publisher>
      <permissions>
         <copyright-year>2005</copyright-year>
         <copyright-holder>Princeton University Press</copyright-holder>
      </permissions>
      <self-uri xlink:href="https://www.jstor.org/stable/j.ctv1ddcxfs"/>
      <abstract abstract-type="short">
         <p>
&lt;p&gt;What does Google's management of billions of Web pages have in
common with analysis of a genome with billions of nucleotides? Both
apply methods that coordinate many processors to accomplish a
single task. From mining genomes to the World Wide Web, from
modeling financial markets to global weather patterns, parallel
computing enables computations that would otherwise be impractical
if not impossible with sequential approaches alone. Its fundamental
role as an enabler of simulations and data analysis continues an
advance in a wide range of application areas. &lt;em&gt;Scientific
Parallel Computing&lt;/em&gt; is the first textbook to integrate all the
fundamentals of parallel computing in a single volume while also
providing a basis for a deeper understanding of the subject.
Designed for graduate and advanced undergraduate courses in the
sciences and in engineering, computer science, and mathematics, it
focuses on the three key areas of algorithms, architecture,
languages, and their crucial synthesis in performance. The book's
computational examples, whose math prerequisites are not beyond the
level of advanced calculus, derive from a breadth of topics in
scientific and engineering simulation and data analysis. The
programming exercises presented early in the book are designed to
bring students up to speed quickly, while the book later develops
projects challenging enough to guide students toward research
questions in the field. The new paradigm of cluster computing is
fully addressed. A supporting web site provides access to all the
codes and software mentioned in the book, and offers topical
information on popular parallel computing systems.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Integrates all the fundamentals of parallel computing essential
for today's high-performance requirements&lt;/li&gt;

&lt;li&gt;Ideal for graduate and advanced undergraduate students in the
sciences and in engineering, computer science, and mathematics&lt;/li&gt;

&lt;li&gt;Extensive programming and theoretical exercises enable students
to write parallel codes quickly&lt;/li&gt;

&lt;li&gt;More challenging projects later in the book introduce research
questions&lt;/li&gt;

&lt;li&gt;New paradigm of cluster computing fully addressed&lt;/li&gt;

&lt;li&gt;Supporting web site provides access to all the codes and
software mentioned in the book&lt;/li&gt;
&lt;/ul&gt;
</p>
      </abstract>
      <custom-meta-group>
         <custom-meta>
            <meta-name>
                    lang
                </meta-name>
            <meta-value>eng</meta-value>
         </custom-meta>
      </custom-meta-group>
   </book-meta>
   <body>
      <book-part book-part-type="book-toc-page-order" indexed="yes">
         <body>
            <book-part>
               <book-part-meta>
                  <book-part-id book-part-id-type="jstor">j.ctv1ddcxfs.1</book-part-id>
                  <title-group>
                     <title>Front Matter</title>
                  </title-group>
                  <fpage>i</fpage>
               </book-part-meta>
            </book-part>
            <book-part>
               <book-part-meta>
                  <book-part-id book-part-id-type="jstor">j.ctv1ddcxfs.2</book-part-id>
                  <title-group>
                     <title>Table of Contents</title>
                  </title-group>
                  <fpage>v</fpage>
               </book-part-meta>
            </book-part>
            <book-part>
               <book-part-meta>
                  <book-part-id book-part-id-type="jstor">j.ctv1ddcxfs.3</book-part-id>
                  <title-group>
                     <title>Preface</title>
                  </title-group>
                  <fpage>ix</fpage>
               </book-part-meta>
            </book-part>
            <book-part>
               <book-part-meta>
                  <book-part-id book-part-id-type="jstor">j.ctv1ddcxfs.4</book-part-id>
                  <title-group>
                     <title>Notation</title>
                  </title-group>
                  <fpage>xiii</fpage>
               </book-part-meta>
            </book-part>
            <book-part>
               <book-part-meta>
                  <book-part-id book-part-id-type="jstor">j.ctv1ddcxfs.5</book-part-id>
                  <title-group>
                     <label>Chapter One</label>
                     <title>Introduction</title>
                  </title-group>
                  <fpage>1</fpage>
                  <abstract>
                     <p>This chapter introduces many of the basic notions of parallel computation. In Section 1.1 we give a short overview of the book, and Section 1.2 attempts to define what we mean by parallel computing. Section 1.3 introduces the critical topic of performance, which is central to the entire subject. In Section 1.4 we describe some of the motivating factors for the development of parallel computers. This is followed by some examples of parallelizing computational problems. The first two examples (Section 1.5) are quite simple, but serve to introduce many of the most important concepts. The next examples (Section 1.6) help</p>
                  </abstract>
               </book-part-meta>
            </book-part>
            <book-part>
               <book-part-meta>
                  <book-part-id book-part-id-type="jstor">j.ctv1ddcxfs.6</book-part-id>
                  <title-group>
                     <label>Chapter Two</label>
                     <title>Parallel Performance</title>
                  </title-group>
                  <fpage>37</fpage>
                  <abstract>
                     <p>Parallel performance is more complex than sequential performance in many ways: first and foremost, it is more difficult to achieve good parallel performance, but it is also more difficult to predict and even to measure. This is due largely to the fact that essentially independent computers are cooperating on a single task. With simple (low performance) sequential processors, it has been possible to predict sequential performance based on a textual analysis of code or a mathematical description of an algorithm, by counting the number of basic operations. With parallel computation, other factors become critical, such as synchronization (Section 5.4.1), load</p>
                  </abstract>
               </book-part-meta>
            </book-part>
            <book-part>
               <book-part-meta>
                  <book-part-id book-part-id-type="jstor">j.ctv1ddcxfs.7</book-part-id>
                  <title-group>
                     <label>Chapter Three</label>
                     <title>Computer Architecture</title>
                  </title-group>
                  <fpage>71</fpage>
                  <abstract>
                     <p>In this chapter, we explore some basic concepts regarding computer architecture. The main objective is to provide a quantitative basis for modeling the performance of various (parallel) computers. Fortunately, there is a well-developed language for describing the various components, and relations between them, in a computer system. We will begin with a brief introduction to this language and illustrate it by using it to describe basic (sequential) computer concepts. Then we will use the notation to describe more advanced sequential computer concepts as well as parallel computers.</p>
                     <p>We utilize the “PMS” notation [134] to describe key components of a computer</p>
                  </abstract>
               </book-part-meta>
            </book-part>
            <book-part>
               <book-part-meta>
                  <book-part-id book-part-id-type="jstor">j.ctv1ddcxfs.8</book-part-id>
                  <title-group>
                     <label>Chapter Four</label>
                     <title>Dependences</title>
                  </title-group>
                  <fpage>99</fpage>
                  <abstract>
                     <p>Dependences between different program parts are the major obstacle to achieving high performance on modern computer architectures. There are different sources of dependences in computer programs. In “imperative” programming languages like Fortran and C, dependences are indicated by</p>
                     <p>flow of data (through memory), hence <italic>data dependences</italic>, and</p>
                     <p>flow of control (through the program), hence <italic>control dependences</italic>.</p>
                     <p>We will limit discussion to data dependences in this chapter. See [4] for more information on dependence analysis.</p>
                     <p>Our goal is to comprehend techniques for identifying dependences in codes and determining whether they can be easily remedied. Such techniques can guide restructuring of codes</p>
                  </abstract>
               </book-part-meta>
            </book-part>
            <book-part>
               <book-part-meta>
                  <book-part-id book-part-id-type="jstor">j.ctv1ddcxfs.9</book-part-id>
                  <title-group>
                     <label>Chapter Five</label>
                     <title>Parallel Languages</title>
                  </title-group>
                  <fpage>127</fpage>
                  <abstract>
                     <p>We have seen that the aim of parallel computing is to partition a computation into subcomputations that can be executed in parallel. In the spirit of a subcomputation being a unit of work, we use the generic term <italic>task</italic> for them. Parallel programming languages provide some degree of expression of the coordination among tasks, which by our definition can execute in parallel. These languages come in various forms from the ordinary to elaborate. A plain parallel application without specialized support written in C using Unix system calls could begin with a single process that forks a number of copies of</p>
                  </abstract>
               </book-part-meta>
            </book-part>
            <book-part>
               <book-part-meta>
                  <book-part-id book-part-id-type="jstor">j.ctv1ddcxfs.10</book-part-id>
                  <title-group>
                     <label>Chapter Six</label>
                     <title>Collective Operations</title>
                  </title-group>
                  <fpage>157</fpage>
                  <abstract>
                     <p>Collective operations are operations involving a group of processors to produce a single data structure or take a particular action. These were introduced in Section 5.5.2. A barrier in shared memory computing (Section 5.4.3) is a collective action that causes a group of processors to pause until all are present. A broadcast in message passing (Section 5.5.2) sends data from one processor to the group. Collective operations require efficient algorithms since they can involve potentially a large number of processors and large amounts of data. Moreover, they provide a good first introduction to some complex parallel algorithms.</p>
                     <p>There are different</p>
                  </abstract>
               </book-part-meta>
            </book-part>
            <book-part>
               <book-part-meta>
                  <book-part-id book-part-id-type="jstor">j.ctv1ddcxfs.11</book-part-id>
                  <title-group>
                     <label>Chapter Seven</label>
                     <title>Current Programming Standards</title>
                  </title-group>
                  <fpage>177</fpage>
                  <abstract>
                     <p>It dates a book to say anything about anything current. But it is helpful to examine current programming standards for two reasons. First it provides an easier entry point for using them to implement specific algorithms. Second it illustrates the abstract principles presented so far. We therefore give a brief introduction to two widely accepted parallel programming standards, one related to the message passing paradigm (MPI) and the other to the shared memory paradigm (Posix threads).</p>
                     <p>Message passing libraries have a long history. Most of the early commercial distributed memory machines had their own libraries. Although these had many key</p>
                  </abstract>
               </book-part-meta>
            </book-part>
            <book-part>
               <book-part-meta>
                  <book-part-id book-part-id-type="jstor">j.ctv1ddcxfs.12</book-part-id>
                  <title-group>
                     <label>Chapter Eight</label>
                     <title>The IPlanguage Model</title>
                  </title-group>
                  <fpage>191</fpage>
                  <abstract>
                     <p>The IPlanguages are simple extensions of Fortran (IPfortran) and C (IPC), cast in an SPMD model of parallel computation. The IPlanguages are an <italic>explicitly parallel</italic> approach in which all variables are replicated. All processes execute the same text, with parallelism exploited by explicit partitions of data and control flow (see Section 5.5). In this chapter we will describe the basic ideas of the IPlanguage extensions. The complete details can be found in their respective manuals [15] and [12].</p>
                     <p>The main feature of the IPlanguages is the memory model: guarded memory [13]. This allows us to formalize the notation x@n introduced</p>
                  </abstract>
               </book-part-meta>
            </book-part>
            <book-part>
               <book-part-meta>
                  <book-part-id book-part-id-type="jstor">j.ctv1ddcxfs.13</book-part-id>
                  <title-group>
                     <label>Chapter Nine</label>
                     <title>High Performance Fortran</title>
                  </title-group>
                  <fpage>213</fpage>
                  <abstract>
                     <p>High Performance Fortran (HPF) [92, 109] is an extension of Fortran which is <italic>sequentially consistent</italic>. It is often called a data-parallel language because it supports and exploits data-parallelism as a central theme. Parallelism is indicated <italic>implicitly</italic> by data distribution directives which suggest how data structures should be distributed among processors. A minor point is that the directives appear as comment statements instead of executable statements. This allows sequential code to be modified without losing the ability to compile it correctly for a sequential machine.</p>
                     <p>The decomposition of work is inferred from the data distribution by the owner-computes rule [28]. This</p>
                  </abstract>
               </book-part-meta>
            </book-part>
            <book-part>
               <book-part-meta>
                  <book-part-id book-part-id-type="jstor">j.ctv1ddcxfs.14</book-part-id>
                  <title-group>
                     <label>Chapter Ten</label>
                     <title>Loop Tiling</title>
                  </title-group>
                  <fpage>227</fpage>
                  <abstract>
                     <p>We have already seen that there is at least one industrial strength implementation of the basic shared memory programming model via POSIX threads in Section 7.3. We now consider another model that allows a higher-level approach to shared memory programming. This has a relatively long history (in comparison with other parallel programming languages), which we will not describe in detail. Reference [98] is an important waypoint in this history. We will describe this approach abstractly using the term tiling that was used by Kendall Square Research (KSR)¹ as its primary programming model.</p>
                     <p>The concept of a tiling in mathematics refers</p>
                  </abstract>
               </book-part-meta>
            </book-part>
            <book-part>
               <book-part-meta>
                  <book-part-id book-part-id-type="jstor">j.ctv1ddcxfs.15</book-part-id>
                  <title-group>
                     <label>Chapter Eleven</label>
                     <title>Matrix Eigen Analysis</title>
                  </title-group>
                  <fpage>237</fpage>
                  <abstract>
                     <p>This chapter pulls together algorithmic and language ideas from earlier chapters toward the development of a complete parallel implementation of a common computational problem: finding eigenvalue/eigenvector pairs. We give an extended example for a population modeling system based on the Leslie matrix. Owing to the simplicity of the Leslie matrix model, one chapter is sufficient to cover the derivation of the model (Section 11.1), the basic concepts from linear algebra and the power method, a standard algorithm for solving the eigenvalue problem inherent to the Leslie matrix method (Section 11.2), and program requirements and implementation (Section 11.3).</p>
                     <p>The main computational</p>
                  </abstract>
               </book-part-meta>
            </book-part>
            <book-part>
               <book-part-meta>
                  <book-part-id book-part-id-type="jstor">j.ctv1ddcxfs.16</book-part-id>
                  <title-group>
                     <label>Chapter Twelve</label>
                     <title>Linear Systems</title>
                  </title-group>
                  <fpage>257</fpage>
                  <abstract>
                     <p>One of the most basic numerical computations is the solution of linear equations by direct methods. Gaussian¹ elimination is the familiar technique of adding a suitable multiple of one equation to the other equations to reduce to a smaller system of equations. Done repeatedly, this eventually produces a triangular system that can be solved easily. We consider basic algorithms for parallelizing Gaussian elimination and solution of triangular systems of equations.</p>
                     <p>The solution of triangular systems represents one of the greatest challenges in parallel computing, especially for the sparse matrix case. We analyze several algorithms in part to provide some more</p>
                  </abstract>
               </book-part-meta>
            </book-part>
            <book-part>
               <book-part-meta>
                  <book-part-id book-part-id-type="jstor">j.ctv1ddcxfs.17</book-part-id>
                  <title-group>
                     <label>Chapter Thirteen</label>
                     <title>Particle Dynamics</title>
                  </title-group>
                  <fpage>283</fpage>
                  <abstract>
                     <p>Particle dynamics models are used in disciplines as diverse as astrophysics, biophysics, and fluid dynamics.</p>
                     <p>The “particles” in questions may represent atoms in a molecular dynamics simulation,¹ or clusters of galaxies in astrophysics. They are an abstraction that can be divided further in a more accurate model (an atom can be subdivided and modeled using quantum mechanics), but in the particle model the individual particles are autonomous and indivisible.</p>
                     <p>Only the simplest codes for particle dynamics have intrinsic parallelism of any substantial amount. Thus the primary objective in this chapter will be to modify basic algorithms to achieve parallelism. Often</p>
                  </abstract>
               </book-part-meta>
            </book-part>
            <book-part>
               <book-part-meta>
                  <book-part-id book-part-id-type="jstor">j.ctv1ddcxfs.18</book-part-id>
                  <title-group>
                     <label>Chapter Fourteen</label>
                     <title>Mesh Methods</title>
                  </title-group>
                  <fpage>315</fpage>
                  <abstract>
                     <p>Many computations in science and engineering are based on meshes (or grids) of one sort or another. The term “grid computing” has taken on a new meaning: computing using resources distributed over the planet [48]. Parallel computing is essential to “grid computing,” but in this chapter “grids” mean just meshes. We will attempt to use the term “mesh” whenever possible, but the term “multigrid” is so well established that we are compelled to continue with it. The term “multimesh” just does not sound right, even though it would mean the same thing.</p>
                     <p>These computations can be done with finite difference,</p>
                  </abstract>
               </book-part-meta>
            </book-part>
            <book-part>
               <book-part-meta>
                  <book-part-id book-part-id-type="jstor">j.ctv1ddcxfs.19</book-part-id>
                  <title-group>
                     <label>Chapter Fifteen</label>
                     <title>Sorting</title>
                  </title-group>
                  <fpage>335</fpage>
                  <abstract>
                     <p>This chapter surveys some basic parallel sorting algorithms. Sequential sorting methods are recalled in Section 15.1, and are then extended to parallel sorting in Section 15.2. Two parallel sorting methods, <italic>bitonic sort</italic> and <italic>odd-even transposition sort</italic>, are presented in Sections 15.2.1 and 15.2.2. In Section 15.3 examples from particle dynamics (Chapter 13) illustrate application of the methods. Parallel sorting and collective data movement, especially reductions (Chapter 6), draw from some similar principles. Data movement methods akin to those used in implementing parallel reduction operations are a fundamental component to efficient parallel sorting algorithms. In addition, we present in this chapter</p>
                  </abstract>
               </book-part-meta>
            </book-part>
            <book-part>
               <book-part-meta>
                  <book-part-id book-part-id-type="jstor">j.ctv1ddcxfs.20</book-part-id>
                  <title-group>
                     <title>Bibliography</title>
                  </title-group>
                  <fpage>357</fpage>
               </book-part-meta>
            </book-part>
            <book-part>
               <book-part-meta>
                  <book-part-id book-part-id-type="jstor">j.ctv1ddcxfs.21</book-part-id>
                  <title-group>
                     <title>Index</title>
                  </title-group>
                  <fpage>369</fpage>
               </book-part-meta>
            </book-part>
         </body>
      </book-part>
   </body>
</book>
