<?xml version="1.0" encoding="UTF-8"?>


<article dtd-version="1.0" article-type="research-article">
  <front>
      <journal-meta>
         <journal-id xmlns:xlink="http://www.w3.org/1999/xlink" journal-id-type="jstor">machtran</journal-id>
         <journal-id xmlns:xlink="http://www.w3.org/1999/xlink" journal-id-type="jstor">j50000036</journal-id>
         <journal-title-group xmlns:xlink="http://www.w3.org/1999/xlink">
            <journal-title>Machine Translation</journal-title>
         </journal-title-group>
      
         <publisher>
            <publisher-name>Springer</publisher-name>
         </publisher>
         <issn xmlns:xlink="http://www.w3.org/1999/xlink" pub-type="ppub">09226567</issn>
         <issn xmlns:xlink="http://www.w3.org/1999/xlink" pub-type="epub">15730573</issn>
         <custom-meta-group xmlns:xlink="http://www.w3.org/1999/xlink"/>
      </journal-meta>
      <article-meta xmlns:xlink="http://www.w3.org/1999/xlink">
         <volume xmlns:mml="http://www.w3.org/1998/Math/MathML"
                 xmlns:oasis="http://docs.oasis-open.org/ns/oasis-exchange/table">31</volume>
         <issue xmlns:mml="http://www.w3.org/1998/Math/MathML"
                xmlns:oasis="http://docs.oasis-open.org/ns/oasis-exchange/table">1/2</issue>
         <issue-id>i40211882</issue-id>
         <article-id pub-id-type="jstor">44987838</article-id>
         <title-group>
            <article-title>Zero-resource machine translation by multimodal encoder-decoder network with multimedia pivot</article-title>
         </title-group>
         <contrib-group>
            <contrib>
               <string-name>
                  <given-names>Hideki</given-names>
                  <surname>Nakayama</surname>
               </string-name>
            </contrib>
            <contrib>
               <string-name>
                  <given-names>Noriki</given-names>
                  <surname>Nishida</surname>
               </string-name>
            </contrib>
         </contrib-group>
         <pub-date pub-type="ppub">
            <day>1</day>
            <month>6</month>
            <year>2017</year>
         </pub-date>
         <fpage>49</fpage>
         <lpage>64</lpage>
      
      
      
      
      
      
      
      
         <permissions>
            <copyright-statement>© Springer Science+Business Media 2017</copyright-statement>
         </permissions>
         <self-uri xlink:href="https://www.jstor.org/stable/44987838"/>
      
      
         <abstract>
            <p>We propose an approach to build a neural machine translation system with no supervised resources (i.e., no parallel corpora) using multimodal embedded representation over texts and images. Based on the assumption that text documents are often likely to be described with other multimedia information (e.g., images) some-what related to the content, we try to indirectly estimate the relevance between two languages. Using multimedia as the "pivot", we project all modalities into one common hidden space where samples belonging to similar semantic concepts should come close to each other, whatever the observed space of each sample is. This modalityagnostic representation is the key to bridging the gap between different modalities. Putting a decoder on top of it, our network can flexibly draw the outputs from any input modality. Notably, in the testing phase, we need only source language texts as the input for translation. In experiments, we tested our method on two benchmarks to show that it can achieve reasonable translation performance. We compared and investigated several possible implementations and found that an end-to-end model that simultaneously optimized both rank loss in multimodal encoders and cross-entropy loss in decoders performed the best.</p>
         </abstract>
         <custom-meta-group>
            <custom-meta>
               <meta-name>lang</meta-name>
               <meta-value>eng</meta-value>
            </custom-meta>
         </custom-meta-group>
      </article-meta>
  </front>
  <back>
    
      <fn-group xmlns:xlink="http://www.w3.org/1999/xlink">
        <title>[Footnotes]</title>
        <fn id="d3461e218a1310">
            <label>1</label>
          
            <p>
               <mixed-citation id="d3461e225" publication-type="other">
http://www.statmt.org/wmt16/multimodal-task.html.</mixed-citation>
            </p>
        </fn>
      </fn-group>
    
    
      <ref-list>
        <title>References</title>
        <ref id="d3461e241a1310">
          
            <mixed-citation id="d3461e245" publication-type="other">
Bergsma S, Van Durme B (2011) Learning bilingual lexicons using the visual similarity of labeled web
images. In: Proc. LJCAI, pp 1764-1769</mixed-citation>
        </ref>
        <ref id="d3461e255a1310">
          
            <mixed-citation id="d3461e259" publication-type="other">
Deng J, Dong W, Socher R, Li LJ, Li K, Fei-Fei L (2009) ImageNet: a Large-scale Hierarchical Image
Database. In: Proc. IEEE CVPR</mixed-citation>
        </ref>
        <ref id="d3461e269a1310">
          
            <mixed-citation id="d3461e273" publication-type="other">
Elliott D, Frank S, Sima' an K, Specia L (2016) Multi30K: multilingual English-German image descriptions.
In: Proceedings of the 5th ACL Workshop on Vision and Language, pp 70-74</mixed-citation>
        </ref>
        <ref id="d3461e283a1310">
          
            <mixed-citation id="d3461e287" publication-type="other">
Firat O, Sankaran B, Al-Onaizan Y, Vural FTY, Cho K (2016) Zero-resource translation with multi-lingual
neural machine translation. In: Proc. EMNLP, pp 268-277</mixed-citation>
        </ref>
        <ref id="d3461e298a1310">
          
            <mixed-citation id="d3461e302" publication-type="other">
Frome A, Corrado G, Shlens J (2013) Devise: a deep visual-semantic embedding model. In: Proc. NIPS,
PP i-ii</mixed-citation>
        </ref>
        <ref id="d3461e312a1310">
          
            <mixed-citation id="d3461e316" publication-type="other">
Funaki R, Nakayama H (2015) Image-mediated learning for zero-shot cross-lingual document retrieval. In:
Proc. EMNLP, pp 585-590</mixed-citation>
        </ref>
        <ref id="d3461e326a1310">
          
            <mixed-citation id="d3461e330" publication-type="other">
Griibinger M, Clough P, Müller H, Deselaers T (2006) The IAPR TC-12 benchmark: a new evaluation
resource for visual information systems. In: Proc. LREC, pp 13-23</mixed-citation>
        </ref>
        <ref id="d3461e340a1310">
          
            <mixed-citation id="d3461e344" publication-type="other">
Hardoon DR, Szedmak S, Shawe-taylor J (2004) Canonical correlation analysis: an overview with appli-
cation to learning methods. Neural Comput 16(12):2639-2664</mixed-citation>
        </ref>
        <ref id="d3461e354a1310">
          
            <mixed-citation id="d3461e358" publication-type="other">
Hitschler J, Riezler S (2016) Multimodal pivots for image caption translation. In: Proc. ACL, pp 2399-2409</mixed-citation>
        </ref>
        <ref id="d3461e365a1310">
          
            <mixed-citation id="d3461e369" publication-type="other">
Hochreiter S, Schmidhuber J (1997) Long short-term memory. Neural Comput 9(8): 1-32</mixed-citation>
        </ref>
        <ref id="d3461e377a1310">
          
            <mixed-citation id="d3461e381" publication-type="other">
Hotelling H (1936) Relations between two sets of variants. Biometrika 28:321-377</mixed-citation>
        </ref>
        <ref id="d3461e388a1310">
          
            <mixed-citation id="d3461e392" publication-type="other">
Huang PY, Liu F, Shiang SR, Oh J, Dyer C (2016) Attention-based multimodal neural machine translation.
In: Proc. the first conference on machine translation (WMT), vol 2, pp 639-645</mixed-citation>
        </ref>
        <ref id="d3461e402a1310">
          
            <mixed-citation id="d3461e406" publication-type="other">
Jia Y, Shelhamer E, Donahue J, Karayev S, Long J, Girshick R, Guadarrama S, Darreil T (2014) Caffé :
convolutional architecture for fast feature embedding. In: ACM conference on multimedia, pp 675-678</mixed-citation>
        </ref>
        <ref id="d3461e416a1310">
          
            <mixed-citation id="d3461e420" publication-type="other">
Johnson J, Karpathy A, Fei-Fei L (2016) DenseCap: fully convolutional localization networks for dense
captioning. In: Proc. IEEE CVPR</mixed-citation>
        </ref>
        <ref id="d3461e430a1310">
          
            <mixed-citation id="d3461e434" publication-type="other">
Kiela D, Vulic I, Clark S (2015) Visual bilingual lexicon induction with transferred ConvNet features. In:
Proc. EMNLP, pp 148-158</mixed-citation>
        </ref>
        <ref id="d3461e444a1310">
          
            <mixed-citation id="d3461e448" publication-type="other">
Kingma DP, Ba JL (2015) Adam: a method for stochastic optimization. In: Proc. ICLR</mixed-citation>
        </ref>
        <ref id="d3461e456a1310">
          
            <mixed-citation id="d3461e460" publication-type="other">
Kiros R, Salakhutdinov R, Zemel RS (2015) Unifying visual-semantic embeddings with multimodal neural
language models. Transactions of the Association for Computational Linguistics (TACL)</mixed-citation>
        </ref>
        <ref id="d3461e470a1310">
          
            <mixed-citation id="d3461e474" publication-type="other">
Koehn P (2005) Europarl: a parallel corpus for statistical machine translation. Proc Mach Transi Summit
11:79-86</mixed-citation>
        </ref>
        <ref id="d3461e484a1310">
          
            <mixed-citation id="d3461e488" publication-type="other">
Koehn P (2009) Statistical machine translation. Cambridge University Press, Cambridge</mixed-citation>
        </ref>
        <ref id="d3461e495a1310">
          
            <mixed-citation id="d3461e499" publication-type="other">
Krizhevsky A, Sutskever I, Hinton GE (2012) ImageNet classification with deep convolutional neural
networks. In: Proc. NIPS, pp 1097-1105</mixed-citation>
        </ref>
        <ref id="d3461e509a1310">
          
            <mixed-citation id="d3461e513" publication-type="other">
Lin C Y, Och FJ (2004) Orange: a method for evaluating automatic evaluation metrics for machine translation.
In: Proc. COLING, pp 501-507</mixed-citation>
        </ref>
        <ref id="d3461e523a1310">
          
            <mixed-citation id="d3461e527" publication-type="other">
Luong MT, Le QV, Sutskever I, Vinyals O, Kaiser L (2016) Multi -task sequence to sequence learning. In:
Proc. ICLR</mixed-citation>
        </ref>
        <ref id="d3461e538a1310">
          
            <mixed-citation id="d3461e542" publication-type="other">
Oard D (1999) Issues in cross-language retrieval from document image collections. In: Proceedings of
symposium on document image understanding technology, pp 229-234</mixed-citation>
        </ref>
        <ref id="d3461e552a1310">
          
            <mixed-citation id="d3461e556" publication-type="other">
Papineni K, Roukos S, Ward T, Zhu Wj (2002) BLEU : a method for automatic evaluation of machine
translation. In: Proc. ACL, pp 31 1-318</mixed-citation>
        </ref>
        <ref id="d3461e566a1310">
          
            <mixed-citation id="d3461e570" publication-type="other">
Rajendran J, Khapra MM, Chandar S, Ravindran B (2016) Bridge correlational neural networks for multi-
lingual multimodal representation learning. In: Proc. NAACL-HLT, pp 171-181</mixed-citation>
        </ref>
        <ref id="d3461e580a1310">
          
            <mixed-citation id="d3461e584" publication-type="other">
Riesa J, Marcu D (2012) Automatic parallel fragment extraction from noisy data. In: Proc. NAACL, pp
538-542</mixed-citation>
        </ref>
        <ref id="d3461e594a1310">
          
            <mixed-citation id="d3461e598" publication-type="other">
Saha A, Khapra MM, Chandar S, Rajendran J, Cho K (2016) A correlational encoder decoder architecture
for pivot based sequence generation. In: Proc. COLING</mixed-citation>
        </ref>
        <ref id="d3461e608a1310">
          
            <mixed-citation id="d3461e612" publication-type="other">
Shen S, Cheng Y, He Z, He W, Wu H, Sun M, Liu Y (2016) Minimum risk training for neural machine
translation. In: Proc. ACL, pp 1683-1692</mixed-citation>
        </ref>
        <ref id="d3461e623a1310">
          
            <mixed-citation id="d3461e627" publication-type="other">
Silberer C, Lapata M (2014) Learning grounded meaning representations with autoencoders. In: Proc. ACL,
pp 721-732</mixed-citation>
        </ref>
        <ref id="d3461e637a1310">
          
            <mixed-citation id="d3461e641" publication-type="other">
Simonyan K, Zisserman A (2015) Very deep convolutional networks for large-scale image recognition. In:
Proc. ICLR</mixed-citation>
        </ref>
        <ref id="d3461e651a1310">
          
            <mixed-citation id="d3461e655" publication-type="other">
Sutskever I, Vinyals O, Le QV (2014) Sequence to sequence learning with neural networks. In: Proc. NIPS,
pp 3104-31 12</mixed-citation>
        </ref>
        <ref id="d3461e665a1310">
          
            <mixed-citation id="d3461e669" publication-type="other">
Taeger W (2011) The sentence-aligned European Patent Corpus. In: Proc. EAMT, pp 177-184</mixed-citation>
        </ref>
        <ref id="d3461e676a1310">
          
            <mixed-citation id="d3461e680" publication-type="other">
Udupa R, Khapra MM (2010) Improving the multilingual user experience of wikipedia using cross-language
name search. In: Proc. NAACL, pp 492-500</mixed-citation>
        </ref>
        <ref id="d3461e690a1310">
          
            <mixed-citation id="d3461e694" publication-type="other">
Uszkoreit J, Ponte J, Popat AC, Dubiner M (2010) Large scale parallel document mining for machine
translation. In: Proc. COLING, pp 1101-1109</mixed-citation>
        </ref>
        <ref id="d3461e705a1310">
          
            <mixed-citation id="d3461e709" publication-type="other">
Vinyals O, Toshev A, Bengio S, Erhan D (2015) Show and tell : a neural image caption generator. In: Proc.
IEEE CVPR</mixed-citation>
        </ref>
        <ref id="d3461e719a1310">
          
            <mixed-citation id="d3461e723" publication-type="other">
Vuli I, Kiela D, Clark S, Moens MF (2016) Multi-modal representations for improved bilingual lexicon
learning. In: Proc. ACL, pp 188-194</mixed-citation>
        </ref>
        <ref id="d3461e733a1310">
          
            <mixed-citation id="d3461e737" publication-type="other">
Wu H, Wang H (2007) Pivot language approach for phrase-based statistical machine translation. Mach
Transi 21(3):165-181</mixed-citation>
        </ref>
        <ref id="d3461e747a1310">
          
            <mixed-citation id="d3461e751" publication-type="other">
Wu H, Wang H (2009) Revisiting pivot language approach for machine translation. In: Proc. IJCNLP-ACL,
pp 154-162</mixed-citation>
        </ref>
        <ref id="d3461e761a1310">
          
            <mixed-citation id="d3461e765" publication-type="other">
Young P, Lai A, Hodosh M, Hockenmaier J (2014) From Image descriptions to visual denotations: new
similarity metrics for semantic inference over event descriptions. Trans ACL 2:67-78</mixed-citation>
        </ref>
      </ref-list>
    
  </back>
</article>


