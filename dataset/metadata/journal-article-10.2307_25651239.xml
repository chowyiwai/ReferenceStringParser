<?xml version="1.0" encoding="UTF-8"?>


<article dtd-version="1.0" article-type="research-article">
  <front>
      <journal-meta>
         <journal-id xmlns:xlink="http://www.w3.org/1999/xlink" journal-id-type="jstor">jcompgrapstat</journal-id>
         <journal-id xmlns:xlink="http://www.w3.org/1999/xlink" journal-id-type="jstor">j100879</journal-id>
         <journal-title-group xmlns:xlink="http://www.w3.org/1999/xlink">
            <journal-title>Journal of Computational and Graphical Statistics</journal-title>
         </journal-title-group>
      
         <publisher>
            <publisher-name>JCGS Management Committee of the American Statistical Association, Institute of Mathematical Statistics, and Interface Foundation of North America</publisher-name>
         </publisher>
         <issn xmlns:xlink="http://www.w3.org/1999/xlink" pub-type="ppub">10618600</issn>
         <custom-meta-group xmlns:xlink="http://www.w3.org/1999/xlink"/>
      </journal-meta>
      <article-meta xmlns:xlink="http://www.w3.org/1999/xlink">
         <volume xmlns:mml="http://www.w3.org/1998/Math/MathML"
                 xmlns:oasis="http://docs.oasis-open.org/ns/oasis-exchange/table">17</volume>
         <issue xmlns:mml="http://www.w3.org/1998/Math/MathML"
                xmlns:oasis="http://docs.oasis-open.org/ns/oasis-exchange/table">4</issue>
         <issue-id>i25651225</issue-id>
         <article-id pub-id-type="jstor">25651239</article-id>
         <title-group>
            <article-title>A Gradient-Based Optimization Algorithm for LASSO</article-title>
         </title-group>
         <contrib-group>
            <contrib>
               <string-name>
                  <given-names>Jinseog</given-names>
                  <surname>Kim</surname>
               </string-name>
            </contrib>
            <contrib>
               <string-name>
                  <given-names>Yuwon</given-names>
                  <surname>Kim</surname>
               </string-name>
            </contrib>
            <contrib>
               <string-name>
                  <given-names>Yongdai</given-names>
                  <surname>Kim</surname>
               </string-name>
            </contrib>
         </contrib-group>
         <pub-date pub-type="ppub">
            <day>1</day>
            <month>12</month>
            <year>2008</year>
         </pub-date>
         <fpage>994</fpage>
         <lpage>1009</lpage>
      
      
      
      
      
      
      
      
         <permissions>
            <copyright-statement>© 2008 The American Statistical Association, the Institute of Mathematical Statistics, and the Interface Foundation of North America</copyright-statement>
         </permissions>
      
         <self-uri xlink:href="https://www.jstor.org/stable/25651239"/>
      
      
         <abstract>
            <p>LASSO is a useful method for achieving both shrinkage and variable selection simultaneously. The main idea of LASSO is to use the L₁ constraint in the regularization step which has been applied to various models such as wavelets, kernel machines, smoothing splines, and multiclass logistic models. We call such models with the L₁ constraint generalized LASSO models. In this article, we propose a new algorithm called the gradient LASSO algorithm for generalized LASSO. The gradient LASSO algorithm is computationally more stable than QP-based algorithms because it does not require matrix inversions, and thus it can be more easily applied to high-dimensional data. Simulation results show that the proposed algorithm is fast enough for practical purposes and provides reliable results. To illustrate its computing power with high-dimensional data, we analyze multiclass microarray data using the proposed algorithm.</p>
         </abstract>
         <custom-meta-group>
            <custom-meta>
               <meta-name>lang</meta-name>
               <meta-value>eng</meta-value>
            </custom-meta>
         </custom-meta-group>
      </article-meta>
  </front>
  <back>
    
      <ref-list>
        <title>References</title>
        <ref id="d1886848e213a1310">
          
            <mixed-citation id="d1886848e217" publication-type="other">
Bakin, S. (1999), "Adaptive Regression and Model Selection in Data Mining Problem," PhD thesis, Australian
National University, Australia.</mixed-citation>
        </ref>
        <ref id="d1886848e227a1310">
          
            <mixed-citation id="d1886848e231" publication-type="other">
Barron, A. R. (1993), "A Universal Approximation Bounds for Superpositions of a Sigmoidal Function," IEEE
Transactions on Information Theory, 39, 930-945.</mixed-citation>
        </ref>
        <ref id="d1886848e241a1310">
          
            <mixed-citation id="d1886848e245" publication-type="other">
Chen, S. S., Donoho, D. L., and Saunders, M. A. (1999), "Atomic Decomposition by Basis Pursuit," SIAM Journal
on Scientific Computing, 20, 33-61.</mixed-citation>
        </ref>
        <ref id="d1886848e255a1310">
          
            <mixed-citation id="d1886848e259" publication-type="other">
Dudoit, S., Fridlyand, J., and Speed, T. (2002), "Comparison of Discrimination Methods for the Classification of
Tumors using Gene Expression Data," Journal of the American Statistical Association, 97, 77-87.</mixed-citation>
        </ref>
        <ref id="d1886848e270a1310">
          
            <mixed-citation id="d1886848e274" publication-type="other">
Efron, B., Hastie, T., Johnstone, I., and Tibshirani, R. (2004), "Least Angle Regression," The Annals of Statistics,
32, 407-499.</mixed-citation>
        </ref>
        <ref id="d1886848e284a1310">
          
            <mixed-citation id="d1886848e288" publication-type="other">
Fan, J., and Li, R. (2001), "Variable Selection via Noncocave Penalized Likelihood and its Oracle Properties,"
Journal of the American Statistical Association, 96, 1348-1360.</mixed-citation>
        </ref>
        <ref id="d1886848e298a1310">
          
            <mixed-citation id="d1886848e302" publication-type="other">
Golub, T., Slonim, D., Tamayo, P., Huard, C, Gaasenbeek, M., Mesirov, J., Coller, H., Loh, M., Downing, J.,
Caligiuri, M., Bloomfleld, C, and Lander, E. (1999), "Molecular Classification of Cancer: Class Discovery
and Class Prediction by Gene Expression Monitoring," Science, 286, 531-537.</mixed-citation>
        </ref>
        <ref id="d1886848e315a1310">
          
            <mixed-citation id="d1886848e319" publication-type="other">
Grandvalet, Y., and Canu, S. (1999), "Outcomes of the Equivalence of Adaptive Ridge with Least Absolute
Shrinkage," in Advances in Neural Information Processing Systems, eds. M. Kearns, S. Solla, and D. Cohn,
vol. 11, Cambridge, MA: MIT Press, pp. 445-451.</mixed-citation>
        </ref>
        <ref id="d1886848e332a1310">
          
            <mixed-citation id="d1886848e336" publication-type="other">
Gunn, S. R., and Kandola, J. S. (2002), "Structural Modelling with Sparse Kernels," Machine Learning, 48,
115-136.</mixed-citation>
        </ref>
        <ref id="d1886848e346a1310">
          
            <mixed-citation id="d1886848e350" publication-type="other">
Jones, L. K. (1992), "A Simple Lemma on Greedy Approximation in Hilbert Space and Convergence Rates for
Projection Pursuit Regression and Neural Network Training," The Annals of Statistics, 20, 608-613.</mixed-citation>
        </ref>
        <ref id="d1886848e361a1310">
          
            <mixed-citation id="d1886848e365" publication-type="other">
Kim, Y., and Kim, J. (2004), "Gardient Lasso for Feature Selection," in Proceedings of the 21th International
Conference on Machine Learning, New York: Morgan Kaufmann, pp. 473-480.</mixed-citation>
        </ref>
        <ref id="d1886848e375a1310">
          
            <mixed-citation id="d1886848e379" publication-type="other">
Kim, Y, Kim, J., and Kim, Y. (2006), "Blockwise Sparse Regression," Statistica Sinica, 16, 375-390.</mixed-citation>
        </ref>
        <ref id="d1886848e386a1310">
          
            <mixed-citation id="d1886848e390" publication-type="other">
Krishnapuram, B., Carlin, L., Figueiredo, M., and Hartemink, A. (2004), "Learning Sparse Classifier: Multi-
class Formulation, Fast Algorithms and Generalization Bounds," IEEE Tranactions on Pattern Analysis and
Machine Intelligence, 27, 957-968.</mixed-citation>
        </ref>
        <ref id="d1886848e403a1310">
          
            <mixed-citation id="d1886848e407" publication-type="other">
Lokhorst, J., Turlach, B. A., and Venables, W. N. (2006), "Lasso2*: An S-plus Library to Solve Regression
Problems While Imposing an L \ Constraint on the Parameters " Documentation in R-lasso2 library.</mixed-citation>
        </ref>
        <ref id="d1886848e417a1310">
          
            <mixed-citation id="d1886848e421" publication-type="other">
Ma, S., and Huang, J. (2005), "Lasso Method for Additive Risk Models with High-Dimensional Covariates,"
Technical Report, Department of Statistics and Actuarial Science, The University of Iowa.</mixed-citation>
        </ref>
        <ref id="d1886848e431a1310">
          
            <mixed-citation id="d1886848e435" publication-type="other">
Osborne, M. R., Presnell, B., and Turlach, B. A. (2000a), "A New Approach to Variable Selection in Least Squares
Problems," IMA Journal of Numerical Analysis, 20, 389-404.</mixed-citation>
        </ref>
        <ref id="d1886848e446a1310">
          
            <mixed-citation id="d1886848e450" publication-type="other">
-(2000b), "On the LASSO and its Dual," Journal of the Computational and Graphical Statistics, 9, 319-
337.</mixed-citation>
        </ref>
        <ref id="d1886848e460a1310">
          
            <mixed-citation id="d1886848e464" publication-type="other">

              Park, M. Y, and Hastie, T. (2007), "
              
              -Regularization Path Algorithm for Generalized Linear Models," Journal
            
of the Royal Statistical Society, Series B, 69, 659-677.</mixed-citation>
        </ref>
        <ref id="d1886848e477a1310">
          
            <mixed-citation id="d1886848e481" publication-type="other">
Perkins, S., Lacker, K, and Theiler, J. (2003), "Grafting: Fast, Incremetal Feature Selection by Gradient Descent
in Function Space," Journal of Machine Learning Research, 3, 1333-1356.</mixed-citation>
        </ref>
        <ref id="d1886848e491a1310">
          
            <mixed-citation id="d1886848e495" publication-type="other">
Rosset, S. (2005), "Following Curved Regularized Optimization Solution Paths," in Advances in Neural Infor-
mation Processing Systems 17, eds. L. K. Saul, Y. Weiss, and L. Bottou, Cambridge, MA: MIT Press, pp.
1153-1160.</mixed-citation>
        </ref>
        <ref id="d1886848e508a1310">
          
            <mixed-citation id="d1886848e512" publication-type="other">
Rosset, S., and Zhu, J. (2007), "Piecewise Linear Regularized Solution Path," The Annals of Statistics, 35, 1012—
1030.</mixed-citation>
        </ref>
        <ref id="d1886848e522a1310">
          
            <mixed-citation id="d1886848e526" publication-type="other">
Rosset, S., Zhu, J., and Hastie, T. (2004), "Boosting as a Regularized Path to a Maximum Margin Classifier,"
Journal of Machine Learning Research, 5, 941-973.</mixed-citation>
        </ref>
        <ref id="d1886848e537a1310">
          
            <mixed-citation id="d1886848e541" publication-type="other">
Roth, V. (2004), "The Generalized Lasso," IEEE Transactions on Neural Networks, 15, 16-28.</mixed-citation>
        </ref>
        <ref id="d1886848e548a1310">
          
            <mixed-citation id="d1886848e552" publication-type="other">
Staunton, J. E., Slonim, D. K., Coller, H. A., Tamayo, P., Angelo, M. J., Park, J., Scherf, U., Lee, J. K., Reinhold,
W. O., Weinstein, J. N., Mesirov, J. P., Lander, E. S., and Golub, T. R. (2001), "Chemosensitivity Prediction
by Transcriptional Profiling," in Proceedings of the National Academy of Science, vol. 98, pp. 10787-10792.</mixed-citation>
        </ref>
        <ref id="d1886848e565a1310">
          
            <mixed-citation id="d1886848e569" publication-type="other">
Tibshirani, R. (1996), "Regression Shrinkage and Selection via the Lasso," Journal of the Royal Statistical Soci-
ety, Series B, 58, 267-288.</mixed-citation>
        </ref>
        <ref id="d1886848e579a1310">
          
            <mixed-citation id="d1886848e583" publication-type="other">
Tibshirani, R., Saunders, M., Rosset, S., Zhu, J., and Knight, K. (2005), "Sparsity and Smoothness via the Fused
Lasso," Journal of the Royal Statistical Society, Series B, 67, 91-108.</mixed-citation>
        </ref>
        <ref id="d1886848e593a1310">
          
            <mixed-citation id="d1886848e597" publication-type="other">
Yuan, M., and Lin, Y (2006), "Model Selection and Estimation in Regression with Grouped Variables," Journal
of the Royal Statistical Society, Series B, 68, 49-67.</mixed-citation>
        </ref>
        <ref id="d1886848e607a1310">
          
            <mixed-citation id="d1886848e611" publication-type="other">
Zhang, H. H., Wahba, G., Lin, Y, Voelker, M., Ferris, M., Klein, R., and Klein, B. (2004), "Variable Selection and
Model Building via Likelihood Basis Pursuit," Journal of the American Statistical Association, 99, 659-672.</mixed-citation>
        </ref>
        <ref id="d1886848e622a1310">
          
            <mixed-citation id="d1886848e626" publication-type="other">
Zhang, T. (2003), "Sequential Greedy Approximation for Certain Convex Optimization Problems," IEEE Trans-
actions on Information Theory, 49, 682-691.</mixed-citation>
        </ref>
        <ref id="d1886848e636a1310">
          
            <mixed-citation id="d1886848e640" publication-type="other">
Zhao, P., and Yu, B. (2004), "Boosted Lasso," Technical Report 678, Department of Statistics, UC Berkeley.</mixed-citation>
        </ref>
        <ref id="d1886848e647a1310">
          
            <mixed-citation id="d1886848e651" publication-type="other">
Zou, H., and Hastie, T. (2005), "Regularization and Variable Selection via Elastic Net," Journal of the Royal
Statistical Society, Series B, 67, 301-320.</mixed-citation>
        </ref>
      </ref-list>
    
  </back>
</article>


