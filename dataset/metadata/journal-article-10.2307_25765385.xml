<?xml version="1.0" encoding="UTF-8"?>


<article dtd-version="1.0" article-type="research-article">
  <front>
      <journal-meta>
         <journal-id xmlns:xlink="http://www.w3.org/1999/xlink" journal-id-type="jstor">jcompgrapstat</journal-id>
         <journal-id xmlns:xlink="http://www.w3.org/1999/xlink" journal-id-type="jstor">j100879</journal-id>
         <journal-title-group xmlns:xlink="http://www.w3.org/1999/xlink">
            <journal-title>Journal of Computational and Graphical Statistics</journal-title>
         </journal-title-group>
      
         <publisher>
            <publisher-name>JCGS Management Committee of the American Statistical Association, Institute of Mathematical Statistics, and Interface Foundation of North America</publisher-name>
         </publisher>
         <issn xmlns:xlink="http://www.w3.org/1999/xlink" pub-type="ppub">10618600</issn>
         <custom-meta-group xmlns:xlink="http://www.w3.org/1999/xlink"/>
      </journal-meta>
      <article-meta xmlns:xlink="http://www.w3.org/1999/xlink">
         <volume xmlns:mml="http://www.w3.org/1998/Math/MathML"
                 xmlns:oasis="http://docs.oasis-open.org/ns/oasis-exchange/table">19</volume>
         <issue xmlns:mml="http://www.w3.org/1998/Math/MathML"
                xmlns:oasis="http://docs.oasis-open.org/ns/oasis-exchange/table">4</issue>
         <issue-id>i25765371</issue-id>
         <article-id pub-id-type="jstor">25765385</article-id>
         <article-categories>
            <subj-group>
               <subject>Lasso, LARS, and L₁ Regularization</subject>
            </subj-group>
         </article-categories>
         <title-group>
            <article-title>An Extension of Least Angle Regression Based on the Information Geometry of Dually Flat Spaces</article-title>
         </title-group>
         <contrib-group>
            <contrib>
               <string-name>
                  <given-names>Yoshihiro</given-names>
                  <surname>Hirose</surname>
               </string-name>
            </contrib>
            <contrib>
               <string-name>
                  <given-names>Fumiyasu</given-names>
                  <surname>Komaki</surname>
               </string-name>
            </contrib>
         </contrib-group>
         <pub-date pub-type="ppub">
            <day>1</day>
            <month>12</month>
            <year>2010</year>
         </pub-date>
         <fpage>1007</fpage>
         <lpage>1023</lpage>
      
      
      
      
      
      
      
      
         <permissions>
            <copyright-statement>© 2010 American Statistical Association, the Institute of Mathematical Statistics, and the Interface Foundat</copyright-statement>
         </permissions>
      
         <self-uri xlink:href="https://www.jstor.org/stable/25765385"/>
      
      
         <abstract>
            <p>We extend the least angle regression algorithm using the information geometry of dually flat spaces. The extended least angle regression algorithm is used for estimating parameters in generalized linear regression, and it can be also used for selecting explanatory variables. We use the fact that a model manifold of an exponential family is a dually flat space. In estimating parameters, curves corresponding to bisectors in the Euclidean space play an important role. Originally, the least angle regression algorithm is used for estimating parameters and selecting explanatory variables in linear regression. It is an efficient algorithm in the sense that the number of iterations is the same as the number of explanatory variables. We extend the algorithm while keeping this efficiency. However, the extended least angle regression algorithm differs significantly from the original algorithm. The extended least angle regression algorithm reduces one explanatory variable in each iteration while the original algorithm increases one explanatory variable in each iteration. We show results of the extended least angle regression algorithm for two types of datasets. The behavior of the extended least angle regression algorithm is shown. Especially, estimates of parameters become smaller and smaller, and vanish in turn.</p>
         </abstract>
         <custom-meta-group>
            <custom-meta>
               <meta-name>lang</meta-name>
               <meta-value>eng</meta-value>
            </custom-meta>
         </custom-meta-group>
      </article-meta>
  </front>
  <back>
    
      <ref-list>
        <title>REFERENCES</title>
        <ref id="d2631912e306a1310">
          
            <mixed-citation id="d2631912e310" publication-type="other">
Amari, S. (1985), Differential-Geometrical Methods in Statistics. Springer Lecture Notes in Statistics, Vol. 28,
New York: Springer. [1008,1010]</mixed-citation>
        </ref>
        <ref id="d2631912e320a1310">
          
            <mixed-citation id="d2631912e324" publication-type="other">
Amari, S., and Nagaoka, H. (2000), Methods of Information Geometry. Translations of Mathematical Mono-
graphs, Vol. 191, Providence: American Mathematical Society and Oxford University Press. [1008,1010]</mixed-citation>
        </ref>
        <ref id="d2631912e334a1310">
          
            <mixed-citation id="d2631912e338" publication-type="other">
Candes, E., and Tao, T. (2007), "The Dantzig Selector: Statistical Estimation When p Is Much Larger Than n"
(with discussion), The Annals of Statistics, 35, 2313-2351. [1022]</mixed-citation>
        </ref>
        <ref id="d2631912e348a1310">
          
            <mixed-citation id="d2631912e352" publication-type="other">
Efron, B., Hastie, T., Johnstone, I., and Tibshirani, R. (2004), "Least Angle Regression" (with discussion), The
Annals of Statistics, 32, 407-499. [1007,1019]</mixed-citation>
        </ref>
        <ref id="d2631912e363a1310">
          
            <mixed-citation id="d2631912e367" publication-type="other">
Hastie, T., Tibshirani, R., and Friedman, J. (2009), The Elements of Statistical Learning: Data Mining, Inference,
and Prediction (2nd ed.), New York: Springer. [1020,1022]</mixed-citation>
        </ref>
        <ref id="d2631912e377a1310">
          
            <mixed-citation id="d2631912e381" publication-type="other">
Kass, R., and Vos, P. (1997), Geometrical Foundations of Asymptotic Inference, New York: Wiley. [1010]</mixed-citation>
        </ref>
        <ref id="d2631912e388a1310">
          
            <mixed-citation id="d2631912e392" publication-type="other">
Osborne, M., Presnell, B., and Turlach, B. (2000), "A New Approach to Variable Selection in Least Squares
Problems," IMA Journal of Numerical Analysis, 20, 389-403. [1008]</mixed-citation>
        </ref>
        <ref id="d2631912e402a1310">
          
            <mixed-citation id="d2631912e406" publication-type="other">
Park, M. Y, and Hastie, T. (2007), "Li-Regularization Path Algorithm for Generalized Linear Models," Journal
of the Royal Statistical Society, Sen B, 69, 659-677. [1008,1020,1021]</mixed-citation>
        </ref>
        <ref id="d2631912e416a1310">
          
            <mixed-citation id="d2631912e420" publication-type="other">
R Development Core Team (2009), R: A Language and Environment for Statistical Computing, Vienna, Austria:
R Foundation for Statistical Computing. ISBN 3-900051-07-0. Available at http://www.R-project.org. [1018]</mixed-citation>
        </ref>
        <ref id="d2631912e430a1310">
          
            <mixed-citation id="d2631912e434" publication-type="other">
Tibshirani, R. (1996), "Regression Shrinkage and Selection via the Lasso," Journal of the Royal Statistical Soci-
ety, Sen B, 58, 267-288. [1008]</mixed-citation>
        </ref>
      </ref-list>
    
  </back>
</article>


