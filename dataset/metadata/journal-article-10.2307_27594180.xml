<?xml version="1.0" encoding="UTF-8"?>


<article dtd-version="1.0" article-type="research-article">
  <front>
      <journal-meta>
         <journal-id xmlns:xlink="http://www.w3.org/1999/xlink" journal-id-type="jstor">jcompgrapstat</journal-id>
         <journal-id xmlns:xlink="http://www.w3.org/1999/xlink" journal-id-type="jstor">j100879</journal-id>
         <journal-title-group xmlns:xlink="http://www.w3.org/1999/xlink">
            <journal-title>Journal of Computational and Graphical Statistics</journal-title>
         </journal-title-group>
      
         <publisher>
            <publisher-name>American Statistical Association, Institute of Mathematical Statistics, and Interface Foundation of North America</publisher-name>
         </publisher>
         <issn xmlns:xlink="http://www.w3.org/1999/xlink" pub-type="ppub">10618600</issn>
         <custom-meta-group xmlns:xlink="http://www.w3.org/1999/xlink"/>
      </journal-meta>
      <article-meta xmlns:xlink="http://www.w3.org/1999/xlink">
         <volume xmlns:mml="http://www.w3.org/1998/Math/MathML"
                 xmlns:oasis="http://docs.oasis-open.org/ns/oasis-exchange/table">15</volume>
         <issue xmlns:mml="http://www.w3.org/1998/Math/MathML"
                xmlns:oasis="http://docs.oasis-open.org/ns/oasis-exchange/table">2</issue>
         <issue-id>i27594177</issue-id>
         <article-id pub-id-type="jstor">27594180</article-id>
         <article-id pub-id-type="pub-doi">10.1198/106186006X113548</article-id>
         <title-group>
            <article-title>Conjugate Direction Boosting</article-title>
         </title-group>
         <contrib-group>
            <contrib>
               <string-name>
                  <given-names>Roman Werner</given-names>
                  <surname>Lutz</surname>
               </string-name>
            </contrib>
            <contrib>
               <string-name>
                  <given-names>Peter</given-names>
                  <surname>Bühlmann</surname>
               </string-name>
            </contrib>
         </contrib-group>
         <pub-date pub-type="ppub">
            <day>1</day>
            <month>6</month>
            <year>2006</year>
         </pub-date>
         <fpage>287</fpage>
         <lpage>311</lpage>
         <permissions>
            <copyright-statement>Copyright 2006 American Statistical Association, the Institute of Mathematical Statistics, and the Interface Foundation of North America</copyright-statement>
         </permissions>
         <self-uri xlink:href="https://www.jstor.org/stable/27594180"/>
      
      
         <abstract>
            <p>Boosting in the context of linear regression has become more attractive with the invention of least angle regression (LARS), where the connection between the lasso and forward stagewise fitting (boosting) has been established. Earlier it has been found that boosting is a functional gradient optimization. Instead of the gradient, we propose a conjugate direction method (CDBoost). As a result, we obtain a fast forward stepwise variable selection algorithm. The conjugate direction of CDBoost is analogous to the constrained gradient in boosting. Using this analogy, we generalize CDBoost to: (1) include small step sizes (shrinkage) which often improves prediction accuracy; and (2) the nonparametric setting with fitting methods such as trees or splines, where least angle regression and the lasso seem to be unfeasible. The step size in CDBoost has a tendency to govern the degree between L0- and L1-penalization. This makes CDBoost surprisingly flexible. We compare the different methods on simulated and real datasets. CDBoost achieves the best predictions mainly in complicated settings with correlated covariates, where it is difficult to determine the contribution of a given covariate to the response. The gain of CDBoost over boosting is especially high in sparse cases with high signal to noise ratio and few effective covariates.</p>
         </abstract>
         <kwd-group>
            <kwd>Forward stepwise variable selection</kwd>
            <kwd>High-dimensional linear regression</kwd>
            <kwd>L0- and L1-penalization</kwd>
            <kwd>Least angle regression</kwd>
            <kwd>Orthogonal greedy algorithm</kwd>
         </kwd-group>
         <custom-meta-group>
            <custom-meta>
               <meta-name>lang</meta-name>
               <meta-value>eng</meta-value>
            </custom-meta>
         </custom-meta-group>
      </article-meta>
  </front>
  <back>
    
      <ref-list>
        <title>References</title>
        <ref id="d1370178e147a1310">
          
            <mixed-citation id="d1370178e151" publication-type="other">
Breiman, L. (1998), "Arcing Classifier (Pkg: p801-849)," The Annals of Statistics, 26, 801–824.</mixed-citation>
        </ref>
        <ref id="d1370178e158a1310">
          
            <mixed-citation id="d1370178e162" publication-type="other">
—(1999), "Prediction Games and Arcing Algorithms," Neural Computation, 11, 1493–1517.</mixed-citation>
        </ref>
        <ref id="d1370178e169a1310">
          
            <mixed-citation id="d1370178e173" publication-type="other">
Breiman, L., and Friedman, J. H. (1985), "Estimating Optimal Transformations for Multiple Regression and
Correlation (C/R: p598–619)," Journal of the American Statistical Association, 80, 580–598.</mixed-citation>
        </ref>
        <ref id="d1370178e183a1310">
          
            <mixed-citation id="d1370178e187" publication-type="other">
Bühlmann, P., and Yu, B. (2003), "Boosting with the L2-Loss: Regression and Classification," Journal of the
American Statistical Association, 98, 324–339.</mixed-citation>
        </ref>
        <ref id="d1370178e198a1310">
          
            <mixed-citation id="d1370178e202" publication-type="other">
—(2005), "Sparse Boosting," to appear in Journal of Machine Learning Research.</mixed-citation>
        </ref>
        <ref id="d1370178e209a1310">
          
            <mixed-citation id="d1370178e213" publication-type="other">
Duffy, N., and Helmbold, D. (2000), "Leaveraging for Regression," in Proceedings of the Thirteenth Annual
Conference on Computational Learning Theory.</mixed-citation>
        </ref>
        <ref id="d1370178e223a1310">
          
            <mixed-citation id="d1370178e227" publication-type="other">
Efron, B., Hastie, T., Johnstone, I., and Tibshirani, R. (2004), "Least Angle Regression," The Annals of Statistics,
32,407–451.</mixed-citation>
        </ref>
        <ref id="d1370178e237a1310">
          
            <mixed-citation id="d1370178e241" publication-type="other">
Freund, Y. (1995), "Boosting a Weak Learning Algorithm by Majority," Information and Computation, 121,
256–285.</mixed-citation>
        </ref>
        <ref id="d1370178e251a1310">
          
            <mixed-citation id="d1370178e255" publication-type="other">
Freund, Y, and Schapire, R. E. (1996), "Experiments With a New Boosting Algorithm," in Proceedings of the
Thirteenth International Conference on Machine Learning, pp. 148–156.</mixed-citation>
        </ref>
        <ref id="d1370178e265a1310">
          
            <mixed-citation id="d1370178e269" publication-type="other">
Friedman, J. H. (2001), "Greedy Function Approximation: A Gradient Boosting Machine," The Annals of Statistics,
29, 1189–1232.</mixed-citation>
        </ref>
        <ref id="d1370178e280a1310">
          
            <mixed-citation id="d1370178e284" publication-type="other">
Golub, T., Slonim, D., Tamayo, P., Huard, C, Gassenbeek, M., Mesirov, J., Coller, H., Loh, M., Downing, J.,
Caligiuri, M., Bloomfield, C.,, and Lander, E. (1999), "Molecular Classification of Cancer: Class Discovery
and Class Prediction by Gene Expression Monitoring," Science, 286, 531–537.</mixed-citation>
        </ref>
        <ref id="d1370178e297a1310">
          
            <mixed-citation id="d1370178e301" publication-type="other">
Mallat, S., and Zhang, Z. (1993), "Matching Pursuits With Time-Frequency Dictionaries," IEEE Transactions on
Signal Processing, 41, 3397–3415.</mixed-citation>
        </ref>
        <ref id="d1370178e311a1310">
          
            <mixed-citation id="d1370178e315" publication-type="other">
Meir, R., and Rätsch, G. (2003), "An Introduction to Boosting and Leveraging," in Advanced Lectures on Machine
Learning, eds. S. Mendelson and A. Smola, New York: Springer, pp. 119–184.</mixed-citation>
        </ref>
        <ref id="d1370178e325a1310">
          
            <mixed-citation id="d1370178e329" publication-type="other">
Miller, A. (2002), Subset Selection in Regression, London: Chapman &amp;amp; Hall.</mixed-citation>
        </ref>
        <ref id="d1370178e336a1310">
          
            <mixed-citation id="d1370178e340" publication-type="other">
Nocedal, J., and Wright, S. J. (1999), Numerical Optimization, New York: Springer Series in Operations Research.</mixed-citation>
        </ref>
        <ref id="d1370178e347a1310">
          
            <mixed-citation id="d1370178e351" publication-type="other">
Phatak, A., and de Hoog, F. (2002), "Exploiting the Connection Between PLS, Lanczos Methods and Conjugate
Gradients: Alternative Proofs of Some Properties of PLS," Journal of Chemometrics, 16, 361–367.</mixed-citation>
        </ref>
        <ref id="d1370178e362a1310">
          
            <mixed-citation id="d1370178e366" publication-type="other">
Ratsch, G., Demiriz, A., and Bennett, K. (2002), "Sparse Regression Ensembles in Infinite and Finite Hypothesis
Spaces," Machine Learning, 48, 193–221.</mixed-citation>
        </ref>
        <ref id="d1370178e376a1310">
          
            <mixed-citation id="d1370178e380" publication-type="other">
Ratsch, G., Onoda, T., and Müller, K.-R. (2001), "Soft Margins for AdaBoost," Machine Learning, 42, 287–320.</mixed-citation>
        </ref>
        <ref id="d1370178e387a1310">
          
            <mixed-citation id="d1370178e391" publication-type="other">
Schapire, R. E. (1990), "The Strength of Weak Learnability," Machine Learning, 5, 197–227.</mixed-citation>
        </ref>
        <ref id="d1370178e398a1310">
          
            <mixed-citation id="d1370178e402" publication-type="other">
Temlyakov, V. (2000), "Weak Greedy Algorithms," Advances in Computational Mathematics, 12, 213–227.</mixed-citation>
        </ref>
        <ref id="d1370178e409a1310">
          
            <mixed-citation id="d1370178e413" publication-type="other">
Tibshirani, R. (1996), "Regression Shrinkage and Selection Via the Lasso," Journal of the Royal Statistical Society,
Series B, 58, 267–288.</mixed-citation>
        </ref>
        <ref id="d1370178e423a1310">
          
            <mixed-citation id="d1370178e427" publication-type="other">
Zemel, R., and Pitassi, T. (2001), "A Gradient-Based Boosting Algorithm for Regression Problems," in NIPS-13:
Advances in Neural Information Processing Systems, 13, Cambridge, MA: MIT Press.</mixed-citation>
        </ref>
        <ref id="d1370178e438a1310">
          
            <mixed-citation id="d1370178e442" publication-type="other">
Zou, H., and Hastie, T. (2005), "Regularization and Variable Selection via the Elastic Net," Journal of the Royal
Statistical Society, Series B, 67, 301–320.</mixed-citation>
        </ref>
      </ref-list>
    
  </back>
</article>


